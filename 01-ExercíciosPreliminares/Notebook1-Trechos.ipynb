{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1 - Trechos - Elton Cardoso do Nascimento\n",
    "RA 233840"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchtext\n",
    "!pip install 'portalocker>=2.0.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.functional import one_hot\n",
    "from torchtext.datasets import IMDB\n",
    "from collections import Counter\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "#Added\n",
    "import time\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.1.a, I.1.b, I.1.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit the vocabulary size to 20000 most frequent tokens\n",
    "vocab_size = 20000\n",
    "\n",
    "counter = Counter()\n",
    "counterTarget = Counter() # NEW: counter for target\n",
    "meanN_Word = 0 # NEW: mean words in a sentence\n",
    "for (target, line) in list(IMDB(split='train')):\n",
    "    words = line.split() # NEW: avoid duplication\n",
    "    counter.update(words) # CHANGED: use words variable\n",
    "\n",
    "    # NEW: Update target counter\n",
    "    if target == 1: \n",
    "      counterTarget.update(\"-\")\n",
    "    else:\n",
    "      counterTarget.update(\"+\")\n",
    "\n",
    "    meanN_Word += len(words) # NEW: add the sentence lenght\n",
    "\n",
    "nTrain = len(list(IMDB(split='train'))) # NEW: store size of train dataset\n",
    "meanN_Word /= nTrain # NEW: compute mean words per sentence\n",
    "\n",
    "# create a vocabulary of the 20000 most frequent tokens\n",
    "most_frequent_words = sorted(counter, key=counter.get, reverse=True)[:vocab_size]\n",
    "vocab = {word: i for i, word in enumerate(most_frequent_words, 1)}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# NEW: print required results\n",
    "print(\"Amostras positivas:\", counterTarget[\"+\"])\n",
    "print(\"Amostras negativas:\", counterTarget[\"-\"])\n",
    "print(\"Total de amostras:\", nTrain)\n",
    "print(\"Comprimento médio:\", meanN_Word, \"palavras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.2.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"5 palavras mais frequentes:\", most_frequent_words[:5])\n",
    "print(\"5 palavras menos frequentes:\", most_frequent_words[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.2.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unknown(sentence):\n",
    "  n_unknown = 0\n",
    "  encoded_sentence = encode_sentence(sentence, vocab)\n",
    "  for encoded_word in encoded_sentence:\n",
    "    if encoded_word == 0:\n",
    "      n_unknown += 1\n",
    "\n",
    "  return n_unknown\n",
    "\n",
    "total_unknown = 0\n",
    "\n",
    "for (target, line) in list(IMDB(split='train')):\n",
    "  encode_sentence(line, vocab)\n",
    "\n",
    "  total_unknown += count_unknown(line)\n",
    "\n",
    "print(\"Total de tokens desconhecidos:\", total_unknown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.3.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = []\n",
    "counterTarget = Counter()\n",
    "nTrain = 0\n",
    "for (target, line) in list(IMDB(split='train')):\n",
    "\n",
    "  if target == 1 and counterTarget[\"-\"] < 100: \n",
    "    trainDataset.append((target, line))\n",
    "\n",
    "    counterTarget.update(\"-\")\n",
    "    nTrain += 1\n",
    "  elif target == 2 and counterTarget[\"+\"] < 100:\n",
    "    trainDataset.append((target, line))\n",
    "    \n",
    "    counterTarget.update(\"+\")\n",
    "    nTrain += 1\n",
    "\n",
    "  if nTrain == 200:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit the vocabulary size to 20000 most frequent tokens\n",
    "vocab_size = 20000\n",
    "\n",
    "counter = Counter()\n",
    "counterTarget = Counter()\n",
    "meanN_Word = 0\n",
    "for (target, line) in trainDataset: #CHANGED: use new dataset\n",
    "    words = line.split()\n",
    "    counter.update(words)\n",
    "\n",
    "    # NEW: Update target counter\n",
    "    if target == 1: \n",
    "      counterTarget.update(\"-\")\n",
    "    else:\n",
    "      counterTarget.update(\"+\")\n",
    "\n",
    "    meanN_Word += len(words)\n",
    "\n",
    "nTrain = len(list(IMDB(split='train')))\n",
    "meanN_Word /= nTrain\n",
    "\n",
    "# create a vocabulary of the 20000 most frequent tokens\n",
    "most_frequent_words = sorted(counter, key=counter.get, reverse=True)[:vocab_size]\n",
    "vocab = {word: i for i, word in enumerate(most_frequent_words, 1)}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "\n",
    "print(\"Amostras positivas:\", counterTarget[\"+\"])\n",
    "print(\"Amostras negativas:\", counterTarget[\"-\"])\n",
    "print(\"Total de amostras:\", nTrain)\n",
    "print(\"Comprimento médio:\", meanN_Word, \"palavras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.1.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_info(dataset):\n",
    "\n",
    "  counterTarget = Counter()\n",
    "  meanN_Word = 0\n",
    "  nSentence = len(dataset)\n",
    "  for i in range(nSentence):\n",
    "    encoding, target = dataset[i]\n",
    "\n",
    "    if target == 0:\n",
    "      counterTarget.update(\"-\")\n",
    "    else:\n",
    "      counterTarget.update(\"+\")\n",
    "\n",
    "    meanN_Word += int(encoding.sum())\n",
    "\n",
    "  meanN_Word /= nSentence\n",
    "\n",
    "  print(\"Amostras positivas:\", counterTarget[\"+\"])\n",
    "  print(\"Amostras negativas:\", counterTarget[\"-\"])\n",
    "  print(\"Total de amostras:\", nSentence)\n",
    "  print(\"Média de palavras codificadas:\", meanN_Word, \"palavras\")\n",
    "\n",
    "print(\"TREINO\")\n",
    "print_info(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.1.b\n",
    "\n",
    "(OBS: média de palavras codificadas no vetor está em II.1.a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_coded_words = 0\n",
    "\n",
    "for data in train_data:\n",
    "  input_data, _ = data\n",
    "\n",
    "  mean_coded_words += input_data.sum()\n",
    "\n",
    "mean_coded_words /= len(train_data)\n",
    "\n",
    "mean_coded_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_n_batch = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "model = model.to(device)\n",
    "# Define loss and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.05)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()  # Start time of the epoch\n",
    "    model.train()\n",
    "\n",
    "    n_batch = 0\n",
    "    time_foward = 0\n",
    "    time_backward = 0\n",
    "    time_loader = 0\n",
    "    start_for = time.time()\n",
    "    start_loader = time.time()\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        end_loader = time.time()\n",
    "        time_loader += end_loader - start_loader\n",
    "\n",
    "        ## FORWARD ------------------------------\n",
    "        start_forward = time.time()\n",
    "\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        # OBS: O enunciado indica que o passo forward deve\n",
    "        # começar antes das duas linhas acima (????)\n",
    "\n",
    "        logits = model(inputs)\n",
    "        loss = criterion(logits.squeeze(), targets.float())\n",
    "\n",
    "        end_forward = time.time()\n",
    "        ## --------------------------------------\n",
    "        ## BACKWARD -----------------------------\n",
    "        start_backward = time.time()\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        end_backward = time.time()\n",
    "        ## -------------------------------------\n",
    "\n",
    "        time_foward += end_forward - start_forward\n",
    "        time_backward += end_backward - start_backward\n",
    "\n",
    "        n_batch += 1\n",
    "\n",
    "        if n_batch == run_n_batch:\n",
    "          break\n",
    "\n",
    "        start_loader = time.time()\n",
    "\n",
    "\n",
    "\n",
    "    end_for = time.time()\n",
    "\n",
    "    time_for = (end_for - start_for) /n_batch\n",
    "    time_foward /= n_batch\n",
    "    time_backward /= n_batch\n",
    "    time_loader /= n_batch\n",
    "\n",
    "    print(\"Tempo médio em 1 laço:\", time_for)\n",
    "    print(\"Tempo médio em 1 foward:\", time_foward)\n",
    "    print(\"Tempo médio em 1 backward:\", time_backward)\n",
    "    print(\"Tempo médio loader things:\", time_loader)\n",
    "    break\n",
    "\n",
    "    end_time = time.time()  # End time of the epoch\n",
    "    epoch_duration = end_time - start_time  # Duration of epoch\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], \\\n",
    "            Loss: {loss.item():.4f}, \\\n",
    "            Elapsed Time: {epoch_duration:.2f} sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, split, vocab):\n",
    "        self.data = list(IMDB(split=split))\n",
    "\n",
    "        self.vocab = vocab\n",
    "\n",
    "        #####################\n",
    "        ##HERE\n",
    "        #####################\n",
    "        \n",
    "        # Cache data encoding and target\n",
    "        self.itens = []\n",
    "        for idx in range(len(self.data)):\n",
    "          target, line = self.data[idx]\n",
    "          target = 1 if target == 1 else 0\n",
    "\n",
    "          # one-hot encoding\n",
    "          X = torch.zeros(len(self.vocab) + 1)\n",
    "          for word in encode_sentence(line, self.vocab):\n",
    "              X[word] = 1\n",
    "\n",
    "          self.itens.append((X, torch.tensor(target)))\n",
    "          \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        #####################\n",
    "        ##AND HERE\n",
    "        #####################\n",
    "\n",
    "        #Only retrieves data\n",
    "        return self.itens[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.3.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_space = []\n",
    "for i in range(7):\n",
    "  lr_space.append(10**-i)\n",
    "  lr_space.append(5*10**-i)\n",
    "\n",
    "lr_space = np.sort(lr_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(model, lr:int):\n",
    "  model = model.to(device)\n",
    "  # Define loss and optimizer\n",
    "  criterion = nn.BCEWithLogitsLoss()\n",
    "  optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "  # Training loop\n",
    "  num_epochs = 5\n",
    "  for epoch in range(num_epochs):\n",
    "      start_time = time.time()  # Start time of the epoch\n",
    "      model.train()\n",
    "\n",
    "      for inputs, targets in train_loader:\n",
    "\n",
    "          inputs = inputs.to(device)\n",
    "          targets = targets.to(device)\n",
    "\n",
    "          logits = model(inputs)\n",
    "          loss = criterion(logits.squeeze(), targets.float())\n",
    "\n",
    "\n",
    "          # Backward and optimize\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "def evaluate(model):\n",
    "  model.eval()\n",
    "\n",
    "  with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        logits = model(inputs)\n",
    "        predicted = torch.round(torch.sigmoid(logits.squeeze()))\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "  return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = np.zeros_like(lr_space, np.float64)\n",
    "\n",
    "for i in range(len(lr_space)):\n",
    "  lr = lr_space[i]\n",
    "\n",
    "  for _ in range(5):\n",
    "    model = OneHotMLP(vocab_size)\n",
    "    train(model, lr)\n",
    "    accuracy = evaluate(model)\n",
    "\n",
    "    accuracies[i] += accuracy\n",
    "\n",
    "accuracies /= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "plt.plot(lr_space, accuracies, \"o\")\n",
    "\n",
    "plt.xlabel(\"Taxa de treino\")\n",
    "plt.ylabel(\"Acurácia\")\n",
    "plt.title(\"Impacto da taxa de treino\")\n",
    "\n",
    "plt.xscale(\"log\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(\"ImpactoDaTaxaDeTreino.png\", facecolor=\"white\", transparent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.3.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.argmax(accuracies)\n",
    "l_opt = lr_space[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.4.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text:str) -> str:\n",
    "  text = text.lower()\n",
    "\n",
    "  for punctuation in string.punctuation:\n",
    "    text = text.replace(punctuation, \"\")\n",
    "\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit the vocabulary size to 20000 most frequent tokens\n",
    "vocab_size = 20000\n",
    "\n",
    "counter = Counter()\n",
    "for (target, line) in list(IMDB(split='train')):\n",
    "    #AQUI↓################################\n",
    "    line = clean_text(line) \n",
    "    #AQUI↑################################\n",
    "\n",
    "    counter.update(line.split())\n",
    "\n",
    "# create a vocabulary of the 20000 most frequent tokens\n",
    "most_frequent_words = sorted(counter, key=counter.get, reverse=True)[:vocab_size]\n",
    "vocab = {word: i for i, word in enumerate(most_frequent_words, 1)}\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(sentence, vocab):\n",
    "  #AQUI↓################################\n",
    "  sentence = clean_text(sentence)\n",
    "  #AQUI↑################################\n",
    "\n",
    "  return [vocab.get(word, 0) for word in sentence.split()] # 0 for OOV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "accuracy = 0\n",
    "\n",
    "lr = lr_opt\n",
    "\n",
    "for _ in range(5):\n",
    "  model = OneHotMLP(vocab_size)\n",
    "  train(model, lr)\n",
    "  accuracy += evaluate(model)\n",
    "\n",
    "accuracy /= 5\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.2.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for inputs, targets in train_loader:\n",
    " i +=1 \n",
    "\n",
    "print(\"Iterações:\", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.2.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for inputs, targets in train_loader:\n",
    "  i +=1\n",
    "  last_size = len(inputs)\n",
    "\n",
    "last_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.2.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = [int(sum(targets))/len(targets) for  _, targets in train_loader]\n",
    "\n",
    "R_mean = np.mean(R)\n",
    "\n",
    "R_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.2.d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Entrada\", batch[0].shape, batch[0].dtype)\n",
    "print(\"Saída\", batch[1].shape, batch[1].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.3.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_size in [1, 128, 512]:\n",
    "  # define dataloaders\n",
    "  train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "  test_loader  = DataLoader(test_data,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "  model = OneHotMLP(vocab_size)\n",
    "  model = model.to(device)\n",
    "\n",
    "  # Define loss and optimizer\n",
    "  criterion = nn.BCEWithLogitsLoss()\n",
    "  optimizer = optim.SGD(model.parameters(), lr=lr_opt)\n",
    "\n",
    "  # Training loop\n",
    "  num_epochs = 5\n",
    "  for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "\n",
    "      inputs = inputs.to(device)\n",
    "      targets = targets.to(device)\n",
    "\n",
    "      logits = model(inputs)\n",
    "\n",
    "      if batch_size == 1:\n",
    "        loss = criterion(logits[0], targets.float())\n",
    "      else:\n",
    "        loss = criterion(logits.squeeze(), targets.float())\n",
    "\n",
    "\n",
    "      # Backward and optimize\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "    if epoch+1 in [1,5]:\n",
    "      print(f\"batch_size {batch_size} | epoch {epoch+1} | loss {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.1.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OneHotMLP(vocab_size)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "(inputs, targets) = batch\n",
    "\n",
    "logits  = model(inputs)\n",
    "\n",
    "print(torch.sigmoid(logits).squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.1.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "(inputs, targets) = batch\n",
    "\n",
    "logits  = model(inputs)\n",
    "predicted = torch.round(torch.sigmoid(logits.squeeze()))\n",
    "total = targets.size(0)\n",
    "correct = (predicted == targets).sum().item()\n",
    "\n",
    "print(f'Accuracy: {100 * correct / total}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.1.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "for layer in [model.fc1, model.fc2]:\n",
    "  print(np.prod(layer.weight.shape))\n",
    "  print(np.prod(layer.bias.shape))\n",
    "\n",
    "  total += np.prod(layer.weight.shape) + np.prod(layer.bias.shape)\n",
    "\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.1.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.BCELoss()\n",
    "\n",
    "output = torch.tensor([0.5, 0.5], dtype=torch.float64)\n",
    "target = torch.tensor([0, 1], dtype=torch.float64)\n",
    "\n",
    "print(float(loss(output, target)), np.log(0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.1.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(input,target) = next(iter(train_loader))\n",
    "\n",
    "model = OneHotMLP(vocab_size)\n",
    "\n",
    "output = model(input)\n",
    "\n",
    "prob = torch.sigmoid(output.squeeze())\n",
    "\n",
    "L = torch.sum(\n",
    "    (target*torch.log(prob)) + \n",
    "     ((1-target)*torch.log(1-prob))\n",
    "     )\n",
    "\n",
    "L *= -1/(len(target))\n",
    "\n",
    "print(L.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V.1.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(input,target) = next(iter(train_loader))\n",
    "\n",
    "model = OneHotMLP(vocab_size)\n",
    "\n",
    "logits = model(input)\n",
    "predicted = torch.sigmoid(logits.squeeze())\n",
    "\n",
    "loss = nn.BCELoss()\n",
    "\n",
    "print(loss(predicted, target.float()).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.1.d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(input,target) = next(iter(train_loader))\n",
    "\n",
    "model = OneHotMLP(vocab_size)\n",
    "\n",
    "logits = model(input)\n",
    "\n",
    "loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "print(loss(logits.squeeze(), target.float()).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.2.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "model = OneHotMLP(vocab_size)\n",
    "model = model.to(device)\n",
    "# Define loss and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr_opt)\n",
    "\n",
    "total_loss = 0\n",
    "n = 0\n",
    "for inputs, targets in train_loader:\n",
    "  inputs = inputs.to(device)\n",
    "  targets = targets.to(device)\n",
    "  logits = model(inputs)\n",
    "  loss = criterion(logits.squeeze(), targets.float())\n",
    "  total_loss += loss.item()\n",
    "  n+=1\n",
    "\n",
    "print(f\"Total Loss: {total_loss/n:.4f}\")\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()  # Start time of the epoch\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        # Forward pass\n",
    "        logits = model(inputs)\n",
    "        loss = criterion(logits.squeeze(), targets.float())\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    end_time = time.time()  # End time of the epoch\n",
    "    epoch_duration = end_time - start_time  # Duration of epoch\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], \\\n",
    "            Loss: {loss.item():.4f}, \\\n",
    "            Elapsed Time: {epoch_duration:.2f} sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI.1.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI.1.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = inputs.to(device)\n",
    "targets = targets.to(device)\n",
    "logits = model(inputs)\n",
    "\n",
    "predicted = torch.round(torch.sigmoid(logits.squeeze()))\n",
    "predicted2 = (logits.squeeze() > 0).float()\n",
    "\n",
    "equal = (predicted != predicted2).sum() == 0\n",
    "equal = equal.item()\n",
    "\n",
    "print(\"É igual?\", equal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit (logits.squeeze() > 0).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit predicted = torch.round(torch.sigmoid(logits.squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit (logits.squeeze() > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI.2.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.BCELoss()\n",
    "\n",
    "output = torch.tensor([0.5, 0.5], dtype=torch.float64)\n",
    "target = torch.tensor([0, 1], dtype=torch.float64)\n",
    "\n",
    "np.exp(float(loss(output, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI.2.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "N_hist = list(range(1, 10))\n",
    "ppl_hist = []\n",
    "\n",
    "for N in N_hist:\n",
    "\n",
    "  output = torch.ones(N, dtype=torch.float)\n",
    "  \n",
    "  #Simétrico+média -> apenas 1 classe é necessário\n",
    "  target = torch.tensor(0, dtype=torch.int64) \n",
    "\n",
    "  l = loss(output, target)\n",
    "  ppl = torch.exp(l)\n",
    "\n",
    "  ppl_hist.append(ppl.item())\n",
    "\n",
    "N_hist = np.array(N_hist)\n",
    "ppl_hist = np.array(ppl_hist)\n",
    "\n",
    "print(ppl_hist)\n",
    "print(1/N_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI.3.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        logits = model(inputs)\n",
    "        predicted = torch.round(torch.sigmoid(logits.squeeze()))\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "        total_loss += criterion(logits.squeeze(), targets.float())*targets.size(0)\n",
    "\n",
    "    total_loss /= total\n",
    "    PPL = torch.exp(total_loss)\n",
    "\n",
    "\n",
    "    print(f'Test Accuracy: {100 * correct / total}%')\n",
    "    print(\"Perplexity:\", PPL.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
