{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMI0JT_YuYF3"
      },
      "source": [
        "## Exercício: Modelo de Linguagem (Bengio 2003) - MLP + Embeddings\n",
        "\n",
        "Neste exercício iremos treinar uma rede neural similar a do Bengio 2003 para prever a próxima palavra de um texto, data as palavras anteriores como entrada. Esta tarefa é chamada de \"Modelagem da Linguagem\".\n",
        "\n",
        "Portanto, você deve implementar o modelo de linguagem inspirado no artigo do Bengio, para prever a próxima palavra usando rede com embeddings e duas camadas.\n",
        "Sugestão de alguns parâmetros:\n",
        "* context_size = 9\n",
        "* max_vocab_size = 3000\n",
        "* embedding_dim = 64\n",
        "* usar pontuação no vocabulário\n",
        "* descartar qualquer contexto ou target que não esteja no vocabulário\n",
        "* É esperado conseguir uma perplexidade da ordem de 50.\n",
        "* Procurem fazer asserts para garantir que partes do seu programa estão testadas\n",
        "\n",
        "Este enunciado não é fixo, podem mudar qualquer um dos parâmetros acima, mas procurem conseguir a perplexidade esperada ou menor.\n",
        "\n",
        "Gerem alguns frases usando um contexto inicial e depois deslocando o contexto e prevendo a próxima palavra gerando frases compridas para ver se está gerando texto plausível.\n",
        "\n",
        "Algumas dicas:\n",
        "- Inclua caracteres de pontuação (ex: `.` e `,`) no vocabulário.\n",
        "- Deixe tudo como caixa baixa (lower-case).\n",
        "- A escolha do tamanho do vocabulario é importante: ser for muito grande, fica difícil para o modelo aprender boas representações. Se for muito pequeno, o modelo apenas conseguirá gerar textos simples.\n",
        "- Remova qualquer exemplo de treino/validação/teste que tenha pelo menos um token desconhecido (ou na entrada ou na saída).\n",
        "- Durante a depuração, faça seu dataset ficar bem pequeno, para que a depuração seja mais rápida e não precise de GPU. Somente ligue a GPU quando o seu laço de treinamento já está funcionando\n",
        "- Não deixe para fazer esse exercício na véspera. Ele é trabalhoso.\n",
        "\n",
        "Procure por `TODO` para entender onde você precisa inserir o seu código."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import string\n",
        "from collections import Counter # Conta as palavras no dataset\n",
        "import re\n",
        "from typing import List, Dict, Union\n",
        "import random\n",
        "\n",
        "import numba\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "random.seed(18)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYbkEzdD37sZ"
      },
      "source": [
        "## Faz download e carrega o dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qAnqY_q0beK",
        "outputId": "f810fdb0-138d-4917-b7ef-69ab266acef6"
      },
      "outputs": [],
      "source": [
        "!wget https://www.gutenberg.org/ebooks/67724.txt.utf-8\n",
        "!wget https://www.gutenberg.org/ebooks/67725.txt.utf-8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100   304  100   304    0     0    556      0 --:--:-- --:--:-- --:--:--   558\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "\n",
            " 11  364k   11 42594    0     0  34836      0  0:00:10  0:00:01  0:00:09 34836\n",
            "100  364k  100  364k    0     0   194k      0  0:00:01  0:00:01 --:--:--  495k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100   304  100   304    0     0    556      0 --:--:-- --:--:-- --:--:--   558\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "\n",
            " 25  337k   25 88902    0     0  70621      0  0:00:04  0:00:01  0:00:03 70621\n",
            "100  337k  100  337k    0     0   189k      0  0:00:01  0:00:01 --:--:--  486k\n"
          ]
        }
      ],
      "source": [
        "!curl -LO https://www.gutenberg.org/ebooks/67724.txt.utf-8\n",
        "!curl -LO https://www.gutenberg.org/ebooks/67725.txt.utf-8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_UzC9pV091C",
        "outputId": "1553b04f-24c4-4027-8cab-0907f92f04df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4969"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = open(\"67724.txt.utf-8\",\"r\", encoding=\"utf8\").read()\n",
        "text += open(\"67725.txt.utf-8\",\"r\", encoding=\"utf8\").read()\n",
        "\n",
        "paragraphs = text.split(\"\\n\\n\")\n",
        "len(paragraphs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhUFjtNdDuG0",
        "outputId": "78798c0c-deca-4454-d3fb-7d3ba70f3e91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SAMPLE ----------------\n",
            "﻿the project gutenberg ebook of o guarany : romance brazileiro , vol . 999 ( of 999 ) this ebook is for the use of anyone anywhere in the united states and most other parts of the world at no cost and with almost no restrictions whatsoever . you may copy it , give it away or re-use it under the terms of the project gutenberg license included with this ebook or online at www.gutenberg.org . if you are not located in the united states , you\n",
            "---------------------\n",
            "4892\n"
          ]
        }
      ],
      "source": [
        "cleaned_paragraphs = [paragraph.replace(\"\\n\", \" \") for paragraph in paragraphs if paragraph.strip()]\n",
        "\n",
        "#Paper:\n",
        "#ponctuation -> keep (separado das outras palavras, \"pontuação,\" -> \"pontuação\"+\",\")\n",
        "#numeric -> special symbol (colocando todos como 999 para convergir para o mesmo símbolo)\n",
        "#upper -> lower\n",
        "#proper nouns -> special symbol (difícil identificar, ignorado)\n",
        "#rare words -> special symbol (feito na parte de encoding)\n",
        "\n",
        "cleaned_paragraphs = [cleaned_paragraph.lower() for cleaned_paragraph in cleaned_paragraphs]\n",
        "\n",
        "number_counter = Counter()\n",
        "\n",
        "\n",
        "for i in range(len(cleaned_paragraphs)):\n",
        "    old_p = cleaned_paragraphs[i].split()\n",
        "    new_p = []\n",
        "\n",
        "    for j in range(len(old_p)):\n",
        "        word = old_p[j] \n",
        "        if word.isdigit():\n",
        "            number_counter.update(word)\n",
        "            word = \"999\"\n",
        "        elif len(word) > 1 and word[0] in string.punctuation:\n",
        "            old_p.insert(j+1, word[1:])\n",
        "            word = word[0]\n",
        "        elif word[-1] in string.punctuation and len(word) > 1:\n",
        "            old_p.insert(j+1, word[:-1])\n",
        "            old_p.insert(j+2, word[-1])\n",
        "            \n",
        "            word = \"\"\n",
        "        \n",
        "        if len(word) > 0:\n",
        "            new_p.append(word)\n",
        "        #j += 1\n",
        "    \n",
        "    cleaned_paragraphs[i] = \" \".join(new_p)\n",
        "\n",
        "print(\"SAMPLE ----------------\")\n",
        "print(cleaned_paragraphs[0])\n",
        "print(\"---------------------\")\n",
        "\n",
        "print(len(cleaned_paragraphs))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Counter({'1': 37,\n",
              "         '2': 13,\n",
              "         '8': 23,\n",
              "         '7': 7,\n",
              "         '3': 11,\n",
              "         '5': 23,\n",
              "         '6': 8,\n",
              "         '0': 31,\n",
              "         '4': 7,\n",
              "         '9': 10})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "number_counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "del paragraphs, number_counter, new_p, old_p, text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFVN2ihb33Rf"
      },
      "source": [
        "## Análise do dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSRHqe3H4ZFw",
        "outputId": "4a985c7a-ce1d-4b72-d253-c9fbbc5f9440"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "11470"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "def count_words(texts):\n",
        "    word_counts = Counter()\n",
        "    for text in texts:\n",
        "        #Regular expression removes ponctuation\n",
        "        #word_counts.update(re.findall(r'\\w+', text.lower())) \n",
        "        word_counts.update(text.split(\" \"))\n",
        "    return word_counts\n",
        "\n",
        "word_counts = count_words(cleaned_paragraphs)\n",
        "\n",
        "len(word_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyGVDL9KzJ_I"
      },
      "source": [
        "## Criando um vocabulário"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FiP7OCo9zJ_I"
      },
      "outputs": [],
      "source": [
        "vocab_size = 10000 #3000#\n",
        "most_frequent_words = [word for word, count in word_counts.most_common(vocab_size)]\n",
        "vocab = {word: i for i, word in enumerate(most_frequent_words, 1)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def encode_sentence(sentence:Union[str,List[str]], vocab:Dict) -> List[int]:\n",
        "    if isinstance(sentence, list):\n",
        "        words = sentence\n",
        "    else:\n",
        "        words = sentence.split(\" \") #Removido o regex por não pegar pontuação e ser ~3x mais lento\n",
        "    \n",
        "    return [vocab.get(word, 0) for word in words]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Checando se o encoding faz sentido.\n",
        "\n",
        "As palavras mais frequentes são pontuações, potencialmente problemático."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20 palavras mais frequentes: [',', 'a', 'que', '-', 'o', 'de', 'e', ';', '.', 'um', 'do', 'não', 'uma', 'os', 'se', 'da', 'com', 'sua', 'para', 'seu']\n"
          ]
        }
      ],
      "source": [
        "print(\"20 palavras mais frequentes:\", most_frequent_words[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "publicando 5496\n",
            "este 126\n",
            "livro 3463\n",
            "em 21\n",
            "999 153\n",
            ", 1\n",
            "se 15\n",
            "disse 57\n",
            "ser 122\n",
            "aquella 221\n",
            "primeira 197\n",
            "edição 2103\n",
            "uma 13\n",
            "prova 960\n",
            "typographica 5497\n",
            ", 1\n",
            "que 3\n",
            "algum 192\n",
            "dia 134\n",
            "talvez 281\n",
            "o 5\n",
            "autor 2105\n"
          ]
        }
      ],
      "source": [
        "encoded20 = encode_sentence(cleaned_paragraphs[20], vocab)\n",
        "words = cleaned_paragraphs[20].split(\" \")\n",
        "\n",
        "for i in range(len(words)):\n",
        "    print(words[i], encoded20[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'most_frequent_words' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-14-ef4b475a78a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mdel\u001b[0m \u001b[0mword_counts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmost_frequent_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoded20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m: name 'most_frequent_words' is not defined"
          ]
        }
      ],
      "source": [
        "del word_counts, most_frequent_words, encoded20, words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wia_ygbvzJ_J"
      },
      "source": [
        "## Classe do dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Iy-elI1magRR"
      },
      "outputs": [],
      "source": [
        "context_size = 5 # 5 palavras de entrada. O target é a próxima palavra\n",
        "\n",
        "x_all = []\n",
        "y_all = []\n",
        "\n",
        "for paragraph in cleaned_paragraphs:\n",
        "    start = 0\n",
        "    end = context_size\n",
        "\n",
        "    paragraph = encode_sentence(paragraph, vocab)\n",
        "\n",
        "    while end < len(paragraph):\n",
        "        x = paragraph[start:end]\n",
        "        y = paragraph[end]\n",
        "\n",
        "        if not ( 0 in x or 0 == y):\n",
        "            x_all.append(x)\n",
        "            y_all.append(y)\n",
        "\n",
        "        start += 1\n",
        "        end += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "del paragraph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Checando se o dataset está correto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3440, 68, 214, 493, 48, 5, 684, 44, 866, 867, 1, 3441, 9, 153, 348, 48, 153, 383, 145, 493, 309, 262, 37, 494, 48, 957, 2095, 106, 37, 411, 329, 92, 958, 412, 2096, 48, 37, 2097, 645, 24, 1492, 92, 136, 2098, 24, 2099, 2100, 9, 82, 446, 495, 384, 1, 1493, 384, 2101, 93, 2102, 384, 802, 37, 276, 48, 37, 68, 214, 447, 1494, 136, 145, 493, 93, 1495, 645, 1496, 9, 263, 82, 252, 215, 803, 106, 37, 411, 329, 1, 82]\n",
            "[3440, 68, 214, 493, 48] | 5\n",
            "[68, 214, 493, 48, 5] | 684\n",
            "[214, 493, 48, 5, 684] | 44\n"
          ]
        }
      ],
      "source": [
        "print(encode_sentence(cleaned_paragraphs[0], vocab))\n",
        "\n",
        "for i in range(3):\n",
        "    print(x_all[i], \"|\", y_all[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "74741"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(x_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "assert len(x_all) == len(y_all)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Divisão treino|validação|teste\n",
        "\n",
        "60%|20%|20%\n",
        "\n",
        "OBS: seed determinada no início do notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Embaralhando para evitar viés\n",
        "indexes = list(range(len(x_all)))\n",
        "random.shuffle(indexes)\n",
        "\n",
        "x_all = np.array(x_all)\n",
        "y_all = np.array(y_all)\n",
        "\n",
        "x_all = x_all[indexes]\n",
        "y_all = y_all[indexes]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "size_all = len(x_all)\n",
        "\n",
        "cut1 = int(0.6*size_all)\n",
        "cut2 = int(0.8*size_all)\n",
        "\n",
        "x_train = x_all[0:cut1]\n",
        "y_train = y_all[0:cut1]\n",
        "\n",
        "x_val = x_all[cut1:cut2]\n",
        "y_val = y_all[cut1:cut2]\n",
        "\n",
        "x_test = x_all[cut2:]\n",
        "y_test = y_all[cut2:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "assert len(x_train)+len(x_val)+len(x_test) == size_all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Classe para o dataset\n",
        "\n",
        "OBS: utilizar tensores q [context_size x vocab_size] esparsos utiliza muita memória, preferi alterar a primeira camada para evitar precisar gerar estes tensores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TextPredictDataset(Dataset):\n",
        "    def __init__(self, x_data:List[int], y_data:List[int]):\n",
        "        self._x_data = torch.tensor(x_data)\n",
        "        self._y_data = torch.tensor(y_data)\n",
        "        \n",
        "        if len(x_data) != len(y_data):\n",
        "            raise ValueError(f\"x_data and y_data must have same size. ({len(x_data)} ≠ {len(y_data)})\")\n",
        "        \n",
        "        self._size = len(x_data)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self._x_data[idx], self._y_data[idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aD1CVci2zJ_J",
        "outputId": "5bf0839e-f30e-4ff2-ed6f-4f3fda782b7c"
      },
      "outputs": [],
      "source": [
        "train_data = TextPredictDataset(x_train, y_train)\n",
        "val_data = TextPredictDataset(x_val, y_val)\n",
        "test_data = TextPredictDataset(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "gC0C5qn2zJ_J"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_batch = next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5_-Yud0zJ_K"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "I2qKG9YczJ_K"
      },
      "outputs": [],
      "source": [
        "\n",
        "class LanguageModel(nn.Module):\n",
        "    \"\"\"TODO: implementar o modelo de linguagem\"\"\"\n",
        "\n",
        "    def __init__(self, context_size:int, vocab_size:int, embed_dim:int, hidden_units:int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.C = torch.Tensor(vocab_size, embed_dim)\n",
        "        nn.init.xavier_uniform_(self.C)\n",
        "        self.C = torch.nn.Parameter(self.C)\n",
        "        \n",
        "        \n",
        "        #V = |Vocab|, m = |Embed|\n",
        "        #n-1 = c = |Context|\n",
        "        #h = |Hidden|\n",
        "        \n",
        "        #C[V, m](input) -> x[c*m]\n",
        "        #Linear1(x) -> x2[h]\n",
        "        #ReLU(x2) -> x3[h]  | alterado do paper (tanh)\n",
        "        #Linear2(x) -> x4[V]\n",
        "        #Linear3(x3) -> x5[V] | sem bias (Linear2 já tem bias)\n",
        "        #Add(x4, x5) -> output\n",
        "        #Sem softmax -> melhor estabilidade\n",
        "\n",
        "        m = int(context_size*embed_dim)\n",
        "\n",
        "        self.linear1 = nn.Linear(m, hidden_units)\n",
        "        self.relu = nn.ReLU() \n",
        "        self.linear2 = nn.Linear(m, vocab_size)\n",
        "        self.linear3 = nn.Linear(hidden_units, vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, input_x:torch.Tensor) -> torch.Tensor:\n",
        "        #No batch: x = torch.index_select(self.C, 0, input_x).flatten()\n",
        "        x = torch.stack([torch.index_select(self.C, 0, input_i).flatten() for input_i in input_x])\n",
        "        \n",
        "        x2 = self.linear1(x)\n",
        "        x3 = self.relu(x2)\n",
        "        x4 = self.linear2(x)\n",
        "        x5 = self.linear3(x3)\n",
        "\n",
        "        output = x4+x5\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "7yjQ1KXOzJ_K"
      },
      "outputs": [],
      "source": [
        "embed_dim = 64\n",
        "hidden_units = 300\n",
        "model = LanguageModel(context_size, vocab_size, embed_dim, hidden_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "xmsD59TfzJ_K"
      },
      "outputs": [],
      "source": [
        "# sample = next(iter(train_loader))\n",
        "input = sample_batch[0]\n",
        "target = sample_batch[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "HGbJcT5KzJ_K"
      },
      "outputs": [],
      "source": [
        "output = model(input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "um0lR4mNzJ_K",
        "outputId": "e6041da8-ca7f-4c9d-b28b-9e1250e820f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([2656, 2773, 9901, 9799, 2881, 3629, 3690, 2576, 6445,  515, 3629, 9799,\n",
              "        7575, 1506, 9901, 7575, 6302, 2656, 2881, 6445, 6445, 6445, 7474, 8238,\n",
              "         515, 3629, 6721, 9799, 5800, 6302, 9799, 9901])"
            ]
          },
          "execution_count": 135,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output.argmax(dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "la-b-f8jzJ_L",
        "outputId": "f040cef4-e409-4d20-d335-a3133aaeb63c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([   7,    8,   65,  112,  869,  238,    1,  144,  176,  164,   22,    3,\n",
              "           1, 4044,    5,  117,    2,   24, 1084,  676,  394,    2,   14,  817,\n",
              "         108,    1,    7, 2807,    6,    8,  908,    1], dtype=torch.int32)"
            ]
          },
          "execution_count": 136,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [],
      "source": [
        "assert output.argmax(dim=1).shape == target.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [],
      "source": [
        " #V = |Vocab|, m = |Embed|\n",
        "        #n-1 = c = |Context|\n",
        "        #h = |Hidden|\n",
        "\n",
        "n_param_real = sum([p.numel() for p in model.parameters()])\n",
        "\n",
        "m = embed_dim\n",
        "n = context_size+1\n",
        "V = vocab_size\n",
        "h = hidden_units\n",
        "\n",
        "n_param_theoretical = V*(1+(n*m)+h)\n",
        "n_param_theoretical += h*(1+((n-1)*m))\n",
        "\n",
        "assert n_param_real == n_param_theoretical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6946300 parâmetros\n"
          ]
        }
      ],
      "source": [
        "print(n_param_real, \"parâmetros\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UngUhyu7zJ_L"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wntaV50nzJ_L",
        "outputId": "a054092b-d801-4c60-eb75-85abfe57151d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 140,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Verifica se há uma GPU disponível e define o dispositivo para GPU se possível, caso contrário, usa a CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRwSPiwizJ_L"
      },
      "outputs": [],
      "source": [
        "epochs = 10\n",
        "lr = \"\"\"TODO\"\"\"\"\n",
        "criterion = \"\"\"TODO CrossEntropy\"\"\"\"\n",
        "\n",
        "optimizer = \"\"\"TODO: AdamW ou outro\"\"\"\"\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "\"\"\"TODO: Implemente o loop de treinamento. Em cada época, calcule e imprima a loss no dataset de validação\"\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSXfwYISDoPN"
      },
      "source": [
        "## Avaliação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXXO78GSDqPg"
      },
      "outputs": [],
      "source": [
        "\"\"\" TODO: calcule a perplexidade final no dataset de validação \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1zhxVqfzJ_M"
      },
      "source": [
        "## Exemplo de uso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PExkoWOzJ_M"
      },
      "outputs": [],
      "source": [
        "text = \"\"\n",
        "\n",
        "def generate_text(model, vocab, text, max_length):\n",
        "    \"\"\"TODO: implemente a função para gerar texto até atingir o max_length\"\"\"\n",
        "\n",
        "context = 5\n",
        "max_length= 10\n",
        "generate_text(text, max_length)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
