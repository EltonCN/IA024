{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMI0JT_YuYF3"
      },
      "source": [
        "## Exercício: Modelo de Linguagem com auto-atenção\n",
        "\n",
        "Este exercício é similar ao da aula passada, mas iremos agora treinar uma rede neural *com auto-atenção* para prever a próxima palavra de um texto, data as palavras anteriores como entrada.\n",
        "\n",
        "Na camada de auto-atenção, deve-se implementar (vide slide 34):\n",
        "- Embeddings de posição\n",
        "- Projeções lineares (WQ, WK, WV, WO)\n",
        "- Camada de feed forward (2-layer MLP)\n",
        "\n",
        "Instrucões:\n",
        "- É necessário fazer duas implementações da camada de auto-atenção: uma usando laços (ineficiente, mas fácil de entender) e outra matricial (eficiente mas difícil de entender). Usar slide 36 como referência.\n",
        "\n",
        "- Fazer um assert para garantir que o resultado das duas implementações é exatamente igual.\n",
        "\n",
        "- No treinamento, usar apenas a implementação matricial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 314,
      "metadata": {},
      "outputs": [],
      "source": [
        "import string\n",
        "from collections import Counter\n",
        "from typing import List, Dict, Union, Tuple\n",
        "import random\n",
        "import os\n",
        "import time\n",
        "import abc\n",
        "\n",
        "import numpy as np\n",
        "from numpy.testing import assert_raises, assert_array_equal, assert_array_almost_equal\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tqdm\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab_size = 3000 # Quantidade de palavras no vocabulário\n",
        "context_size = 4 # n palavras de entrada. O target é a próxima palavra\n",
        "embed_dim = 64 # Tamanho do feature vector de cada palavra\n",
        "hidden_units = 300 # Quantidade de unidades na camada escondida\n",
        "epochs = 10 # Quantidade de epochs que serão treinadas\n",
        "lr = 5e-1 # Taxa de treinamento\n",
        "weight_decay = 1e-3 # Regularização\n",
        "batch_size = 32\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYbkEzdD37sZ"
      },
      "source": [
        "## Faz download e carrega o dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qAnqY_q0beK",
        "outputId": "f810fdb0-138d-4917-b7ef-69ab266acef6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100   304  100   304    0     0    450      0 --:--:-- --:--:-- --:--:--   452\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "\n",
            " 11  364k   11 42594    0     0  31946      0  0:00:11  0:00:01  0:00:10 31946\n",
            "100  364k  100  364k    0     0   183k      0  0:00:01  0:00:01 --:--:--  495k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100   304  100   304    0     0    533      0 --:--:-- --:--:-- --:--:--   535\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "\n",
            " 25  337k   25 88902    0     0  68973      0  0:00:05  0:00:01  0:00:04 68973\n",
            "100  337k  100  337k    0     0   183k      0  0:00:01  0:00:01 --:--:--  461k\n"
          ]
        }
      ],
      "source": [
        "if not os.path.isfile(\"67724.txt.utf-8\"):\n",
        "    !curl -LO https://www.gutenberg.org/ebooks/67724.txt.utf-8\n",
        "\n",
        "if not os.path.isfile(\"67725.txt.utf-8\"):\n",
        "    !curl -LO https://www.gutenberg.org/ebooks/67725.txt.utf-8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_UzC9pV091C",
        "outputId": "1553b04f-24c4-4027-8cab-0907f92f04df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4969"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = open(\"67724.txt.utf-8\",\"r\", encoding=\"utf8\").read()\n",
        "text += open(\"67725.txt.utf-8\",\"r\", encoding=\"utf8\").read()\n",
        "\n",
        "paragraphs = text.split(\"\\n\\n\")\n",
        "len(paragraphs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_text(text:str) -> str:\n",
        "    '''\n",
        "    Clean the text, changing upper case and setting numbers to 999\n",
        "    '''\n",
        "    \n",
        "    text = text.lower()\n",
        "    old_text = text.split()\n",
        "    new_text = []\n",
        "\n",
        "    for j in range(len(old_text)):\n",
        "        word = old_text[j] \n",
        "        if word.isdigit():\n",
        "            word = \"999\"\n",
        "        elif len(word) > 1 and word[0] in string.punctuation:\n",
        "            old_text.insert(j+1, word[1:])\n",
        "            word = word[0]\n",
        "        elif word[-1] in string.punctuation and len(word) > 1:\n",
        "            old_text.insert(j+1, word[:-1])\n",
        "            old_text.insert(j+2, word[-1])\n",
        "            \n",
        "            word = \"\"\n",
        "        \n",
        "        if len(word) > 0:\n",
        "            new_text.append(word)\n",
        "    \n",
        "    return \" \".join(new_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhUFjtNdDuG0",
        "outputId": "78798c0c-deca-4454-d3fb-7d3ba70f3e91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SAMPLE ----------------\n",
            "﻿the project gutenberg ebook of o guarany : romance brazileiro , vol . 999 ( of 999 ) this ebook is for the use of anyone anywhere in the united states and most other parts of the world at no cost and with almost no restrictions whatsoever . you may copy it , give it away or re-use it under the terms of the project gutenberg license included with this ebook or online at www.gutenberg.org . if you are not located in the united states , you\n",
            "---------------------\n",
            "4892\n"
          ]
        }
      ],
      "source": [
        "cleaned_paragraphs = [paragraph.replace(\"\\n\", \" \") for paragraph in paragraphs if paragraph.strip()]\n",
        "\n",
        "#Paper:\n",
        "#ponctuation -> keep (separado das outras palavras, \"pontuação,\" -> \"pontuação\"+\",\")\n",
        "#numeric -> special symbol (colocando todos como 999 para convergir para o mesmo símbolo)\n",
        "#upper -> lower\n",
        "#proper nouns -> special symbol (difícil identificar, ignorado)\n",
        "#rare words -> special symbol (feito na parte de encoding)\n",
        "\n",
        "for i in range(len(cleaned_paragraphs)):\n",
        "    cleaned_paragraphs[i] = clean_text(cleaned_paragraphs[i])\n",
        "\n",
        "print(\"SAMPLE ----------------\")\n",
        "print(cleaned_paragraphs[0])\n",
        "print(\"---------------------\")\n",
        "\n",
        "print(len(cleaned_paragraphs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "del paragraphs, text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFVN2ihb33Rf"
      },
      "source": [
        "## Análise do dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSRHqe3H4ZFw",
        "outputId": "4a985c7a-ce1d-4b72-d253-c9fbbc5f9440"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "11470"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "def count_words(texts:List[str]) -> Counter:\n",
        "    word_counts = Counter()\n",
        "    for text in texts:\n",
        "        word_counts.update(text.split(\" \"))\n",
        "    return word_counts\n",
        "\n",
        "word_counts = count_words(cleaned_paragraphs)\n",
        "\n",
        "len(word_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyGVDL9KzJ_I"
      },
      "source": [
        "## Criando um vocabulário"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "FiP7OCo9zJ_I"
      },
      "outputs": [],
      "source": [
        "most_frequent_words = [word for word, count in word_counts.most_common(vocab_size)]\n",
        "vocab = {word: i for i, word in enumerate(most_frequent_words, 1)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def encode_sentence(sentence:Union[str,List[str]], vocab:Dict) -> List[int]:\n",
        "    if isinstance(sentence, list):\n",
        "        words = sentence\n",
        "    else:\n",
        "        words = sentence.split(\" \")\n",
        "    \n",
        "    return [vocab.get(word, 0) for word in words]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "inverse_vocab = list(vocab.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def decode_sentence(encoding, inverse_vocab):\n",
        "    result = []\n",
        "\n",
        "    for encoding_i in encoding:\n",
        "        if encoding_i == 0:\n",
        "            result.append(\"???\")\n",
        "        else:\n",
        "            result.append(inverse_vocab[encoding_i-1])\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "del word_counts, most_frequent_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wia_ygbvzJ_J"
      },
      "source": [
        "## Classe do dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_sequences(texts:List[str], context_size:int, \n",
        "                     vocab:Dict) -> Tuple[List[List[int]], List[int]]:\n",
        "    '''\n",
        "    Generates\n",
        "    '''\n",
        "    x_all = []\n",
        "    y_all = []\n",
        "\n",
        "    for paragraph in texts:\n",
        "        start = 0\n",
        "        end = context_size\n",
        "\n",
        "        paragraph = encode_sentence(paragraph, vocab)\n",
        "\n",
        "        while end < len(paragraph):\n",
        "            x = paragraph[start:end]\n",
        "            y = paragraph[end]\n",
        "\n",
        "            if not ( 0 in x or 0 == y):\n",
        "                x_all.append(x)\n",
        "                y_all.append(y)\n",
        "\n",
        "            start += 1\n",
        "            end += 1\n",
        "    return x_all, y_all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_all, y_all = create_sequences(cleaned_paragraphs, context_size, vocab)\n",
        "assert len(x_all) == len(y_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "gC0C5qn2zJ_J"
      },
      "outputs": [],
      "source": [
        "#Embaralhando para evitar viés\n",
        "indexes = list(range(len(x_all)))\n",
        "random.shuffle(indexes)\n",
        "\n",
        "x_all = np.array(x_all)\n",
        "y_all = np.array(y_all)\n",
        "\n",
        "x_all = x_all[indexes]\n",
        "y_all = y_all[indexes]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "size_all = len(x_all)\n",
        "\n",
        "cut1 = int(0.6*size_all)\n",
        "cut2 = int(0.8*size_all)\n",
        "\n",
        "x_train = x_all[0:cut1]\n",
        "y_train = y_all[0:cut1]\n",
        "\n",
        "x_val = x_all[cut1:cut2]\n",
        "y_val = y_all[cut1:cut2]\n",
        "\n",
        "x_test = x_all[cut2:]\n",
        "y_test = y_all[cut2:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_train = len(x_train)\n",
        "n_val = len(x_val)\n",
        "n_test = len(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Treino: 27219\n",
            "Validação: 9073\n",
            "Teste: 9074\n"
          ]
        }
      ],
      "source": [
        "print(\"Treino:\", n_train)\n",
        "print(\"Validação:\", n_val)\n",
        "print(\"Teste:\", n_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "assert n_train+n_val+n_test == size_all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TextPredictDataset(Dataset):\n",
        "    def __init__(self, x_data:List[int], y_data:List[int]):\n",
        "        self._x_data = torch.tensor(x_data)-1\n",
        "        self._y_data = torch.tensor(y_data, dtype=torch.int64)-1\n",
        "        \n",
        "        if len(x_data) != len(y_data):\n",
        "            raise ValueError(f\"x_data and y_data must have same size. ({len(x_data)} ≠ {len(y_data)})\")\n",
        "        \n",
        "        self._size = len(x_data)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self._x_data[idx], self._y_data[idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data = TextPredictDataset(x_train, y_train)\n",
        "val_data = TextPredictDataset(x_val, y_val)\n",
        "test_data = TextPredictDataset(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_batch = next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5_-Yud0zJ_K"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SingleHeadAttentionBase(torch.nn.Module, abc.ABC):\n",
        "    def __init__(self, embed_dim:int) -> None:\n",
        "        super().__init__()\n",
        "        \n",
        "        #d_model = dv = dk = embed_dim\n",
        "        #h = 1\n",
        "\n",
        "        self.wQ = torch.Tensor(embed_dim, embed_dim)\n",
        "        self.wK = torch.Tensor(embed_dim, embed_dim)\n",
        "        self.wV = torch.Tensor(embed_dim, embed_dim)\n",
        "        self.w0 = torch.Tensor(embed_dim, embed_dim)\n",
        "\n",
        "        self.wQ = torch.nn.Parameter(self.wQ)\n",
        "        self.wK = torch.nn.Parameter(self.wK)\n",
        "        self.wV = torch.nn.Parameter(self.wV)\n",
        "        self.w0 = torch.nn.Parameter(self.w0)\n",
        "\n",
        "        self.dk_root = torch.sqrt(torch.tensor(embed_dim, dtype=torch.float32))\n",
        "\n",
        "        for w in [self.wQ, self.wK, self.wV, self.w0]:\n",
        "            torch.nn.init.xavier_uniform_(w)\n",
        "    \n",
        "    @abc.abstractmethod\n",
        "    def forward(self, query:torch.Tensor, key:torch.Tensor, value:torch.Tensor) -> torch.Tensor:\n",
        "        ...\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SingleHeadAttention(SingleHeadAttentionBase):\n",
        "\n",
        "    def __init__(self, embed_dim: int) -> None:\n",
        "        super().__init__(embed_dim)\n",
        "\n",
        "    def forward(self, query:torch.Tensor, key:torch.Tensor, value:torch.Tensor) -> torch.Tensor:\n",
        "        print(self.wQ.shape, query.shape)\n",
        "        \n",
        "        Q = query @ self.wQ\n",
        "        K = key @ self.wK\n",
        "        V = value @ self.wV\n",
        "\n",
        "\n",
        "        scores = Q @ K.permute(0,2,1)\n",
        "        scores /= self.dk_root\n",
        "        probs = torch.softmax(scores, dim=-1)\n",
        "        E = probs @ V\n",
        "\n",
        "        result = E @ self.w0\n",
        "\n",
        "        return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 247,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SingleHeadAttentionLoop(SingleHeadAttentionBase):\n",
        "    def __init__(self, embed_dim: int) -> None:\n",
        "        super().__init__(embed_dim)\n",
        "\n",
        "\n",
        "    def forward(self, query:torch.Tensor, key:torch.Tensor, value:torch.Tensor) -> torch.Tensor:\n",
        "        batch_size = query.shape[0]\n",
        "        sequence_size = query.shape[1]\n",
        "        \n",
        "        result = torch.empty_like(query)\n",
        "        scores = torch.empty(sequence_size, device=query.device)\n",
        "\n",
        "        for batch_index in range(batch_size):\n",
        "            for word_index in range(sequence_size):\n",
        "                xq = query[batch_index, word_index]\n",
        "                q = xq @ self.wQ\n",
        "                \n",
        "                for key_index in range(sequence_size):\n",
        "                    xk = key[batch_index][key_index]\n",
        "                    k = xk @ self.wK\n",
        "                    score = q @ k.T\n",
        "                    scores[key_index] = score\n",
        "                \n",
        "                probs = torch.softmax(scores, dim=-1)\n",
        "\n",
        "                e = 0\n",
        "                for xv, p in zip(value[batch_index], probs):\n",
        "                    v = xv @ self.wV\n",
        "                    e += v*p\n",
        "                \n",
        "                e = e @ self.w0\n",
        "\n",
        "                result[batch_index, word_index] = e\n",
        "\n",
        "        return result\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 328,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_embed_dim = 5\n",
        "matrix_version = SingleHeadAttention(test_embed_dim).eval()\n",
        "loop_version = SingleHeadAttentionLoop(test_embed_dim).eval()\n",
        "torch_version = torch.nn.MultiheadAttention(test_embed_dim, num_heads=1, bias=False, batch_first=True).eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 329,
      "metadata": {},
      "outputs": [],
      "source": [
        "wQ = matrix_version.wQ\n",
        "wK = wQ\n",
        "wV = wQ\n",
        "w0 = wQ\n",
        "\n",
        "#wQ = torch.nn.Parameter(torch.ones((test_embed_dim, test_embed_dim)))\n",
        "#wK = torch.nn.Parameter(torch.ones((test_embed_dim, test_embed_dim)))\n",
        "#wV = torch.nn.Parameter(torch.ones((test_embed_dim, test_embed_dim)))\n",
        "#w0 = torch.nn.Parameter(torch.ones((test_embed_dim, test_embed_dim)))\n",
        "\n",
        "#wQ = torch.nn.Parameter(torch.eye(test_embed_dim))\n",
        "#wK = torch.nn.Parameter(torch.eye(test_embed_dim))\n",
        "#wV = torch.nn.Parameter(torch.eye(test_embed_dim))\n",
        "#w0 = torch.nn.Parameter(torch.eye(test_embed_dim))\n",
        "\n",
        "matrix_version.wQ = wQ\n",
        "matrix_version.wK = wK\n",
        "matrix_version.wV = wV\n",
        "matrix_version.w0 = w0\n",
        "\n",
        "loop_version.wQ = wQ\n",
        "loop_version.wK = wK\n",
        "loop_version.wV = wV\n",
        "loop_version.w0 = w0\n",
        "\n",
        "#torch_version.q_proj_weight = wQ\n",
        "#torch_version.k_proj_weight = wK\n",
        "#torch_version.v_proj_weight = wV\n",
        "torch_version.in_proj_weight = torch.nn.Parameter(torch.concat((wQ, wK, wV)))\n",
        "torch_version.out_proj.weight = w0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 330,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_data = torch.ones((2, 3, test_embed_dim))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 331,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_data = torch.rand(2, 3, test_embed_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 332,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([5, 5]) torch.Size([2, 3, 5])\n"
          ]
        }
      ],
      "source": [
        "result_matrix = matrix_version(test_data, test_data, test_data)\n",
        "result_loop = loop_version(test_data, test_data, test_data)\n",
        "result_torch, _ = torch_version(test_data, test_data, test_data, need_weights=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 333,
      "metadata": {},
      "outputs": [],
      "source": [
        "result_matrix = result_matrix.detach()\n",
        "result_loop = result_loop.detach()\n",
        "result_torch = result_torch.detach()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 334,
      "metadata": {},
      "outputs": [],
      "source": [
        "assert result_matrix.shape == result_torch.shape\n",
        "assert result_loop.shape == result_torch.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 335,
      "metadata": {},
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "\nArrays are not almost equal to 6 decimals\n\nMismatched elements: 30 / 30 (100%)\nMax absolute difference: 0.07272851\nMax relative difference: 1.7289221\n x: array([[[-0.737511,  0.418121,  0.302359, -0.0113  , -0.870395],\n        [-0.745576,  0.393842,  0.322722,  0.004232, -0.834106],\n        [-0.74752 ,  0.365238,  0.344328,  0.020905, -0.786164]],...\n y: array([[[-0.73273 ,  0.462243,  0.268469, -0.037401, -0.943123],\n        [-0.75386 ,  0.414645,  0.310067, -0.005806, -0.875633],\n        [-0.757841,  0.357597,  0.353191,  0.027468, -0.780088]],...",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-335-5e14f33c7b54>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0massert_array_almost_equal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_loop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "    \u001b[1;31m[... skipping hidden 2 frame]\u001b[0m\n",
            "\u001b[1;31mAssertionError\u001b[0m: \nArrays are not almost equal to 6 decimals\n\nMismatched elements: 30 / 30 (100%)\nMax absolute difference: 0.07272851\nMax relative difference: 1.7289221\n x: array([[[-0.737511,  0.418121,  0.302359, -0.0113  , -0.870395],\n        [-0.745576,  0.393842,  0.322722,  0.004232, -0.834106],\n        [-0.74752 ,  0.365238,  0.344328,  0.020905, -0.786164]],...\n y: array([[[-0.73273 ,  0.462243,  0.268469, -0.037401, -0.943123],\n        [-0.75386 ,  0.414645,  0.310067, -0.005806, -0.875633],\n        [-0.757841,  0.357597,  0.353191,  0.027468, -0.780088]],..."
          ]
        }
      ],
      "source": [
        "assert_array_almost_equal(result_matrix, result_loop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 336,
      "metadata": {},
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "\nArrays are not almost equal to 6 decimals\n\nMismatched elements: 30 / 30 (100%)\nMax absolute difference: 1.1894937\nMax relative difference: 17.108639\n x: array([[[-0.737511,  0.418121,  0.302359, -0.0113  , -0.870395],\n        [-0.745576,  0.393842,  0.322722,  0.004232, -0.834106],\n        [-0.74752 ,  0.365238,  0.344328,  0.020905, -0.786164]],...\n y: array([[[ 0.061579, -0.509062, -0.083738, -0.38707 ,  0.319099],\n        [ 0.097306, -0.442378, -0.020034, -0.456165,  0.321708],\n        [ 0.122666, -0.391349,  0.026576, -0.504543,  0.322033]],...",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-336-110a99f35eec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0massert_array_almost_equal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_torch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "    \u001b[1;31m[... skipping hidden 2 frame]\u001b[0m\n",
            "\u001b[1;31mAssertionError\u001b[0m: \nArrays are not almost equal to 6 decimals\n\nMismatched elements: 30 / 30 (100%)\nMax absolute difference: 1.1894937\nMax relative difference: 17.108639\n x: array([[[-0.737511,  0.418121,  0.302359, -0.0113  , -0.870395],\n        [-0.745576,  0.393842,  0.322722,  0.004232, -0.834106],\n        [-0.74752 ,  0.365238,  0.344328,  0.020905, -0.786164]],...\n y: array([[[ 0.061579, -0.509062, -0.083738, -0.38707 ,  0.319099],\n        [ 0.097306, -0.442378, -0.020034, -0.456165,  0.321708],\n        [ 0.122666, -0.391349,  0.026576, -0.504543,  0.322033]],..."
          ]
        }
      ],
      "source": [
        "assert_array_almost_equal(result_matrix, result_torch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 316,
      "metadata": {},
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "\nArrays are not almost equal to 6 decimals\n\nMismatched elements: 30 / 30 (100%)\nMax absolute difference: 0.05720511\nMax relative difference: 0.13257349\n x: array([[[0.692497, 0.579445, 0.719626, 0.339527, 0.599161],\n        [0.778973, 0.521562, 0.735156, 0.404873, 0.521556],\n        [0.82875 , 0.48351 , 0.725223, 0.488702, 0.483208]],...\n y: array([[[0.723061, 0.558729, 0.724087, 0.36514 , 0.572077],\n        [0.761729, 0.532796, 0.73083 , 0.394851, 0.537442],\n        [0.785138, 0.515186, 0.727293, 0.431497, 0.519029]],...",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-316-12196ff9d0f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0massert_array_almost_equal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_loop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_torch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "    \u001b[1;31m[... skipping hidden 2 frame]\u001b[0m\n",
            "\u001b[1;31mAssertionError\u001b[0m: \nArrays are not almost equal to 6 decimals\n\nMismatched elements: 30 / 30 (100%)\nMax absolute difference: 0.05720511\nMax relative difference: 0.13257349\n x: array([[[0.692497, 0.579445, 0.719626, 0.339527, 0.599161],\n        [0.778973, 0.521562, 0.735156, 0.404873, 0.521556],\n        [0.82875 , 0.48351 , 0.725223, 0.488702, 0.483208]],...\n y: array([[[0.723061, 0.558729, 0.724087, 0.36514 , 0.572077],\n        [0.761729, 0.532796, 0.73083 , 0.394851, 0.537442],\n        [0.785138, 0.515186, 0.727293, 0.431497, 0.519029]],..."
          ]
        }
      ],
      "source": [
        "assert_array_almost_equal(result_loop, result_torch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2qKG9YczJ_K"
      },
      "outputs": [],
      "source": [
        "\n",
        "class LanguageModel(torch.nn.Module):\n",
        "    \"\"\"TODO: implementar o modelo de linguagem\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yjQ1KXOzJ_K"
      },
      "outputs": [],
      "source": [
        "model = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmsD59TfzJ_K"
      },
      "outputs": [],
      "source": [
        "# sample = next(iter(train_loader))\n",
        "input = sample[0]\n",
        "target = sample[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGbJcT5KzJ_K"
      },
      "outputs": [],
      "source": [
        "output = model(input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "um0lR4mNzJ_K",
        "outputId": "e6041da8-ca7f-4c9d-b28b-9e1250e820f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([4842, 2163, 7516, 2652, 6373, 7429, 8003, 3759, 1768, 7740, 2595, 1859,\n",
              "        3189, 8049, 5727, 6132])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output.argmax(dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "la-b-f8jzJ_L",
        "outputId": "f040cef4-e409-4d20-d335-a3133aaeb63c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([   2,    3,    4,   37,    3,  215,   71,  411, 1263,  355,   87, 3653,\n",
              "         584,  980,    1,    7])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UngUhyu7zJ_L"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wntaV50nzJ_L",
        "outputId": "a054092b-d801-4c60-eb75-85abfe57151d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Verifica se há uma GPU disponível e define o dispositivo para GPU se possível, caso contrário, usa a CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRwSPiwizJ_L"
      },
      "outputs": [],
      "source": [
        "epochs = 10\n",
        "lr = \"\"\"TODO\"\"\"\"\n",
        "criterion = \"\"\"TODO CrossEntropy\"\"\"\"\n",
        "\n",
        "optimizer = \"\"\"TODO: AdamW ou outro\"\"\"\"\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "\"\"\"TODO: Implemente o loop de treinamento. Em cada época, calcule e imprima a loss no dataset de validação\"\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSXfwYISDoPN"
      },
      "source": [
        "## Avaliação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXXO78GSDqPg"
      },
      "outputs": [],
      "source": [
        "\"\"\" TODO: calcule a perplexidade final no dataset de validação \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1zhxVqfzJ_M"
      },
      "source": [
        "## Exemplo de uso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PExkoWOzJ_M"
      },
      "outputs": [],
      "source": [
        "text = \"\"\n",
        "\n",
        "def generate_text(model, vocab, text, max_length):\n",
        "    \"\"\"TODO: implemente a função para gerar texto até atingir o max_length\"\"\"\n",
        "\n",
        "context = 5\n",
        "max_length= 10\n",
        "generate_text(text, max_length)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
