{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OG5DT_dm6mk"
      },
      "source": [
        "# Fine Tuning do BERT no IMDB\n",
        "\n",
        "Nome: Elton Cardoso do Nascimento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ80hHaftwUd"
      },
      "source": [
        "## Instruções:\n",
        "> \n",
        "> \n",
        "> Treinar e medir a acurácia de um modelo BERT (ou variantes) para classificação binária usando o dataset do IMDB (20k/5k amostras de treino/validação).\n",
        "> \n",
        "> Importante:\n",
        "> - [x] Deve-se implementar o próprio laço de treinamento.\n",
        "> - [x] Implementar o acumulo de gradiente.\n",
        "> \n",
        "> Dicas:\n",
        "> - BERT geralmente costuma aprender bem uma tarefa com poucas épocas (de 3 a 5 épocas). Se tiver demorando mais de 5 épocas para chegar em 80% de acurácia, ajuste os hiperparametros.\n",
        "> \n",
        "> - Solução para erro de memória:\n",
        ">   - Usar bfloat16 permite quase dobrar o batch size\n",
        "> \n",
        "> Opcional:\n",
        "> - Pode-se usar a função trainer da biblioteca Transformers/HuggingFace para verificar se seu laço de treinamento está correto. Note que ainda assim é obrigatório implementar o laço próprio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os # Manipular arquivos\n",
        "import random # Operações randômicas\n",
        "import pickle # Serializar/deserializar backups\n",
        "import time # Medição de tempo\n",
        "from concurrent.futures import ThreadPoolExecutor # Parelização\n",
        "from typing import Tuple, List, Dict, Optional # Type hints\n",
        "\n",
        "import numpy as np # Operações vetoriais\n",
        "import matplotlib.pyplot as plt # Plots\n",
        "import torch # ML\n",
        "from torch.utils.data import Dataset, DataLoader # Preparação de dados\n",
        "\n",
        "try:\n",
        "    import wandb # Logging\n",
        "except:\n",
        "    wandb = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhpAkifICdJo"
      },
      "source": [
        "## Fixando a seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {},
      "outputs": [],
      "source": [
        "def reset_seeds():\n",
        "    random.seed(123)\n",
        "    np.random.seed(123)\n",
        "    torch.manual_seed(123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHeZ9nAOEB0U",
        "outputId": "bdd4a1f7-e1d0-4377-9638-a4ee1e968a38"
      },
      "outputs": [],
      "source": [
        "reset_seeds()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXFdJz2KVeQw"
      },
      "source": [
        "## Preparando Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHMi_Kq65fPM"
      },
      "source": [
        "> Primeiro, fazemos download do dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wbnfzst5O3k",
        "outputId": "bebda5c0-5614-4cd0-a2f4-5754cdb9c336"
      },
      "outputs": [],
      "source": [
        "if not os.path.isfile(\"aclImdb.tgz\"):\n",
        "    !curl -LO http://files.fast.ai/data/aclImdb.tgz\n",
        "    !tar -xzf aclImdb.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Giyi5Rv_NIm"
      },
      "source": [
        "### Carregando o dataset\n",
        "\n",
        "> Criaremos uma divisão de treino (20k exemplos) e validação (5k exemplos) artificialmente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_valid = 5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_texts(folder):\n",
        "    texts = []\n",
        "    for path in os.listdir(folder):\n",
        "        with open(os.path.join(folder, path), encoding=\"utf8\") as f:\n",
        "            texts.append(f.read())\n",
        "    return texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "executor = ThreadPoolExecutor(max_workers=4)\n",
        "\n",
        "folders = ['aclImdb/train/pos', 'aclImdb/train/neg', 'aclImdb/test/pos', 'aclImdb/test/neg']\n",
        "\n",
        "futures = []\n",
        "for folder in folders:\n",
        "    future = executor.submit(load_texts, folder) \n",
        "\n",
        "    futures.append(future)\n",
        "\n",
        "all_texts = []\n",
        "\n",
        "for future in futures:\n",
        "    texts = future.result()\n",
        "\n",
        "    all_texts.append(texts)\n",
        "\n",
        "executor.shutdown()\n",
        "\n",
        "x_train_pos = all_texts[0]\n",
        "x_train_neg = all_texts[1]\n",
        "x_test_pos = all_texts[2]\n",
        "x_test_neg = all_texts[3]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train = x_train_pos + x_train_neg\n",
        "x_test = x_test_pos + x_test_neg\n",
        "y_train = [True] * len(x_train_pos) + [False] * len(x_train_neg)\n",
        "y_test = [True] * len(x_test_pos) + [False] * len(x_test_neg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Embaralhamos o treino para depois fazermos a divisão treino/valid.\n",
        "c = list(zip(x_train, y_train))\n",
        "random.shuffle(c)\n",
        "x_train, y_train = zip(*c)\n",
        "\n",
        "x_valid = x_train[-max_valid:]\n",
        "y_valid = y_train[-max_valid:]\n",
        "x_train = x_train[:-max_valid]\n",
        "y_train = y_train[:-max_valid]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20000 amostras de treino.\n",
            "5000 amostras de desenvolvimento.\n",
            "25000 amostras de teste.\n"
          ]
        }
      ],
      "source": [
        "print(len(x_train), 'amostras de treino.')\n",
        "print(len(x_valid), 'amostras de desenvolvimento.')\n",
        "print(len(x_test), 'amostras de teste.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HIN_xLI_TuT",
        "outputId": "787fc595-88b1-486a-8c0c-bcde36396793"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3 primeiras amostras treino:\n",
            "False POSSIBLE SPOILERS<br /><br />The Spy Who Shagged Me is a muchly overrated and over-hyped sequel. Int\n",
            "False The long list of \"big\" names in this flick (including the ubiquitous John Mills) didn't bowl me over\n",
            "True Bette Midler showcases her talents and beauty in \"Diva Las Vegas\". I am thrilled that I taped it and\n",
            "3 últimas amostras treino:\n",
            "False I was previously unaware that in the early 1990's Devry University (or was it ITT Tech?) added Film \n",
            "True The story and music (George Gershwin!) are wonderful, as are Levant, Guetary, Foch, and, of course, \n",
            "True This is my favorite show. I think it is utterly brilliant. Thanks to David Chase for bringing this i\n",
            "3 primeiras amostras validação:\n",
            "True Why has this not been released? I kind of thought it must be a bit rubbish since it hasn't been. How\n",
            "True I was amazingly impressed by this movie. It contained fundamental elements of depression, grief, lon\n",
            "True photography was too jumpy to follow. dark scenes hard to see.<br /><br />Had good story line too bad\n",
            "3 últimas amostras validação:\n",
            "True In the early to mid 1970's, Clifford Irving proposed to write the ultimate biography of Howard Hughe\n",
            "True An ultra-modern house in an affluent neighborhood appears to be the cause of each of its inhabitants\n",
            "True Some of the best movies that are categorized as \"comedies\" actually blur between comedy and drama. \"\n"
          ]
        }
      ],
      "source": [
        "print('3 primeiras amostras treino:')\n",
        "for x, y in zip(x_train[:3], y_train[:3]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('3 últimas amostras treino:')\n",
        "for x, y in zip(x_train[-3:], y_train[-3:]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('3 primeiras amostras validação:')\n",
        "for x, y in zip(x_valid[:3], y_test[:3]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('3 últimas amostras validação:')\n",
        "for x, y in zip(x_valid[-3:], y_valid[-3:]):\n",
        "    print(y, x[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {},
      "outputs": [],
      "source": [
        "GOOD_MOVIE = 1 #True\n",
        "BAD_MOVIE = 0 #False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenizador"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preparamos o tokenizador para uso. No caso vamos utilizar o tokenizador preparado para o modelo BERT:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\Elton/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n",
            "C:\\Users\\Elton\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-cased')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos testar o tokenizador imprimindo uma sequência (observe o token inicial \"101\"=\\<CLS> e final \"102\"=\\<SEP>):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([101, 153, 9025, 13882, 13360, 2036, 16625, 2346, 17656, 9637],\n",
              " [156, 2328, 12165, 2508, 1110, 1141, 10010, 10866, 119, 102])"
            ]
          },
          "execution_count": 213,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens = tokenizer(x_train[0], add_special_tokens=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "tokens[\"input_ids\"][:10], tokens[\"input_ids\"][-10:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset e Dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Definimos o dataset para realizar a tokenizador e manipular os dados:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {},
      "outputs": [],
      "source": [
        "class IMDB_Dataset(Dataset):\n",
        "    '''\n",
        "    Dataset for sentiment analisys\n",
        "\n",
        "    Input: tokenized review and mask (for padding).\n",
        "    Output: if is a good (1) or bad (0) review. \n",
        "    '''\n",
        "    def __init__(self, x_data:List[str], y_data:List[bool], tokenizer) -> None:\n",
        "        \"\"\"\n",
        "        Creates a new dataset.\n",
        "\n",
        "        Args:\n",
        "            x_data (List[str]): dataset reviews.\n",
        "            y_data (List[bool]): dataset targets.\n",
        "            tokenizer: tokenizer to encode reviews.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self._x_data = tokenizer(x_data, \n",
        "                                 return_tensors=\"pt\", #Return as torch tensor \n",
        "                                 padding=True, #Add padding to small sequences\n",
        "                                 return_token_type_ids=False, #Don't return sequence mask (only one sequence)\n",
        "                                 truncation=True) #Truncate big sentences (max = 512 tokens, with CLS and SEP)\n",
        "\n",
        "        self._y_data = torch.tensor(y_data, dtype=torch.float32)\n",
        "\n",
        "        self._size = len(self._y_data)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        Gets the size of the dataset.\n",
        "\n",
        "        Returns:\n",
        "            int: dataset size.\n",
        "        \"\"\"\n",
        "\n",
        "        return self._size\n",
        "    \n",
        "    def __getitem__(self, idx:int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Gets a item of the dataset.\n",
        "\n",
        "        Args:\n",
        "            idx (int): data index.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: dataset input.\n",
        "            torch.Tensor: dataset attention mask. \n",
        "            torch.Tensor: dataset target.\n",
        "        \"\"\"\n",
        "        return self._x_data[\"input_ids\"][idx], self._x_data[\"attention_mask\"][idx], self._y_data[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [],
      "source": [
        "datasets = {}\n",
        "\n",
        "xs = [x_train, x_valid, x_test]\n",
        "ys = [y_train, y_valid, y_test]\n",
        "names = [\"train\", \"val\", \"test\"]\n",
        "\n",
        "for i in range(3):\n",
        "    dataset = IMDB_Dataset(xs[i], ys[i], tokenizer)\n",
        "    datasets[names[i]] = dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para evitar precisamos realizar várias vezes a tokenização durante o desenvolvimento, podemos serializar o dataset para posteriormente deserializá-lo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_name = \"datasets.bin\"\n",
        "with open(file_name, \"wb\") as file:\n",
        "    pickle.dump(datasets, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(file_name, \"rb\") as file:\n",
        "    datasets = pickle.load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "E defimos uma função para criar os dataloaders a partir dos ddatasets e batch size:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_dataloaders(datasets:Dict[str, Dataset], batch_size:int) -> Dict[str, DataLoader]:\n",
        "    '''\n",
        "    Generate dataloaders from datasets.\n",
        "\n",
        "    Args:\n",
        "        datasets (Dict[str, Dataset]): named datasets.\n",
        "        batch_size (int): batch sizes.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, DataLoader]: dataloaders for the datasets.\n",
        "    '''\n",
        "\n",
        "\n",
        "    dataloaders = {}\n",
        "\n",
        "    for name in names:\n",
        "        dataloaders[name] = DataLoader(datasets[name], batch_size=batch_size, shuffle=True)\n",
        "    \n",
        "    return dataloaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparação do modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prepamos o modelo a ser utilizado, que é um modelo BERT com uma camada adicional para realizar a classificação, que recebe como entrada o embedding final relacionado ao token CLS:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BinaryClassifierBERT(torch.nn.Module):\n",
        "    '''\n",
        "    Classifier model using BERT.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, dropout_rate:float=0) -> None:\n",
        "        '''\n",
        "        Model constructor.\n",
        "\n",
        "        Args:\n",
        "            dropout_rate (float, optional): Dropout before the final layer. Defaults to 0.\n",
        "        '''\n",
        "        super().__init__()\n",
        "        \n",
        "        self.bert = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-cased')\n",
        "        \n",
        "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
        "        self.linear = torch.nn.Linear(768, 1)\n",
        "\n",
        "    def forward(self, input_ids:torch.Tensor, attention_masks:Optional[torch.Tensor]=None) -> torch.Tensor:\n",
        "        '''\n",
        "        Computes the classification for the input.\n",
        "\n",
        "        Args:\n",
        "            input_ids (torch.Tensor): tokenized input.\n",
        "            attention_masks (torch.Tensor, optional): attention mask of the input. Defaults to None.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: inference result.\n",
        "        '''\n",
        "        \n",
        "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_masks)\n",
        "        c_vector = bert_output.last_hidden_state[:, 0]\n",
        "\n",
        "        y = self.dropout(c_vector)\n",
        "        y = self.linear(y)\n",
        "\n",
        "        return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Treino"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nesta seção iremos realizar o treino, iniciando pela definição de algumas funções auxiliares."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Funções auxiliares\n",
        "\n",
        "Iremos definir três funções auxiliares: uma para calcular a perplexidade a partir da loss, outra para printar informações e uma final para calcular a loss:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ppl(loss:torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Computes the perplexity from the loss.\n",
        "\n",
        "    Args:\n",
        "        loss (torch.Tensor): loss to compute the perplexity.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: corresponding perplexity.\n",
        "    \"\"\"\n",
        "    return torch.exp(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_info(loss_value:torch.Tensor, epoch:int, total_epochs:int, time:float=0.0):\n",
        "    \"\"\"\n",
        "    Prints the information of a epoch.\n",
        "\n",
        "    Args:\n",
        "        loss_value (torch.Tensor): epoch loss.\n",
        "        epoch (int): epoch number.\n",
        "        total_epochs (int): total number of epochs. \n",
        "        time (float, optional): time to run the epoch. Don't print if is 0.0. Defaults to 0.0.\n",
        "    \"\"\"\n",
        "    ppl_value = ppl(loss_value)\n",
        "\n",
        "    \n",
        "    print(f'Epoch [{epoch+1}/{total_epochs}], \\\n",
        "            Loss: {loss_value.item():.4f}, \\\n",
        "            Perplexity: {ppl_value.item():.4f}', end=\"\")\n",
        "    \n",
        "    if time != 0:\n",
        "        print(f\", Elapsed Time: {time:.2f} sec\")    \n",
        "    else:\n",
        "        print(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODE_TRAIN = 0\n",
        "MODE_EVALUATE = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_loss(model:torch.nn.Module, loader:DataLoader, criterion:torch.nn.Module, mode:int = MODE_EVALUATE) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Computes the loss from a model across a dataset.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): model to evaluate.\n",
        "        loader (DataLoader): dataset.\n",
        "        criterion (torch.nn.Module): loss function to compute.\n",
        "        mode (int): mode of the computation. \n",
        "                    If MODE_EVALUATE, computes without gradient, in eval mode and detachs loss.\n",
        "                    If MODE_TRAIN, computes with gradient and in train mode.\n",
        "                    Default is MODE_EVALUATE.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: resulting loss.\n",
        "    \"\"\"\n",
        "    device = next(iter(model.parameters())).device\n",
        "\n",
        "    if mode == MODE_EVALUATE:\n",
        "        model.eval()\n",
        "        torch.set_grad_enabled(False)\n",
        "    elif mode == MODE_TRAIN:\n",
        "        model.train()\n",
        "        torch.set_grad_enabled(True)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown mode: {mode}.\")\n",
        "\n",
        "    total_loss = torch.tensor(0, dtype=torch.float32, device=device)\n",
        "    n = 0\n",
        "    for inputs, masks, targets in loader:\n",
        "        inputs = inputs.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        targets = targets.reshape(-1)\n",
        "        targets = targets.to(device)\n",
        "        \n",
        "        logits = model(inputs, masks)\n",
        "        logits = logits.view(-1, logits.shape[-1])\n",
        "\n",
        "        loss = criterion(logits.squeeze(), targets)\n",
        "        total_loss += loss*targets.size(0)\n",
        "\n",
        "        n += targets.size(0)\n",
        "\n",
        "    total_loss /= n \n",
        "    \n",
        "    torch.set_grad_enabled(True)\n",
        "\n",
        "    if mode == MODE_EVALUATE:\n",
        "        total_loss = total_loss.detach()\n",
        "\n",
        "    return total_loss.detach()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inicialização\n",
        "\n",
        "Começamos o processo de treino inicializando as variáveis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Definimos se será realizado o logging utilizando o wandb:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "use_wandb = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Checamos se existe uma GPU disponível:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verifica se há uma GPU disponível e define o dispositivo para GPU se possível, caso contrário, usa a CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Definimos os parâmetros de treino:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "dropout_rate = 0\n",
        "lr = 5e-5\n",
        "n_epoch = 2\n",
        "optimizer_class = torch.optim.Adam\n",
        "weight_decay = 0\n",
        "\n",
        "config = {\n",
        "    \"batch_size\": batch_size,\n",
        "    \"dropout_rate\": dropout_rate,\n",
        "    \"lr\": lr,\n",
        "    \"n_epoch\": n_epoch,\n",
        "    \"optimizer_class\": optimizer_class.__name__,\n",
        "    \"weight_decay\": weight_decay,\n",
        "}\n",
        "\n",
        "if use_wandb:\n",
        "    wandb.init(project=\"IA024-04-TransformDecoder\", config=config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Reiniciamos as sementes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reset_seeds()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Criamos o modelo, loss, otimizador e dataloaders:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\Elton/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n"
          ]
        }
      ],
      "source": [
        "model = BinaryClassifierBERT(dropout_rate)\n",
        "model.to(device)\n",
        "\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "optimizer = optimizer_class(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "dataloaders = create_dataloaders(datasets, batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Treino"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "E finalmente podemos realizar o processo de treino em si:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hist = {}\n",
        "hist[\"loss_train\"] = []\n",
        "hist[\"loss_val\"] = []\n",
        "hist[\"ppl_train\"] = []\n",
        "hist[\"ppl_val\"] = []\n",
        "\n",
        "#Informações antes da primeira epoch\n",
        "prev_loss = compute_loss(model, dataloaders[\"train\"], criterion, MODE_EVALUATE)\n",
        "print_info(prev_loss, -1, n_epoch, 0)\n",
        "\n",
        "for epoch in range(n_epoch):\n",
        "    start_time = time.time() \n",
        "\n",
        "    loss_train = compute_loss(model, dataloaders[\"train\"], criterion, MODE_TRAIN)\n",
        "\n",
        "    end_time = time.time() \n",
        "    \n",
        "    epoch_duration = end_time - start_time \n",
        "\n",
        "    ppl_train = ppl(loss_train)\n",
        "\n",
        "    print_info(loss_train, epoch, n_epoch, epoch_duration)\n",
        "    \n",
        "    #Validation stats\n",
        "    print(\"VAL \", end=\"\")\n",
        "    loss_val = compute_loss(model, dataloaders[\"val\"], criterion, MODE_EVALUATE)\n",
        "    ppl_val = ppl(loss_val)\n",
        "    print_info(loss_val, epoch, n_epoch)\n",
        "\n",
        "    #Save history\n",
        "    hist[\"loss_train\"].append(loss_train.item())\n",
        "    hist[\"loss_val\"].append(loss_val.item())\n",
        "    hist[\"ppl_train\"].append(ppl_train.item())\n",
        "    hist[\"ppl_val\"].append(ppl_val.item())\n",
        "\n",
        "    log = {\n",
        "        \"loss_train\": loss_train.item(),\n",
        "        \"loss_val\": loss_val.item(),\n",
        "        \"ppl_train\": ppl_train.item(),\n",
        "        \"ppl_val\": ppl_val.item()\n",
        "    }\n",
        "\n",
        "    if use_wandb:\n",
        "        wandb.log(log)\n",
        "\n",
        "for key in hist:\n",
        "    hist[key] = np.array(hist[key])\n",
        "\n",
        "if use_wandb:\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plotamos os gráficos das estatísticas obtidas durante o treinamento, onde podemos observar que TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(hist[\"loss_train\"], \"o-\")\n",
        "plt.plot(hist[\"loss_val\"], \"o-\")\n",
        "\n",
        "plt.legend([\"Train\", \"Val\"])\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss history\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(hist[\"ppl_train\"], \"o-\")\n",
        "plt.plot(hist[\"ppl_val\"], \"o-\")\n",
        "\n",
        "plt.legend([\"Train\", \"Val\"])\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"PPL\")\n",
        "plt.title(\"Perplexity history\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Avaliação\n",
        "\n",
        "Para avaliação começamos calculando a loss no dataset de teste:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_loss = compute_loss(model, dataloaders[\"test\"], criterion, mode=MODE_EVALUATE)\n",
        "test_ppl = ppl(test_loss)\n",
        "\n",
        "test_ppl.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculamos quantos modelos adicionais foram necessários:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "769"
            ]
          },
          "execution_count": 189,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "n_param_bert = sum([p.numel() for p in model.bert.parameters()])\n",
        "\n",
        "n_param = sum([p.numel() for p in model.parameters()])\n",
        "n_param-n_param_bert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "E verificamos qualitativamente a saída do modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokens = tokenizer('''This must be gambling debt. Because only when someone threatens to break your legs \n",
        "                      if you don't pay will you go and agree to make a film like this.''', \n",
        "                    return_tensors=\"pt\",\n",
        "                    return_token_type_ids=False,\n",
        "                    truncation=True)\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = model(tokens[\"input_ids\"], tokens[\"attention_mask\"])\n",
        "\n",
        "print(\"Result:\", logits.item())\n",
        "print(f\"Is this a good movie? {torch.round(logits).item() == GOOD_MOVIE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos observar que ele realizou TODO corretamente/incorretamente TODO a classificação"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
