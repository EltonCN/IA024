{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OG5DT_dm6mk"
      },
      "source": [
        "# Fine Tuning do BERT no IMDB\n",
        "\n",
        "Nome: Elton Cardoso do Nascimento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ80hHaftwUd"
      },
      "source": [
        "## Instruções:\n",
        "> \n",
        "> \n",
        "> Treinar e medir a acurácia de um modelo BERT (ou variantes) para classificação binária usando o dataset do IMDB (20k/5k amostras de treino/validação).\n",
        "> \n",
        "> Importante:\n",
        "> - Deve-se implementar o próprio laço de treinamento.\n",
        "> - Implementar o acumulo de gradiente.\n",
        "> \n",
        "> Dicas:\n",
        "> - BERT geralmente costuma aprender bem uma tarefa com poucas épocas (de 3 a 5 épocas). Se tiver demorando mais de 5 épocas para chegar em 80% de acurácia, ajuste os hiperparametros.\n",
        "> \n",
        "> - Solução para erro de memória:\n",
        ">   - Usar bfloat16 permite quase dobrar o batch size\n",
        "> \n",
        "> Opcional:\n",
        "> - Pode-se usar a função trainer da biblioteca Transformers/HuggingFace para verificar se seu laço de treinamento está correto. Note que ainda assim é obrigatório implementar o laço próprio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhpAkifICdJo"
      },
      "source": [
        "## Fixando a seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1ozXD-xYCcrT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
        "from typing import Tuple, List, Union\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader # Preparação de dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHeZ9nAOEB0U",
        "outputId": "bdd4a1f7-e1d0-4377-9638-a4ee1e968a38"
      },
      "outputs": [],
      "source": [
        "random.seed(123)\n",
        "np.random.seed(123)\n",
        "torch.manual_seed(123);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXFdJz2KVeQw"
      },
      "source": [
        "## Preparando Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHMi_Kq65fPM"
      },
      "source": [
        "> Primeiro, fazemos download do dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wbnfzst5O3k",
        "outputId": "bebda5c0-5614-4cd0-a2f4-5754cdb9c336"
      },
      "outputs": [],
      "source": [
        "if not os.path.isfile(\"aclImdb.tgz\"):\n",
        "    !curl -LO http://files.fast.ai/data/aclImdb.tgz\n",
        "    !tar -xzf aclImdb.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Giyi5Rv_NIm"
      },
      "source": [
        "### Carregando o dataset\n",
        "\n",
        "> Criaremos uma divisão de treino (20k exemplos) e validação (5k exemplos) artificialmente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_valid = 5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_texts(folder):\n",
        "    texts = []\n",
        "    for path in os.listdir(folder):\n",
        "        with open(os.path.join(folder, path), encoding=\"utf8\") as f:\n",
        "            texts.append(f.read())\n",
        "    return texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "executor = ThreadPoolExecutor(max_workers=4)\n",
        "\n",
        "folders = ['aclImdb/train/pos', 'aclImdb/train/neg', 'aclImdb/test/pos', 'aclImdb/test/neg']\n",
        "\n",
        "futures = []\n",
        "for folder in folders:\n",
        "    future = executor.submit(load_texts, folder) \n",
        "\n",
        "    futures.append(future)\n",
        "\n",
        "all_texts = []\n",
        "\n",
        "for future in futures:\n",
        "    texts = future.result()\n",
        "\n",
        "    all_texts.append(texts)\n",
        "\n",
        "executor.shutdown()\n",
        "\n",
        "x_train_pos = all_texts[0]\n",
        "x_train_neg = all_texts[1]\n",
        "x_test_pos = all_texts[2]\n",
        "x_test_neg = all_texts[3]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train = x_train_pos + x_train_neg\n",
        "x_test = x_test_pos + x_test_neg\n",
        "y_train = [True] * len(x_train_pos) + [False] * len(x_train_neg)\n",
        "y_test = [True] * len(x_test_pos) + [False] * len(x_test_neg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Embaralhamos o treino para depois fazermos a divisão treino/valid.\n",
        "c = list(zip(x_train, y_train))\n",
        "random.shuffle(c)\n",
        "x_train, y_train = zip(*c)\n",
        "\n",
        "x_valid = x_train[-max_valid:]\n",
        "y_valid = y_train[-max_valid:]\n",
        "x_train = x_train[:-max_valid]\n",
        "y_train = y_train[:-max_valid]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20000 amostras de treino.\n",
            "5000 amostras de desenvolvimento.\n",
            "25000 amostras de teste.\n"
          ]
        }
      ],
      "source": [
        "print(len(x_train), 'amostras de treino.')\n",
        "print(len(x_valid), 'amostras de desenvolvimento.')\n",
        "print(len(x_test), 'amostras de teste.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HIN_xLI_TuT",
        "outputId": "787fc595-88b1-486a-8c0c-bcde36396793"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3 primeiras amostras treino:\n",
            "False POSSIBLE SPOILERS<br /><br />The Spy Who Shagged Me is a muchly overrated and over-hyped sequel. Int\n",
            "False The long list of \"big\" names in this flick (including the ubiquitous John Mills) didn't bowl me over\n",
            "True Bette Midler showcases her talents and beauty in \"Diva Las Vegas\". I am thrilled that I taped it and\n",
            "3 últimas amostras treino:\n",
            "False I was previously unaware that in the early 1990's Devry University (or was it ITT Tech?) added Film \n",
            "True The story and music (George Gershwin!) are wonderful, as are Levant, Guetary, Foch, and, of course, \n",
            "True This is my favorite show. I think it is utterly brilliant. Thanks to David Chase for bringing this i\n",
            "3 primeiras amostras validação:\n",
            "True Why has this not been released? I kind of thought it must be a bit rubbish since it hasn't been. How\n",
            "True I was amazingly impressed by this movie. It contained fundamental elements of depression, grief, lon\n",
            "True photography was too jumpy to follow. dark scenes hard to see.<br /><br />Had good story line too bad\n",
            "3 últimas amostras validação:\n",
            "True In the early to mid 1970's, Clifford Irving proposed to write the ultimate biography of Howard Hughe\n",
            "True An ultra-modern house in an affluent neighborhood appears to be the cause of each of its inhabitants\n",
            "True Some of the best movies that are categorized as \"comedies\" actually blur between comedy and drama. \"\n"
          ]
        }
      ],
      "source": [
        "print('3 primeiras amostras treino:')\n",
        "for x, y in zip(x_train[:3], y_train[:3]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('3 últimas amostras treino:')\n",
        "for x, y in zip(x_train[-3:], y_train[-3:]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('3 primeiras amostras validação:')\n",
        "for x, y in zip(x_valid[:3], y_test[:3]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('3 últimas amostras validação:')\n",
        "for x, y in zip(x_valid[-3:], y_valid[-3:]):\n",
        "    print(y, x[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenizador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\Elton/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n",
            "C:\\Users\\Elton\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-cased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "BERT_CLS = 101\n",
        "BERT_SEP = 102"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[101, 153, 9025, 13882, 13360, 2036, 16625, 2346, 17656, 9637]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens = tokenizer.encode(x_train[0], add_special_tokens=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "tokens[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset e Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "class IMDB_Dataset(Dataset):\n",
        "    def __init__(self, x_data:Tuple[str], y_data:Tuple[bool], tokenizer) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        x_tokens = torch.empty((len(x_data), 512), dtype=torch.float32)\n",
        "\n",
        "        for i in range(len(x_data)):\n",
        "            x = x_data[i]\n",
        "            tokens = tokenizer.encode(x, add_special_tokens=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "            if len(tokens) > 512:\n",
        "                tokens = tokens[:512]\n",
        "                tokens[-1] = BERT_SEP\n",
        "                \n",
        "            x_tokens[i] = torch.Tensor(tokens)\n",
        "\n",
        "        self._x_data = x_tokens\n",
        "        \n",
        "        self._y_data = torch.tensor(y_data, dtype=torch.float32)\n",
        "\n",
        "        self._size = len(self._x_data)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        Gets the size of the dataset.\n",
        "\n",
        "        Returns:\n",
        "            int: dataset size.\n",
        "        \"\"\"\n",
        "\n",
        "        return self._size\n",
        "    \n",
        "    def __getitem__(self, idx:int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Gets a item of the dataset.\n",
        "\n",
        "        Args:\n",
        "            idx (int): data index.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: dataset input. \n",
        "            torch.Tensor: dataset target.\n",
        "        \"\"\"\n",
        "        return self._x_data[idx], self._y_data[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "ename": "BrokenProcessPool",
          "evalue": "A process in the process pool was terminated abruptly while the future was running or pending.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[19], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m datasets \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m futures:\n\u001b[1;32m---> 16\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     datasets\u001b[38;5;241m.\u001b[39mappend(dataset)\n",
            "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\concurrent\\futures\\_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
            "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[1;31mBrokenProcessPool\u001b[0m: A process in the process pool was terminated abruptly while the future was running or pending."
          ]
        }
      ],
      "source": [
        "args = [(x_train, y_train, tokenizer), (x_valid, y_valid, tokenizer), (x_test, y_test, tokenizer)]\n",
        "\n",
        "xs = [x_train, x_valid, x_test]\n",
        "ys = [y_train, y_valid, y_test]\n",
        "\n",
        "with ProcessPoolExecutor() as executor:\n",
        "    futures = []\n",
        "    for i in range(3):\n",
        "        future = executor.submit(IMDB_Dataset, x_data=xs[i], y_data=ys[i], tokenizer=tokenizer) \n",
        "\n",
        "        futures.append(future)\n",
        "    \n",
        "    datasets = []\n",
        "\n",
        "    for future in futures:\n",
        "        dataset = future.result()\n",
        "\n",
        "        datasets.append(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "thread: 2m58s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raise ValueError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparação do modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "https://pytorch.org/hub/huggingface_pytorch-transformers/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\Elton/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n",
            "C:\\Users\\Elton\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "C:\\Users\\Elton\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Elton\\.cache\\huggingface\\hub\\models--bert-base-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "Using cache found in C:\\Users\\Elton/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n"
          ]
        }
      ],
      "source": [
        "\n",
        "base_model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-cased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\Elton/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n",
            "C:\\Users\\Elton\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Elton\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        }
      ],
      "source": [
        "config = torch.hub.load('huggingface/pytorch-transformers', 'config', 'bert-base-uncased') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokens_tensor = torch.tensor([tokens, tokens])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output = base_model(tokens_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 512, 768])"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output.last_hidden_state.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SentimentAnalisysBERT(torch.nn.Module):\n",
        "    def __init__(self, dropout_rate:float=0) -> None:\n",
        "        super().__init__()\n",
        "        \n",
        "        self.bert_model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-cased')\n",
        "        \n",
        "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
        "        self.linear = torch.nn.Linear(768, 1)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
        "        bert_output = self.bert_model(x)\n",
        "        c_vector = bert_output.last_hidden_state[:, 0]\n",
        "\n",
        "        y = self.dropout(c_vector)\n",
        "        y = self.linear(y)\n",
        "        y = self.relu(y)\n",
        "\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\Elton/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n"
          ]
        }
      ],
      "source": [
        "model = SentimentAnalisysBERT()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "108310272"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "769"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "n_param_bert = sum([p.numel() for p in model.bert_model.parameters()])\n",
        "\n",
        "n_param = sum([p.numel() for p in model.parameters()])\n",
        "n_param-n_param_bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logits = model(tokens_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "criterion = torch.nn.BCEWithLogitsLoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "targets = torch.tensor(y_train[:2], dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.float32"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "targets.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
