{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhjrdWXuPMeW"
      },
      "source": [
        "## Module 2 - Fine-tuning Phi-1.5 for sentence classification using QLoRA\n",
        "\n",
        "This notebook presents an example of how to fine-tune Phi-1.5 for sentence classification using QLoRA.\n",
        "\n",
        "QLoRA is a fine-tuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. For more details, please refer to the [QLoRA paper](https://arxiv.org/abs/2106.09647).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dFZ6y7XMXD4"
      },
      "source": [
        "# Installing required packages\n",
        "\n",
        "In this example, we have to install the following libraries:  `transformers`, `datasets`, `torch`, `peft`, `bitsandbytes`, and `trl`.\n",
        "\n",
        "**`transformers`**:\n",
        "\n",
        "Transformers is an open-source library for NLP developed by Hugging Face. It provides state-of-the-art pre-trained models for various NLP tasks, such as text classification, sentiment analysis, question-answering, named entity recognition, etc.\n",
        "\n",
        "**`datasets`**:\n",
        "\n",
        "Datasets is another open-source library developed by Hugging Face that provides a collection of preprocessed datasets for various NLP tasks, such as sentiment analysis, natural language inference, machine translation, and many more.\n",
        "\n",
        "\n",
        "**`torch`**:\n",
        "\n",
        "PyTorch is an open-source machine learning library that provides a wide range of tools and utilities for building and training custom deep learning models. It is already installed in the Colab environment, but we need to install its latest version.\n",
        "\n",
        "**`peft`**:\n",
        "\n",
        "ðŸ¤— PEFT, or Parameter-Efficient Fine-Tuning (PEFT), is a library for efficiently adapting pre-trained language models (PLMs) to various downstream applications without fine-tuning all the modelâ€™s parameters. We use PEFT in this example because it supports QLoRA.\n",
        "\n",
        "\n",
        "**`bitsandbytes`**:\n",
        "\n",
        "BitsAndBytes is a library designed to optimize the training of neural networks on modern GPUs. It offers efficient implementations of 8-bit optimizers, which significantly reduce the memory footprint of model parameters and gradients. This reduction in memory usage enables training larger models or using larger batch sizes within the same memory constraints.\n",
        "\n",
        "\n",
        "**`trl`**:\n",
        "\n",
        "ðŸ¤— TRL, or Transfer Learning Library, is a library for training and evaluating transfer learning models. It provides a unified API for training and evaluating various transfer learning models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdrBm7o4LPRb",
        "outputId": "594166b1-ca3f-405f-ddd2-4deb08929a35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/pip:6: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  from pkg_resources import load_entry_point\n",
            "\u001b[33mWARNING: The directory '/root/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
            "/bin/pip:6: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  from pkg_resources import load_entry_point\n",
            "\u001b[33mWARNING: The directory '/root/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.19.0-py3-none-any.whl (542 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 542 kB 18.7 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.19.5)\n",
            "Collecting pyarrow>=12.0.0\n",
            "  Downloading pyarrow-16.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.9 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40.9 MB 62.0 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting pyarrow-hotfix\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 116 kB 10.0 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.63.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 194 kB 14.0 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting multiprocess\n",
            "  Downloading multiprocess-0.70.16-py38-none-any.whl (132 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 132 kB 25.9 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2024.3.1)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.9.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.3 MB 37.2 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.21.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.4.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (240 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240 kB 40.4 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 129 kB 29.2 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.9.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (308 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 308 kB 20.2 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0; python_version < \"3.11\"\n",
            "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: pyarrow, pyarrow-hotfix, dill, xxhash, multiprocess, frozenlist, aiosignal, multidict, yarl, async-timeout, aiohttp, datasets\n",
            "Successfully installed aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.19.0 dill-0.3.8 frozenlist-1.4.1 multidict-6.0.5 multiprocess-0.70.16 pyarrow-16.0.0 pyarrow-hotfix-0.6 xxhash-3.4.1 yarl-1.9.4\n",
            "/bin/pip:6: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  from pkg_resources import load_entry_point\n",
            "\u001b[33mWARNING: The directory '/root/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
            "\u001b[31mERROR: torchaudio 0.12.1 has requirement torch==1.12.1, but you'll have torch 2.2.2 which is incompatible.\u001b[0m\n",
            "/bin/pip:6: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  from pkg_resources import load_entry_point\n",
            "\u001b[33mWARNING: The directory '/root/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
            "/bin/pip:6: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  from pkg_resources import load_entry_point\n",
            "\u001b[33mWARNING: The directory '/root/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#!pip install -q torch\n",
        "!pip install -q git+https://github.com/huggingface/transformers #huggingface transformers for downloading models weights\n",
        "!pip install datasets\n",
        "!pip install -q peft  # Parameter efficient finetuning - for qLora Finetuning\n",
        "!pip install -q bitsandbytes  # For Model weights quantization\n",
        "!pip install -q trl  # Transformer Reinforcement Learning - For Finetuning using Supervised Fine-tuning\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGdm9g9fkjxE"
      },
      "source": [
        "# Setting the device\n",
        "\n",
        "In this example, we will use a GPU to speed up the fine-tuning process. GPUs (Graphics Processing Units) are specialized processors that are optimized for performing large-scale computations in parallel. By using a GPU, we can accelerate the training and inference of a machine learning model, which can significantly reduce the time required to complete these tasks.\n",
        "\n",
        "Before we begin, we need to check whether a GPU is available and select it as the default device for our PyTorch operations. This is because PyTorch can use either a CPU or a GPU to perform computations, and by default, it will use the CPU.\n",
        "\n",
        "For using a GPU in Google Colab:\n",
        "1. Click on the \"Runtime\" menu at the top of the screen.\n",
        "2. From the dropdown menu, click on \"Change runtime type\".\n",
        "3. In the popup window that appears, select \"A100 GPU\" as the hardware accelerator.\n",
        "4. Click on the \"Save\" button.\n",
        "\n",
        "That's it! Now you can use the GPU for faster computations in your notebook.\n",
        "\n",
        "**IMPORTANT**: This example requires a GPU with at least 40GB of memory. If you are using Google Colab, you can select a GPU with 40GB of memory by following the steps above. If you are using a different environment, please make sure that your GPU has at least 40GB of memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiV9Y1ZweR2Y",
        "outputId": "83273329-9f44-4211-a02a-a48c5cf14d1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Apr 22 16:14:54 2024       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.86       Driver Version: 470.86       CUDA Version: 11.4     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Quadro RTX 8000     On   | 00000000:3D:00.0 Off |                  Off |\n",
            "| 33%   28C    P8    32W / 260W |      5MiB / 48601MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwYGaELZMwvZ"
      },
      "source": [
        "# Downloading Dataset\n",
        "\n",
        "The SST-2 dataset, or the Stanford Sentiment Treebank, is popular for sentiment analysis tasks in Natural Language Processing (NLP). It consists of movie reviews from the Rotten Tomatoes website that are labeled with either a positive or negative sentiment. The dataset contains 10,662 sentence-level movie reviews, with approximately half of the reviews labeled as positive and the other half labeled as negative. The reviews are also relatively evenly distributed in length, with a median length of 18 tokens.\n",
        "\n",
        "The SST-2 dataset has become a benchmark dataset for sentiment analysis in NLP, and many researchers use it to evaluate the performance of their models. The dataset's popularity is partly due to its high-quality labels and the task's relative simplicity, making it an accessible starting point for researchers and developers new to NLP.\n",
        "\n",
        "In this example, we're using the **`datasets`** library to download and load the training and validation sets of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbX4laO5M6FC",
        "outputId": "5da92422-8eed-470d-cd9c-8e4d50af8bd1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "test_dataset = load_dataset('imdb', split='test')\n",
        "train_dataset = load_dataset('imdb', split='train')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OyZ3D_rKEK2"
      },
      "source": [
        "# Data Preparation\n",
        "\n",
        "Now, we will prepare the data for training our model. First, we define a template with the fields `sentence` and `class`. Then, we use the `map` method to apply this template to the dataset. This will create a new dataset with the fields `sentence` and `class` for each example in the original dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EBjlqME6VUJn"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"<s>[INST] Your task is to classify sentences' sentiment as 'positive' or 'negative'.\n",
        "\n",
        "Sentence: {text} [\\INST]\n",
        "{class}</s>\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1ZalJmCxLn9"
      },
      "source": [
        "Before, we need to convert the labels from 0 and 1 to \"negative\" and \"positive\". We can do this by using the `map` method to apply a function to each example in the dataset. The function will take the label as input and return the corresponding string and store in the column `class`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "4fc5d8efd4164cf98c78817dca300750",
            "69db67eed1ac460cb00fd7ae57fb8336",
            "f27c32a8a41e4766852235fe7657eb18",
            "9054a08a953b4664af94a5d834945950",
            "c0e35d69570b4f0bb45967d7e420226c",
            "ccdf7cde1a834ffdb45261016e8018da",
            "ae9b48d0776e453a961aad557a490be5",
            "ab21c7ec21104159ab595d7d65913e24",
            "79e14c8e280f4bb7a860825f85737a93",
            "731e3053925a411b9357632b78aa0dc0",
            "ba875b0928a74707b6ca9cf23e66c2f9"
          ]
        },
        "id": "65ew6vQDKEz7",
        "outputId": "61be84b5-d4e8-48f3-856b-a8eaa9171faa"
      },
      "outputs": [],
      "source": [
        "POSITIVE_LABEL = \"positive\"\n",
        "NEGATIVE_LABEL = \"negative\"\n",
        "\n",
        "train_dataset = train_dataset.map(lambda example: {'class': POSITIVE_LABEL if example[\"label\"] == 1 else NEGATIVE_LABEL})\n",
        "train_dataset = train_dataset.map(lambda example: {\"text\": template.format(**example)})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OE2EQjQ69uLJ"
      },
      "source": [
        "The code below converts the `label` column of the test dataset into a list of strings with `\"positive\"` and `\"negative\"` labels. This is for comparing the model's predictions with the actual labels of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jCts-_zF9uLM"
      },
      "outputs": [],
      "source": [
        "test_dataset = test_dataset.map(lambda example: {'class': POSITIVE_LABEL if example[\"label\"] == 1 else NEGATIVE_LABEL})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyAho1DLxg7z"
      },
      "source": [
        "# Fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-_4YGOYRpmu"
      },
      "source": [
        "## Setting Model Parameters\n",
        "\n",
        "We need to set various parameters for our fine-tuning process, including QLoRA (Quantization LoRA) parameters, bitsandbytes parameters, and training arguments:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "H76_erzt1-on"
      },
      "outputs": [],
      "source": [
        "# The model that you want to train from the Hugging Face hub\n",
        "# model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "model_name = \"microsoft/phi-1_5\"\n",
        "\n",
        "# Fine-tuned model name\n",
        "new_model = \"phi-1_5-IMDB\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKzTy9MbR2DY"
      },
      "source": [
        "Setting the QLora Parameters\n",
        "\n",
        "1. **lora_r (LoRA attention dimension)**:\n",
        "   - the rank of the update matrices, expressed in int. Lower rank results in smaller update matrices with fewer trainable parameters.\n",
        "\n",
        "2. **lora_alpha (Alpha parameter for LoRA scaling)**:\n",
        "   - This parameter is the LoRA scaling factor applied to the modifications.\n",
        "\n",
        "3. **lora_dropout (Dropout probability for LoRA layers)**:\n",
        "   - This parameter represents the dropout rate applied to the LoRA layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pXH06fiYybTF"
      },
      "outputs": [],
      "source": [
        "# LoRA attention dimension\n",
        "lora_r = 64 # @param\n",
        "\n",
        "# Alpha parameter for LoRA scaling\n",
        "lora_alpha = 16 # @param\n",
        "\n",
        "# Dropout probability for LoRA layers\n",
        "lora_dropout = 0.1 # @param"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g69uRG5jzCEZ"
      },
      "source": [
        "Bitsandbytes parameters. These parameters focus on the implementation of 4-bit precision in model loading and computation. Here's an explanation of each:\n",
        "\n",
        "1. **use_4bit (Activate 4-bit precision base model loading)**:\n",
        "   - This parameter, when set to `True`, indicates that the base model (i.e., the pre-trained model or initial model weights) should be loaded using 4-bit precision.\n",
        "2. **bnb_4bit_compute_dtype (Compute dtype for 4-bit base models)**:\n",
        "   - This parameter specifies the data type to be used for computations in the context of 4-bit base models.\n",
        "   - The value `\"float16\"` indicates that computations should be done using 16-bit floating-point numbers.\n",
        "\n",
        "3. **bnb_4bit_quant_type (Quantization type)**:\n",
        "   - This parameter determines the type of quantization to be used for the 4-bit models.\n",
        "   - The options `\"fp4\"` and `\"nf4\"` refer to different quantization schemes.\n",
        "\n",
        "4. **use_nested_quant (Activate nested quantization for 4-bit base models)**:\n",
        "   - When set to `True`, this parameter enables nested quantization for 4-bit base models.\n",
        "   - Nested quantization, often referred to as double quantization, involves applying a second layer of quantization on top of an already quantized model. This can be used for further reducing the model size or for specialized computational optimizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MtraXCCMzJAc"
      },
      "outputs": [],
      "source": [
        "# Activate 4-bit precision base model loading\n",
        "use_4bit = True # @param\n",
        "\n",
        "# Compute dtype for 4-bit base models\n",
        "bnb_4bit_compute_dtype = \"float16\" # @param\n",
        "\n",
        "# Quantization type (fp4 or nf4)\n",
        "bnb_4bit_quant_type = \"nf4\" # @param [\"nf4\",\"fp4\"]\n",
        "\n",
        "# Activate nested quantization for 4-bit base models (double quantization)\n",
        "use_nested_quant = True # @param"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHTSrnU00F2u"
      },
      "source": [
        "Now, let's define the training arguments.\n",
        "\n",
        "1. **output_dir**:\n",
        "   - Specifies the directory where the model predictions and checkpoints will be stored.\n",
        "\n",
        "2. **num_train_epochs**:\n",
        "   - Sets the number of epochs for training, where one epoch means one pass through the entire training dataset. We set it to `1`\n",
        "\n",
        "3. **fp16, bf16**:\n",
        "   - Enable training with 16-bit floating-point precision (`fp16`) or 16-bit bfloat precision (`bf16`).\n",
        "\n",
        "4. **per_device_train_batch_size**:\n",
        "   - Determines the batch size for training per GPU. This will depend on the GPU used. For an A100, we can use a batch size of 16 examples.\n",
        "\n",
        "5. **per_device_eval_batch_size**:\n",
        "   - Sets the batch size for evaluation per GPU.\n",
        "\n",
        "6. **gradient_accumulation_steps**:\n",
        "   - Indicates the number of update steps over which to accumulate gradients.\n",
        "\n",
        "7. **gradient_checkpointing**:\n",
        "   - When enabled, saves memory by trading compute for memory. Useful for training large models that would otherwise not fit in memory.\n",
        "\n",
        "8. **max_grad_norm (Maximum gradient norm)**:\n",
        "   - Specifies the maximum norm of gradients for gradient clipping, a technique to prevent exploding gradients in deep networks.\n",
        "\n",
        "9. **learning_rate**:\n",
        "   - Sets the initial learning rate for the AdamW optimizer.\n",
        "\n",
        "10. **weight_decay**:\n",
        "    - Specifies the weight decay to apply to all layers except those with bias or LayerNorm weights, as a regularization technique.\n",
        "\n",
        "11. **optim**:\n",
        "    - Defines the optimizer to use, here specified as a variant of AdamW optimized for certain hardware configurations.\n",
        "\n",
        "12. **lr_scheduler_type**:\n",
        "    - Determines the learning rate schedule to use. \"constant\" means the learning rate stays the same throughout training.\n",
        "\n",
        "13. **max_steps**:\n",
        "    - Overrides `num_train_epochs` by setting the number of training steps. If set to a negative value, it's ignored. We set this to `100` to reduce the training time. That means, that our example training does not use the entire traing set.\n",
        "\n",
        "14. **warmup_ratio**:\n",
        "    - Indicates the proportion of total training steps to use for linear warmup of the learning rate.\n",
        "\n",
        "15. **group_by_length**:\n",
        "    - When enabled, sequences are grouped by length into batches. This can save memory and speed up training.\n",
        "\n",
        "16. **save_steps**:\n",
        "    - Determines how often to save a model checkpoint in terms of training steps.\n",
        "\n",
        "17. **logging_steps**:\n",
        "    - Sets the frequency, in terms of training steps, for logging training progress.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ayWlSa-s0SJ2"
      },
      "outputs": [],
      "source": [
        "# Output directory where the model predictions and checkpoints will be stored\n",
        "output_dir = \"./results\" # @param\n",
        "\n",
        "# Number of training epochs\n",
        "num_train_epochs = 1 # @param\n",
        "\n",
        "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
        "fp16 = False # @param\n",
        "bf16 = False # @param\n",
        "\n",
        "# Batch size per GPU for training\n",
        "per_device_train_batch_size = 4 # @param\n",
        "\n",
        "# Batch size per GPU for evaluation\n",
        "per_device_eval_batch_size = 4 # @param\n",
        "\n",
        "# Number of update steps to accumulate the gradients for\n",
        "gradient_accumulation_steps = 1 # @param\n",
        "\n",
        "# Enable gradient checkpointing\n",
        "gradient_checkpointing = True # @param\n",
        "\n",
        "# Maximum gradient normal (gradient clipping)\n",
        "max_grad_norm = 0.3 # @param\n",
        "\n",
        "# Initial learning rate (AdamW optimizer)\n",
        "learning_rate = 5e-4 # @param\n",
        "\n",
        "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
        "weight_decay = 0.001 # @param\n",
        "\n",
        "# Optimizer to use\n",
        "optim = \"paged_adamw_32bit\" # @param\n",
        "\n",
        "# Learning rate schedule (constant a bit better than cosine)\n",
        "lr_scheduler_type = \"constant\" # @param\n",
        "\n",
        "# Number of training steps (overrides num_train_epochs)\n",
        "max_steps = 100 # @param\n",
        "\n",
        "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
        "warmup_ratio = 0.03 # @param\n",
        "\n",
        "# Group sequences into batches with same length\n",
        "# Saves memory and speeds up training considerably\n",
        "group_by_length = True # @param\n",
        "\n",
        "# Save checkpoint every X updates steps\n",
        "save_steps = 25 # @param\n",
        "\n",
        "# Log every X updates steps\n",
        "logging_steps = 25 # @param"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ypfndp911mbp"
      },
      "source": [
        "Now let's defint the SFTTrainer parameters\n",
        "\n",
        "1. **max_seq_length**:\n",
        "   - This parameter specifies the maximum sequence length to be used.\n",
        "\n",
        "2. **packing**:\n",
        "   - This parameter indicates whether or not to pack multiple short examples into the same input sequence.\n",
        "   - When set to `True`, this technique can be used to increase computational efficiency, particularly in batch processing.\n",
        "\n",
        "3. **device_map**:\n",
        "   - This parameter is a dictionary that maps parts of the model to specific computing devices.\n",
        "   - The entry `{\"\": 0}` specifies that the entire model will be loaded onto GPU 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "A_Yi1p_BR0bA"
      },
      "outputs": [],
      "source": [
        "# Maximum sequence length to use\n",
        "max_seq_length = None\n",
        "\n",
        "# Pack multiple short examples in the same input sequence to increase efficiency\n",
        "packing = False\n",
        "\n",
        "# Load the entire model on the GPU 0\n",
        "device_map = {\"\": 0}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "269-A6vtSNfg"
      },
      "source": [
        "### Lets Load the base model\n",
        "Let's load the Mistral 7B Instruct base model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Rg--UCDDZ43f"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "from pprint import pprint\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from datasets import Dataset, load_dataset\n",
        "from huggingface_hub import notebook_login\n",
        "from peft import LoraConfig, PeftModel\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from trl import SFTTrainer # For supervised finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd00Qz6P2LDn"
      },
      "source": [
        "Load the base model with QLoRA configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QrBd2aB95aoj"
      },
      "outputs": [],
      "source": [
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map\n",
        ")\n",
        "\n",
        "base_model.config.use_cache = False\n",
        "base_model.config.pretraining_tp = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_zPMcIO2M4S"
      },
      "source": [
        "Load tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5STGmYHw2ORV"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxYdQFqo5aol"
      },
      "source": [
        "## Fine-Tuning with QLoRA and Supervised Fine-Tuning\n",
        "\n",
        "We're ready to fine-tune our model using QLoRA. For this tutorial, we'll use the `SFTTrainer` from the `trl` library.\n",
        "\n",
        "In the context of the code below, `target_modules` refers to specific components or layers of a neural network model that will be modified or adapted using LoRA (Low-Rank Adaptation). LoRA is a technique used to adapt pre-trained models with minimal additional parameters, often used in the context of Transformer models. Here's a breakdown of what each module likely represents:\n",
        "\n",
        "1. **q_proj, k_proj, v_proj, o_proj**:\n",
        "   - These refer to the projections for query (q), key (k), value (v), and output (o) in the attention mechanism of a Transformer model.\n",
        "\n",
        "2. **gate_proj**:\n",
        "   - This refer to a projection layer associated with gating mechanisms in the model, such as those found in Gated Recurrent Units (GRUs) or similar structures.\n",
        "\n",
        "3. **up_proj, down_proj**:\n",
        "   - These refer to projection layers used in upsampling or downsampling within the model.\n",
        "\n",
        "4. **lm_head**:\n",
        "   - This refers to the language model head of a Transformer, which is the final layer that produces the output (like the next word in a sequence)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "7b6dde22828c44729f28309d65eae1c6",
            "bb1c6d418b3e40f2bfa5d1258ef6e197",
            "4f36c9b0b6fc4fcfa25ca317731ffd2b",
            "9c7a81a7d03a4b32b2021b750c7cc73c",
            "c0cb94e4f9f64db99d533613742a60ac",
            "f81e097860984800970910f17ed127cc",
            "9bb6773b3281465cb1533261035e686a",
            "c3193cae4e594a238bf8fae3a613ba57",
            "b910b8b28bb348648ec5cb689cd31fe5",
            "19a4e1b8f531484ead63d60ab7610f7c",
            "b43906472dbb401fbd0eb3dfa1e68c4e"
          ]
        },
        "id": "nFjA8D0d2oNz",
        "outputId": "3e6b724e-e9bb-4194-d337-223be8cc96a8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/trl/trainer/sft_trainer.py:246: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
            "  warnings.warn(\n",
            "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25000/25000 [00:08<00:00, 2974.73 examples/s]\n",
            "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        }
      ],
      "source": [
        "# Load LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "        \"lm_head\",\n",
        "    ],\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Set training parameters\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    report_to=\"tensorboard\",\n",
        ")\n",
        "\n",
        "# Set supervised fine-tuning parameters\n",
        "trainer = SFTTrainer(\n",
        "    model=base_model,\n",
        "    train_dataset=train_dataset,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=packing,\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i187wmtT5aol"
      },
      "source": [
        "## Let's start the training process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/pip:6: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  from pkg_resources import load_entry_point\n",
            "\u001b[33mWARNING: The directory '/root/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
            "Collecting pynvml\n",
            "  Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53 kB 962 kB/s eta 0:00:01\n",
            "\u001b[?25hInstalling collected packages: pynvml\n",
            "Successfully installed pynvml-11.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pynvml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pynvml\n",
        "\n",
        "def get_used_memory():\n",
        "    pynvml.nvmlInit()\n",
        "    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
        "    info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
        "    mem = info.used/1024**2\n",
        "    \n",
        "    return mem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2476.875"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_used_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "xWxi9tqgEhyZ",
        "outputId": "0f31a0be-e972-487d-b8ea-f0bd011d8aaf"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 01:38, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>3.300300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.706000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>3.109400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.836300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
          ]
        }
      ],
      "source": [
        "# Train model\n",
        "trainer.train()\n",
        "\n",
        "# Save trained model\n",
        "trainer.model.save_pretrained(new_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "28248.875"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_used_memory()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PixsfVRz38YA"
      },
      "source": [
        "## Merge the fine-tuned model\n",
        "\n",
        "After fine-tuning, we can merge the fine-tuned model with the base model to get a single model that can be used for inference. This is done by using the PEFT. First, let's clean up the GPU memory by deleting the fine-tuned model. You can also restart the runtime to clear the GPU memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ujf17z-7--l5",
        "outputId": "bdac249e-61e7-4637-acfb-981a4f67eb03"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "20729"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Empty VRAM\n",
        "import gc\n",
        "del base_model\n",
        "gc.collect()\n",
        "\n",
        "del trainer\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ypNIoSou-_3j"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-cOpLlC_A48",
        "outputId": "430ede60-296c-4c4e-f3c7-00f87413d8cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1312.875"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_used_memory()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAYoqNyp7m7n"
      },
      "source": [
        "Now, let's load the base model and fine-tuned model and merge them using PEFT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "IRWhV5WZ3-BO"
      },
      "outputs": [],
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map={\"\": 0},\n",
        ")\n",
        "merged_model= PeftModel.from_pretrained(base_model, new_model,)\n",
        "merged_model= merged_model.merge_and_unload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NlvU92-7xnR"
      },
      "source": [
        "Let's save our merged model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "oIYyv_y07zrZ"
      },
      "outputs": [],
      "source": [
        "# Save the merged model\n",
        "merged_model.save_pretrained(\"merged_model\", safe_serialization=True)\n",
        "tokenizer.save_pretrained(\"merged_model\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_hU2be-5aom"
      },
      "source": [
        "## Test the merged model\n",
        "\n",
        "The following code performs the inference stage of the evaluation finetuned Mistral-7B-Instruct model. We define a function called **`classify_sentence`** that is designed to use a pretrained model, likely a variant of a large language model similar to GPT, for sentiment analysis. The description below outlines the steps taken in the function to classify the sentiment of a given sentence as either positive, negative, or possibly neutral. I'll expand on the description by going through the function step-by-step:\n",
        "\n",
        "1. The function accepts a single parameter, `sentence`, which is the text input whose sentiment is to be classified.\n",
        "\n",
        "2. The `sentence` is formatted with the predefined prompt template. This prompt engineering is a common practice when using language models for specific tasks, as it provides context to the model about the task it is supposed to perform.\n",
        "\n",
        "3. The `tokenizer` is applied to the formatted text. Tokenizers convert text into a format that models can understand, which in this case is a series of tokens. The tokenizer is configured to:\n",
        "   - Return tensors compatible with PyTorch (`return_tensors=\"pt\"`).\n",
        "   - Not add special tokens that are usually used to indicate the start and end of a sequence (`add_special_tokens=False`).\n",
        "\n",
        "4. The tokenized input (`encodeds`) is then converted to a PyTorch tensor and moved to the appropriate device (GPU) for inference.\n",
        "\n",
        "5. The inference is performed inside a `torch.no_grad()` context manager, which disables gradient calculations. This is used because we are making predictions, not training the model, and therefore do not need gradients, which would only use extra memory and computational power.\n",
        "\n",
        "6. The `model.generate` function is called to generate a response. This function takes several parameters, such as:\n",
        "   - `**model_inputs`: The tokenized inputs prepared earlier.\n",
        "   - `max_length=8000`: This sets the maximum length of the model's output. The choice of 8000 seems unusually high for sentence classification and might be tailored to specific requirements of the task or the model's capacity.\n",
        "   - `bos_token_id=model.config.bos_token_id`: This specifies the beginning-of-sentence token id, signaling the model where a new sentence starts.\n",
        "   - `eos_token_id=model.config.eos_token_id`: This specifies the end-of-sentence token id, signaling the model where a sentence ends.\n",
        "   - `pad_token_id=model.config.eos_token_id`: This is used for padding shorter sentences to a uniform length. It's unusual to see the end-of-sentence token used for padding, which could be a specific requirement of this model or a mistake.\n",
        "\n",
        "7. After the model generates a response, `torch.cuda.empty_cache()` is called to free up unused memory on the GPU. This is helpful in managing GPU resources, especially when processing multiple requests or dealing with large models.\n",
        "\n",
        "8. Finally, the `tokenizer.decode` function is used to convert the model's output tokens back into human-readable text. The `skip_special_tokens=True` argument removes any special tokens (like padding or end-of-sentence tokens) from the output. The function also skips the input tokens (`outputs[0][len(model_inputs[\"input_ids\"][0]):]`) to only return the newly generated text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lf05Fbfe224z",
        "outputId": "53383d91-c5d1-44bf-9216-d6aa13d4ca0d"
      },
      "outputs": [],
      "source": [
        "device = \"cuda:0\"\n",
        "prompt = \"\"\"<s>[INST]You are a sentiment classifier. Use only \"positive\" or \"negative\".\n",
        "\n",
        "Sentence: {sentence}[\\INST]\n",
        "\"\"\"\n",
        "def classify_sentence(sentence, model, print_memory:bool=False):\n",
        "  text = prompt.format(sentence=sentence)\n",
        "  encodeds = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
        "  model_inputs = encodeds.to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = model.generate(**model_inputs,max_new_tokens=1,bos_token_id=model.config.bos_token_id,\n",
        "                                eos_token_id=model.config.eos_token_id,\n",
        "                                pad_token_id=model.config.eos_token_id\n",
        "                             )\n",
        "    \n",
        "    if print_memory:\n",
        "      mem = get_used_memory()\n",
        "      print(f\"Used {mem} MB for inference\")\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  return tokenizer.decode(outputs[0][len(model_inputs[\"input_ids\"][0]):], skip_special_tokens=True)\n",
        "\n",
        "classify_sentence(\"This movie is too bad\", merged_model, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O28cCRIP4ZsJ"
      },
      "source": [
        "The code below uses the **`classify_sentence`** function to make predictions on the test dataset. We loop through the test dataset and apply the **`classify_sentence`** function to each example. The predictions are stored in a list called **`predictions`**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3awVyved9AgW",
        "outputId": "d18eac4f-08c7-4d7c-e64e-f62eed377655"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/25000 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 13151/25000 [10:48<10:42, 18.43it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2259 > 2048). Running this sequence through the model will result in indexing errors\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25000/25000 [19:39<00:00, 21.19it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "predictions = []\n",
        "references = test_dataset[\"class\"]\n",
        "for item in tqdm(test_dataset):\n",
        "  predicted = classify_sentence(item['text'], merged_model)\n",
        "  predictions.append(predicted)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlWk2IL9L_ay",
        "outputId": "b28144f1-5ecf-4282-dbcb-19d60ccad920"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('positive', 20497),\n",
              " ('negative', 3974),\n",
              " ('\\n', 264),\n",
              " ('', 183),\n",
              " ('<', 46),\n",
              " ('-', 20),\n",
              " ('2', 10),\n",
              " ('4', 3),\n",
              " ('*', 3)]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "Counter(predictions).most_common()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Counter({'positive': 20497,\n",
              "         'negative': 3974,\n",
              "         '\\n': 264,\n",
              "         '': 183,\n",
              "         '2': 10,\n",
              "         '<': 46,\n",
              "         '-': 20,\n",
              "         '4': 3,\n",
              "         '*': 3})"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Counter(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "XXEyUMHwVKJ-"
      },
      "outputs": [],
      "source": [
        "predictions = [\"positive\" if p == \"pos\" else p for p in predictions] #Useless"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyKJR-CmVwxF"
      },
      "source": [
        "# Evaluation Metric\n",
        "\n",
        "To compute accuracy, we need to define a custom **`string_accuracy`** function since model outputs text rather than numerical values. Therefore, we cannot use the built-in accuracy function directly, which expects numerical values as inputs.\n",
        "\n",
        "The following code defines the **`string_accuracy`** function. It takes two lists of strings as inputs, **`predictions`** and **`references`**. The function computes accuracy by counting the number of predictions that match the corresponding reference and dividing by the total number of predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "ZtITggRdVvsD"
      },
      "outputs": [],
      "source": [
        "def string_accuracy(predictions, references):\n",
        "    correct = sum([1 for p, r in zip(predictions, references) if p.lower() == r.lower()])\n",
        "    total = len(predictions)\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SltQDZS9zyt",
        "outputId": "551e7431-501e-4230-d0e8-ae0d354a5c5c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.645"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy = string_accuracy(predictions=predictions, references=references)\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/pip:6: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  from pkg_resources import load_entry_point\n",
            "\u001b[33mWARNING: The directory '/root/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
            "Collecting seaborn\n",
            "  Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 294 kB 893 kB/s eta 0:00:01\n",
            "\u001b[?25hCollecting numpy!=1.24.0,>=1.20\n",
            "  Downloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.3 MB 12.1 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.8/dist-packages (from seaborn) (1.3.5)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.8/dist-packages (from seaborn) (3.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.2->seaborn) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.2->seaborn) (2024.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (9.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2->seaborn) (1.16.0)\n",
            "Installing collected packages: numpy, seaborn\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "Successfully installed numpy-1.24.4 seaborn-0.13.2\n"
          ]
        }
      ],
      "source": [
        "!pip install seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "predictions_np = np.array(predictions)\n",
        "mask = predictions_np == \"positive\"\n",
        "mask = np.bitwise_or(mask, predictions_np == \"negative\")\n",
        "\n",
        "predictions_changed = np.array(predictions)\n",
        "predictions_changed[np.bitwise_not(mask)] = \"negative\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "qO0ibw-L-Y8t",
        "outputId": "2a7ce594-b983-4fc2-ca76-de02f0c88531"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# Convert labels to a numerical form\n",
        "labels = {\"positive\": 1, \"negative\": 0}\n",
        "y_true_num = [labels[label] for label in references]\n",
        "y_pred_num = [labels[label] for label in predictions_changed]\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(np.array(y_true_num), np.array(y_pred_num))\n",
        "\n",
        "# Plot the confusion matrix using seaborn\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Negative\", \"Positive\"], yticklabels=[\"Negative\", \"Positive\"])\n",
        "\n",
        "# Labels, title, and ticks\n",
        "ax.set_ylabel('Actual Label')\n",
        "ax.set_xlabel('Predicted Label')\n",
        "ax.set_title('Confusion Matrix')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array(['negative', 'positive'], dtype='<U8'), array([12500, 12500]))"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.unique(np.array(train_dataset[\"class\"]), return_counts=True) #Dataset balanceado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AcurÃ¡cia prÃ©-finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1312.875"
            ]
          },
          "execution_count": 110,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "del base_model\n",
        "gc.collect()\n",
        "\n",
        "del merged_model\n",
        "gc.collect()\n",
        "\n",
        "del new_model\n",
        "gc.collect()\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "get_used_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map\n",
        ")\n",
        "\n",
        "base_model.config.use_cache = False\n",
        "base_model.config.pretraining_tp = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25000/25000 [42:02<00:00,  9.91it/s]  \n"
          ]
        }
      ],
      "source": [
        "predictions_base = []\n",
        "references = test_dataset[\"class\"]\n",
        "for item in tqdm(test_dataset):\n",
        "  predicted = classify_sentence(item['text'], base_model)\n",
        "  predictions_base.append(predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Counter({'\\n': 22589, '': 2411})"
            ]
          },
          "execution_count": 122,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Counter(predictions_base)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "execution_count": 121,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy_base = string_accuracy(predictions=predictions_base, references=references)\n",
        "accuracy_base"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "19a4e1b8f531484ead63d60ab7610f7c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f36c9b0b6fc4fcfa25ca317731ffd2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3193cae4e594a238bf8fae3a613ba57",
            "max": 67349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b910b8b28bb348648ec5cb689cd31fe5",
            "value": 67349
          }
        },
        "4fc5d8efd4164cf98c78817dca300750": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_69db67eed1ac460cb00fd7ae57fb8336",
              "IPY_MODEL_f27c32a8a41e4766852235fe7657eb18",
              "IPY_MODEL_9054a08a953b4664af94a5d834945950"
            ],
            "layout": "IPY_MODEL_c0e35d69570b4f0bb45967d7e420226c"
          }
        },
        "69db67eed1ac460cb00fd7ae57fb8336": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ccdf7cde1a834ffdb45261016e8018da",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ae9b48d0776e453a961aad557a490be5",
            "value": "Map:â€‡100%"
          }
        },
        "731e3053925a411b9357632b78aa0dc0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79e14c8e280f4bb7a860825f85737a93": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7b6dde22828c44729f28309d65eae1c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bb1c6d418b3e40f2bfa5d1258ef6e197",
              "IPY_MODEL_4f36c9b0b6fc4fcfa25ca317731ffd2b",
              "IPY_MODEL_9c7a81a7d03a4b32b2021b750c7cc73c"
            ],
            "layout": "IPY_MODEL_c0cb94e4f9f64db99d533613742a60ac"
          }
        },
        "9054a08a953b4664af94a5d834945950": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_731e3053925a411b9357632b78aa0dc0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ba875b0928a74707b6ca9cf23e66c2f9",
            "value": "â€‡67349/67349â€‡[00:10&lt;00:00,â€‡3977.15â€‡examples/s]"
          }
        },
        "9bb6773b3281465cb1533261035e686a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c7a81a7d03a4b32b2021b750c7cc73c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19a4e1b8f531484ead63d60ab7610f7c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b43906472dbb401fbd0eb3dfa1e68c4e",
            "value": "â€‡67349/67349â€‡[00:09&lt;00:00,â€‡6706.48â€‡examples/s]"
          }
        },
        "ab21c7ec21104159ab595d7d65913e24": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae9b48d0776e453a961aad557a490be5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b43906472dbb401fbd0eb3dfa1e68c4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b910b8b28bb348648ec5cb689cd31fe5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ba875b0928a74707b6ca9cf23e66c2f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb1c6d418b3e40f2bfa5d1258ef6e197": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f81e097860984800970910f17ed127cc",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_9bb6773b3281465cb1533261035e686a",
            "value": "Map:â€‡100%"
          }
        },
        "c0cb94e4f9f64db99d533613742a60ac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0e35d69570b4f0bb45967d7e420226c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3193cae4e594a238bf8fae3a613ba57": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccdf7cde1a834ffdb45261016e8018da": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f27c32a8a41e4766852235fe7657eb18": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab21c7ec21104159ab595d7d65913e24",
            "max": 67349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_79e14c8e280f4bb7a860825f85737a93",
            "value": 67349
          }
        },
        "f81e097860984800970910f17ed127cc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
