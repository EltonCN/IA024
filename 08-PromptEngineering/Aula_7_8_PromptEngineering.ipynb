{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering com IMDB\n",
    "\n",
    "Elton Cardoso do Nascimento - 233840\n",
    "\n",
    "> Utilizar o groq.com para usar a API do Llama 3 70B para fazer análise de sentimentos do IMDB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao todo 4 técnicas serão testadas: zero-shot, few-shot, prompts dinâmicos e chain-of-thought\n",
    "\n",
    "Vamos começar importando todas as bibliotecas que serão utilizadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os # Operações com o SO (ler variáveis de ambiente)\n",
    "import random # Operações randômicas\n",
    "from concurrent.futures import ThreadPoolExecutor # Paralelização\n",
    "import threading # Paralelização\n",
    "import time # Temporização\n",
    "from typing import Optional, List # Type hints\n",
    "\n",
    "import datasets # Obter o dataset IMDB\n",
    "import groq # API para utilizar o Llama 3\n",
    "import tqdm # Print de progresso\n",
    "import torch # ML\n",
    "import pandas # Data manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E instalamos algumas outras que serão necessárias para obter o modelo BERT utilizado para realizar a técnica de \"prompts dinâmicos\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm boto3 requests regex sentencepiece sacremoses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interface para o Groq\n",
    "\n",
    "Para realizar a inferência utilizando a API do Groq, criamos uma classe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroqInterface:\n",
    "    '''\n",
    "    Interface for using the Groq API\n",
    "\n",
    "    Implements a rate limit control for multi-threading use. \n",
    "    '''\n",
    "\n",
    "    _client = None \n",
    "\n",
    "    LLAMA3_70B = \"llama3-70b-8192\"\n",
    "\n",
    "    rate_lock = threading.Lock()\n",
    "\n",
    "    def __init__(self, model:Optional[str]=None):\n",
    "        '''\n",
    "        GroqInterface constructor.\n",
    "\n",
    "        Args:\n",
    "            model (str, optional): model to use. Llama3 70B is used if None. Default is None\n",
    "        '''\n",
    "        \n",
    "        if GroqInterface._client is None:\n",
    "            api_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "\n",
    "            if api_key is None:\n",
    "                raise RuntimeError(\"API key is not in the environment variables ('GROQ_API_KEY' variable is not set).\")\n",
    "\n",
    "            GroqInterface._client = groq.Groq(api_key=api_key)\n",
    "\n",
    "        if model is None:\n",
    "            model = GroqInterface.LLAMA3_70B\n",
    "        self._model = model\n",
    "\n",
    "    def __call__(self, prompt:str) -> str:\n",
    "        '''\n",
    "        Generates the model response\n",
    "\n",
    "        Args:\n",
    "            prompt (str): prompt to send to the model.\n",
    "\n",
    "        Returns:\n",
    "            str: model response. \n",
    "        '''\n",
    "        done = False\n",
    "        while not done:\n",
    "\n",
    "            try:\n",
    "                GroqInterface.rate_lock.acquire()\n",
    "                GroqInterface.rate_lock.release()\n",
    "\n",
    "                chat_completion = GroqInterface._client.chat.completions.create(\n",
    "                        messages=[\n",
    "                            {\n",
    "                                \"role\": \"user\",\n",
    "                                \"content\": prompt,\n",
    "                            }\n",
    "                        ],\n",
    "                        model=self._model,\n",
    "                    )\n",
    "                \n",
    "                done = True\n",
    "            except groq.RateLimitError as exception:\n",
    "                GroqInterface.error = exception\n",
    "                if not GroqInterface.rate_lock.locked():\n",
    "                    GroqInterface.rate_lock.acquire()\n",
    "                    time.sleep(2)\n",
    "                    GroqInterface.rate_lock.release()\n",
    "\n",
    "        return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos testar a interface e verificar que obtemos corretamente a resposta do modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_interface = GroqInterface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi! It's nice to meet you. Is there something I can help you with, or would you like to chat?\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groq_interface(\"Hi!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para facilitar o uso, definimos uma classe específica para análise de sentimento, que realiza um pós-processamento da saída obtida. Observe que, caso o modelo não seja claro na sua resposta que o review é positivo ou negativo um valor aleatório é utilizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSITIVE = 1\n",
    "NEGATIVE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroqSentimentInterface(GroqInterface):\n",
    "    '''\n",
    "    GroqInterface with sentiment analisys post-processing.\n",
    "    '''\n",
    "\n",
    "    def __call__(self, prompt: str) -> int:\n",
    "        '''\n",
    "        Generates the model response for sentiment analisys.\n",
    "\n",
    "        If the model is ambiguous in its response, a random one is generated.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): prompt to send to the model.\n",
    "\n",
    "        Returns:\n",
    "            int: model response. POSITIVE if positive, NEGATIVE otherwise.\n",
    "        '''\n",
    "\n",
    "        response = super().__call__(prompt)\n",
    "        response = response.lower()\n",
    "\n",
    "        if \"positive\" in response and \"negative\" not in response:\n",
    "            return POSITIVE\n",
    "        if \"negative\" in response and \"positive\" not in response:\n",
    "            return NEGATIVE\n",
    "        \n",
    "        return random.choice([POSITIVE, NEGATIVE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_sentiment = GroqSentimentInterface()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de podermos realizar a avaliação das técnicas, precisamos ainda preparar o dataset. Devido a longa demora para obter respostas da API e o limite de prompts por minuto, uma quantidade reduzida do dataset é utilizada para teste e validação, com apenas 100 elementos cada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "executor = ThreadPoolExecutor(max_workers=2)\n",
    "\n",
    "trainbase_future = executor.submit(datasets.load_dataset, \"imdb\", split=\"train\")\n",
    "test_future = executor.submit(datasets.load_dataset, \"imdb\", split='test')\n",
    "\n",
    "trainbase_dataset = trainbase_future.result()\n",
    "testbase_dataset = test_future.result()\n",
    "\n",
    "train_val_dataset = trainbase_dataset.train_test_split(test_size=100, shuffle=True, seed=78)\n",
    "discard_test_dataset = testbase_dataset.train_test_split(test_size=100, shuffle=True, seed=78)\n",
    "\n",
    "train_dataset = train_val_dataset[\"train\"]\n",
    "val_dataset = train_val_dataset[\"test\"]\n",
    "test_dataset = discard_test_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24900, 100, 100)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para a técnica de zero-shot, precisamos apenas preparar um prompt que solicita a classificação ao modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prompt_zero = '''Classify if the movie review is POSITIVE or NEGATIVE: \n",
    "                Review:\n",
    "                {review}\n",
    "\n",
    "                Sentiment:\n",
    "                POSITIVE OR NEGATIVE: \n",
    "                '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = base_prompt_zero.format(review=train_dataset[-1][\"text\"])\n",
    "\n",
    "groq_sentiment(prompt), train_dataset[-1][\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparamos a função para realizar a avaliação de um sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_zero(text:str, label:int) -> bool:\n",
    "    '''\n",
    "    Evaluates the zero-shot response\n",
    "\n",
    "    Args:\n",
    "        text (str): review to evaluate.\n",
    "        label (int): review expected label.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the model classifies correctly.\n",
    "    '''\n",
    "    prompt = base_prompt_zero.format(review=text)\n",
    "    result = groq_sentiment(prompt)\n",
    "\n",
    "    return result == label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E calculamos a acurácia utilizando os dados de validação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [07:30<00:00,  4.50s/it] \n"
     ]
    }
   ],
   "source": [
    "executor = ThreadPoolExecutor(max_workers=4) #More workers -> More RateLimit exceptions\n",
    "\n",
    "futures = []\n",
    "for data in val_dataset:\n",
    "    future = executor.submit(evaluate_zero, **data)\n",
    "    futures.append(future)\n",
    "\n",
    "correct_zero = 0\n",
    "for future in tqdm.tqdm(futures):\n",
    "    correct_zero += future.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia - Zero-shot - Validação: 88.0%\n"
     ]
    }
   ],
   "source": [
    "accuracy_zero = correct_zero/len(val_dataset)\n",
    "print(f\"Acurácia - Zero-shot - Validação: {accuracy_zero*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acurácia - Zero-shot - Validação: 88.0%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para a técnica de few-shot prompt, vamos enviar para o modelo 3 exemplos: um com o template de como a resposta deve funcionar, um positivo e um negativo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_prompt_few = '''Classify if the movie review is positive or negative: \n",
    "                Review:\n",
    "                Movie review\n",
    "\n",
    "                Sentiment:\n",
    "                ONLY POSITIVE OR NEGATIVE\n",
    "\n",
    "                Classify if this movie review is positive or negative:\n",
    "                Review:\n",
    "                {example1}\n",
    "\n",
    "                Sentiment:\n",
    "                {response1}\n",
    "\n",
    "                Classify if this movie review is positive or negative:\n",
    "                Review:\n",
    "                {example2}\n",
    "\n",
    "                Sentiment:\n",
    "                {response2}\n",
    "\n",
    "                Classify if this movie review is positive or negative:\n",
    "                Review:\n",
    "                {{review}}\n",
    "                \n",
    "                Sentiment:\n",
    "                \n",
    "                '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_example = None\n",
    "negative_example = None\n",
    "\n",
    "i = 0\n",
    "while positive_example is None or negative_example is None:\n",
    "    if train_dataset[i][\"label\"] == POSITIVE:\n",
    "        positive_example = train_dataset[i]\n",
    "    else:\n",
    "        negative_example = train_dataset[i]\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify if the movie review is positive or negative: \n",
      "                Review:\n",
      "                Movie review\n",
      "\n",
      "                Sentiment:\n",
      "                ONLY POSITIVE OR NEGATIVE\n",
      "\n",
      "                Classify if this movie review is positive or negative:\n",
      "                Review:\n",
      "                but I want to say I cannot agree more with Moira.<br /><br />What a wonderful film.<br /><br />I was thinking about it just this morning, wanting to give advice to some dopey sod who'd lost money on his debit card through fraud, and wanted to say 'Keep thy money in thine pocket' and realised I was talking like James Mason.<br /><br />Even tho he didn't say those words, I still think he would! I've never forgotten 'Are ye carrying?' in his reconciliation with his son, Hywel Bennet: 'Always have money in thine pocket!' Good advice.<br /><br />Not enough kids have fathers with such unforgiving but well-meant attitudes any more. Or any father at all.<br /><br />It would be a good thing for us to reinstate 'thee', 'thy' and 'thine' in our language to show we care. It is only the same as 'tutoyer' in French or 'du' in German.<br /><br />Addendum: I just realised that a lot of my remarks were about James Mason in The Family Way!<br /><br />I think it's because I mixed up Susan George with Hayley Mills. Well, easy mistake.<br /><br />I stand by the comments tho'.<br /><br />And Spring and Port Wine is so very similar to The Family Way.<br /><br />When you took a girlfriend to the pictures in those days, you really had something to say and talk about afterwards, something that affected your knowledge of the world and your personal development.<br /><br />Theatrical experiences are almost real, and they are important in helping young people to grow up.<br /><br />It doesn't happen now, I think, that teenagers can just go to the pics like we did.\n",
      "\n",
      "                Sentiment:\n",
      "                POSITIVE\n",
      "\n",
      "                Classify if this movie review is positive or negative:\n",
      "                Review:\n",
      "                \"A total waste of time\" Just throw in a few explosions, non stop fighting, exotic cars a deranged millionaire, slow motion computer generated car crashes and last but not least a Hugh Hefner like character with wall to wall hot babes, and mix in a blender and you will have this sorry excuse for a movie. I really got a laugh out of the \"Dr. Evil\" like heavily fortified compound. The plot was somewhere between preposterous and non existent. How many millionaires are willing to make a 25 million dollar bet on a car race? Answer: 4 but, didn't they become millionaires through fiscal responsibility? This was written for pubescent males, it plays like a video game. I did enjoy the Gulfstream II landing in the desert though.\n",
      "\n",
      "                Sentiment:\n",
      "                NEGATIVE\n",
      "\n",
      "                Classify if this movie review is positive or negative:\n",
      "                Review:\n",
      "                {review}\n",
      "                \n",
      "                Sentiment:\n",
      "                \n",
      "                \n"
     ]
    }
   ],
   "source": [
    "base_prompt_few = raw_prompt_few.format(example1=positive_example[\"text\"], response1=\"POSITIVE\", \n",
    "                                        example2=negative_example[\"text\"], response2=\"NEGATIVE\")\n",
    "\n",
    "print(base_prompt_few)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificamos que está funcionando corretamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = base_prompt_few.format(review=train_dataset[-1][\"text\"])\n",
    "\n",
    "groq_sentiment(prompt), train_dataset[-1][\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparamos a função de avaliação e calculamos a acurácia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_few(text:str, label:int) -> bool:\n",
    "    '''\n",
    "    Evaluates the few-shot response\n",
    "\n",
    "    Args:\n",
    "        text (str): review to evaluate.\n",
    "        label (int): review expected label.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the model classifies correctly.\n",
    "    '''\n",
    "     \n",
    "    prompt = base_prompt_few.format(review=text)\n",
    "    result = groq_sentiment(prompt)\n",
    "\n",
    "    return result == label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_few(**train_dataset[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [10:14<00:00,  6.15s/it]\n"
     ]
    }
   ],
   "source": [
    "executor = ThreadPoolExecutor(max_workers=4) #More workers -> More RateLimit exceptions\n",
    "\n",
    "futures = []\n",
    "for data in val_dataset:\n",
    "    future = executor.submit(evaluate_few, **data)\n",
    "    futures.append(future)\n",
    "\n",
    "correct_few = 0\n",
    "for future in tqdm.tqdm(futures):\n",
    "    correct_few += future.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia - Few-shot - Validação: 98.0%\n"
     ]
    }
   ],
   "source": [
    "accuracy_few = correct_few/len(val_dataset)\n",
    "print(f\"Acurácia - Few-shot - Validação: {accuracy_few*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Dinâmico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o prompt dinâmico, iremos enviar um exemplo positivo e um negativo próximo do review que queremos classificar. Para isso, iremos gerar representações vetoriais de todos os prompts de treino disponíveis utilizando o modelo BERT e compará-las com a representação vetorial do review que queremos classificar.\n",
    "\n",
    "Começamos carregando o BERT e seu tokenizador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
      "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-cased')\n",
    "bert = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "bert.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desativamos o cálculo de gradientes e colocamos o modelo em modo de avaliação, visto que nenhum treino será realizado neste projeto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fca8b2a6970>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert.eval()\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para melhorar a eficiência do cálculo das representações vetoriais em batchs, ordenamos os dados de treino pelo tamanho do texto, para tentarmos manter representações tokenizadas de tamanhos similares próximas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmp_func(series:pandas.Series) -> List[int]:\n",
    "    '''\n",
    "    Compares the elements of the series by the lenght.\n",
    "\n",
    "    Args:\n",
    "        series (pandas.Series): series to compare.\n",
    "\n",
    "    Returns:\n",
    "        List[int]: elements sizes.\n",
    "    '''\n",
    "    sizes = []\n",
    "    for element in series:\n",
    "        sizes.append(len(element))\n",
    "    return sizes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_dataset.to_pandas()\n",
    "df.sort_values(by=[\"text\"], key=cmp_func, #By text size\n",
    "                inplace=True, #Inplace \n",
    "                ascending=False) #Descending (if memory error, first)\n",
    "train_dataset_sorted = datasets.Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos as representações vetoriais dos dados de treino:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = torch.Tensor(0, 768).float()\n",
    "train_vectors = train_vectors.to(device)\n",
    "\n",
    "for i in range(0, len(train_dataset_sorted), 256):\n",
    "    start = i\n",
    "    end = min(i+256, len(train_dataset_sorted))\n",
    "    \n",
    "    tokens = bert_tokenizer(train_dataset_sorted[start:end][\"text\"], return_tensors=\"pt\", #Return as torch tensor \n",
    "                                 padding=True, #Add padding to small sequences\n",
    "                                 return_token_type_ids=False, #Don't return sequence mask (only one sequence)\n",
    "                                 truncation=True) #Truncate big sentences (max = 512 tokens, with CLS and SEP))\n",
    "    \n",
    "    input_ids = tokens[\"input_ids\"].to(device)\n",
    "    attention_mask = tokens[\"attention_mask\"].to(device)\n",
    "\n",
    "    result = bert(input_ids = input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    train_vectors = torch.cat((train_vectors, result[\"last_hidden_state\"][:, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E dos dados de validação, que não precisam de batchs pela quantidade reduzida de dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = bert_tokenizer(val_dataset[\"text\"], return_tensors=\"pt\", #Return as torch tensor \n",
    "                                 padding=True, #Add padding to small sequences\n",
    "                                 return_token_type_ids=False, #Don't return sequence mask (only one sequence)\n",
    "                                 truncation=True) #Truncate big sentences (max = 512 tokens, with CLS and SEP))\n",
    "\n",
    "input_ids = tokens[\"input_ids\"].to(device)\n",
    "attention_mask = tokens[\"attention_mask\"].to(device)\n",
    "\n",
    "result = bert(input_ids = input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "val_vectors = result[\"last_hidden_state\"][:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comparar os dados, utilizamos similaridade de cosseno com as representações vetoriais:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors_norm = torch.nn.functional.normalize(train_vectors)\n",
    "val_vectors_norm = torch.nn.functional.normalize(val_vectors)\n",
    "\n",
    "val_cosine_similarity = val_vectors_norm @ train_vectors_norm.T  #[val_index, train_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E obtemos uma amostra positiva e outra negativa para cada sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = torch.argsort(val_cosine_similarity, dim=1)\n",
    "\n",
    "val_examples = []\n",
    "for i in range(len(val_dataset)):\n",
    "    positive = None\n",
    "    negative = None\n",
    "\n",
    "    for j in range(len(train_dataset)):\n",
    "        index = indexes[i, j].item()\n",
    "\n",
    "        if train_dataset_sorted[index][\"label\"] == POSITIVE:\n",
    "            positive = train_dataset_sorted[index][\"text\"]\n",
    "        else:\n",
    "            negative = train_dataset_sorted[index][\"text\"]\n",
    "\n",
    "        if positive is not None and negative is not None:\n",
    "            example = {\"positive\":positive, \"negative\":negative}\n",
    "            val_examples.append(example)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checamos se o prompt funciona:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = raw_prompt_few.format(example1=val_examples[0][\"positive\"], response1=\"POSITIVE\", \n",
    "                                        example2=val_examples[0][\"negative\"], response2=\"NEGATIVE\")\n",
    "prompt.format(review=val_dataset[0][\"text\"])\n",
    "\n",
    "groq_sentiment(prompt), val_dataset[0][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dynamic(index:int) -> bool:\n",
    "    '''\n",
    "    Evaluates the dynamic prompt response\n",
    "\n",
    "    Args:\n",
    "        index (int): index of the review in the validation dataset to evaluate\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the model classifies correctly.\n",
    "    '''\n",
    "    \n",
    "    prompt = raw_prompt_few.format(example1=val_examples[index][\"positive\"], response1=\"POSITIVE\", \n",
    "                                        example2=val_examples[index][\"negative\"], response2=\"NEGATIVE\")\n",
    "    prompt = prompt.format(review=val_dataset[index][\"text\"])\n",
    "\n",
    "    result = groq_sentiment(prompt)\n",
    "\n",
    "    label = val_dataset[index][\"label\"]\n",
    "\n",
    "    return result == label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_dynamic(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E realizamos o cálculo da acurácia no dataset de validação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [08:21<00:00,  5.02s/it] \n"
     ]
    }
   ],
   "source": [
    "executor = ThreadPoolExecutor(max_workers=4) #More workers -> More RateLimit exceptions\n",
    "\n",
    "futures = []\n",
    "for i in range(len(val_dataset)):\n",
    "    future = executor.submit(evaluate_dynamic, i)\n",
    "    futures.append(future)\n",
    "\n",
    "correct_dynamic = 0\n",
    "for future in tqdm.tqdm(futures):\n",
    "    correct_dynamic += future.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia - Dinâmico - Validação: 95.0%\n"
     ]
    }
   ],
   "source": [
    "accuracy_dynamic = correct_dynamic/len(val_dataset)\n",
    "print(f\"Acurácia - Dinâmico - Validação: {accuracy_dynamic*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coletando os tempos utilizados em cada parte podemos obter o tempo total para utilizar esta técnica:\n",
    "\n",
    "- Gerar vetores para os dados de treino: 2 min 50,4 s\n",
    "- Gerar vetores para os dados de validação: 0,0 s\n",
    "- Calcular similaridade de cosseno: 0,0 s\n",
    "- Procurar exemplos: 0,2 s\n",
    "- Chamadas prompts: 8 min 21,9 s\n",
    "\n",
    "Total: 11 min 12,5 s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain-of-Thought"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para a técnica de chain-of-thought, vamos adicionar um campo de \"reasoning\" aos exemplos no prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_prompt_cot = '''Classify if the movie review is positive or negative: \n",
    "Review:\n",
    "Movie review\n",
    "\n",
    "Sentiment:\n",
    "ONLY POSITIVE OR NEGATIVE\n",
    "\n",
    "Classify if this movie review is positive or negative:\n",
    "Review:\n",
    "{example1}\n",
    "\n",
    "Reasoning:\n",
    "{reasoning1}\n",
    "\n",
    "Sentiment:\n",
    "{response1}\n",
    "\n",
    "Classify if this movie review is positive or negative:\n",
    "Review:\n",
    "{example2}\n",
    "\n",
    "Reasoning:\n",
    "{reasoning2}\n",
    "\n",
    "Sentiment:\n",
    "{response2}\n",
    "\n",
    "Classify if this movie review is positive or negative:\n",
    "Review:\n",
    "{{review}}\n",
    "\n",
    "Reasoning:\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O \"reasoning\" dos exemplos positivos e negativos a seguir foram gerados utilizando também o Llama3 70B, porém com a interface gráfica online do Groq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"but I want to say I cannot agree more with Moira.<br /><br />What a wonderful film.<br /><br />I was thinking about it just this morning, wanting to give advice to some dopey sod who'd lost money on his debit card through fraud, and wanted to say 'Keep thy money in thine pocket' and realised I was talking like James Mason.<br /><br />Even tho he didn't say those words, I still think he would! I've never forgotten 'Are ye carrying?' in his reconciliation with his son, Hywel Bennet: 'Always have money in thine pocket!' Good advice.<br /><br />Not enough kids have fathers with such unforgiving but well-meant attitudes any more. Or any father at all.<br /><br />It would be a good thing for us to reinstate 'thee', 'thy' and 'thine' in our language to show we care. It is only the same as 'tutoyer' in French or 'du' in German.<br /><br />Addendum: I just realised that a lot of my remarks were about James Mason in The Family Way!<br /><br />I think it's because I mixed up Susan George with Hayley Mills. Well, easy mistake.<br /><br />I stand by the comments tho'.<br /><br />And Spring and Port Wine is so very similar to The Family Way.<br /><br />When you took a girlfriend to the pictures in those days, you really had something to say and talk about afterwards, something that affected your knowledge of the world and your personal development.<br /><br />Theatrical experiences are almost real, and they are important in helping young people to grow up.<br /><br />It doesn't happen now, I think, that teenagers can just go to the pics like we did.\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_example[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_reasoning = '''This review is positive for several reasons:\n",
    "\n",
    "*. The reviewer starts by praising the film, stating that it is \"a wonderful film\" and expresses how it has affected their thoughts and words.\n",
    "*. They mention specific scenes and characters from the film, demonstrating their engagement and appreciation for the story.\n",
    "*. The reviewer draws parallels between the film and their own life, mentioning how the advice given in the film relates to their own experiences and emotions.\n",
    "*. They express nostalgia for a bygone era, mentioning the past when one could take a girlfriend to the pictures and have meaningful conversations about the film afterwards.\n",
    "*. The reviewer values the impact of theatrical experiences on personal development and feels that this is lacking in modern times.\n",
    "*. Despite making a mistake by mixing up actresses, the reviewer stands by their comments and maintains their positive sentiment towards the film.\n",
    "\n",
    "Overall, the review is positive because it shows appreciation for the film, its characters, and its themes, as well as the reviewer's nostalgia for the past and the significance of theatrical experiences in personal growth.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"A total waste of time\" Just throw in a few explosions, non stop fighting, exotic cars a deranged millionaire, slow motion computer generated car crashes and last but not least a Hugh Hefner like character with wall to wall hot babes, and mix in a blender and you will have this sorry excuse for a movie. I really got a laugh out of the \"Dr. Evil\" like heavily fortified compound. The plot was somewhere between preposterous and non existent. How many millionaires are willing to make a 25 million dollar bet on a car race? Answer: 4 but, didn\\'t they become millionaires through fiscal responsibility? This was written for pubescent males, it plays like a video game. I did enjoy the Gulfstream II landing in the desert though.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_example[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_reasoning = '''This review is negative because of the following reasons:\n",
    "\n",
    "* The reviewer calls the movie \"a total waste of time\" and \"a sorry excuse for a movie\", which indicates a strong negative opinion.\n",
    "* The reviewer sarcastically lists various ingredients that they think were thrown together to make the movie, implying that the film is shallow and lacks substance.\n",
    "* They criticize the plot, calling it \"preposterous\" and \"non-existent\".\n",
    "* They question the plot's plausibility, suggesting that the movie's concept is unrealistic.\n",
    "* They imply that the movie is only suitable for \"pubescent males\" and that it's more like a video game than a serious film.\n",
    "* The only positive comment, about the Gulfstream II landing, is brief and doesn't outweigh the overall negative tone of the review.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montamos o prompt base com os \"reasonings\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify if the movie review is positive or negative: \n",
      "Review:\n",
      "Movie review\n",
      "\n",
      "Sentiment:\n",
      "ONLY POSITIVE OR NEGATIVE\n",
      "\n",
      "Classify if this movie review is positive or negative:\n",
      "Review:\n",
      "but I want to say I cannot agree more with Moira.<br /><br />What a wonderful film.<br /><br />I was thinking about it just this morning, wanting to give advice to some dopey sod who'd lost money on his debit card through fraud, and wanted to say 'Keep thy money in thine pocket' and realised I was talking like James Mason.<br /><br />Even tho he didn't say those words, I still think he would! I've never forgotten 'Are ye carrying?' in his reconciliation with his son, Hywel Bennet: 'Always have money in thine pocket!' Good advice.<br /><br />Not enough kids have fathers with such unforgiving but well-meant attitudes any more. Or any father at all.<br /><br />It would be a good thing for us to reinstate 'thee', 'thy' and 'thine' in our language to show we care. It is only the same as 'tutoyer' in French or 'du' in German.<br /><br />Addendum: I just realised that a lot of my remarks were about James Mason in The Family Way!<br /><br />I think it's because I mixed up Susan George with Hayley Mills. Well, easy mistake.<br /><br />I stand by the comments tho'.<br /><br />And Spring and Port Wine is so very similar to The Family Way.<br /><br />When you took a girlfriend to the pictures in those days, you really had something to say and talk about afterwards, something that affected your knowledge of the world and your personal development.<br /><br />Theatrical experiences are almost real, and they are important in helping young people to grow up.<br /><br />It doesn't happen now, I think, that teenagers can just go to the pics like we did.\n",
      "\n",
      "Reasoning:\n",
      "This review is positive for several reasons:\n",
      "\n",
      "*. The reviewer starts by praising the film, stating that it is \"a wonderful film\" and expresses how it has affected their thoughts and words.\n",
      "*. They mention specific scenes and characters from the film, demonstrating their engagement and appreciation for the story.\n",
      "*. The reviewer draws parallels between the film and their own life, mentioning how the advice given in the film relates to their own experiences and emotions.\n",
      "*. They express nostalgia for a bygone era, mentioning the past when one could take a girlfriend to the pictures and have meaningful conversations about the film afterwards.\n",
      "*. The reviewer values the impact of theatrical experiences on personal development and feels that this is lacking in modern times.\n",
      "*. Despite making a mistake by mixing up actresses, the reviewer stands by their comments and maintains their positive sentiment towards the film.\n",
      "\n",
      "Overall, the review is positive because it shows appreciation for the film, its characters, and its themes, as well as the reviewer's nostalgia for the past and the significance of theatrical experiences in personal growth.\n",
      "\n",
      "Sentiment:\n",
      "POSITIVE\n",
      "\n",
      "Classify if this movie review is positive or negative:\n",
      "Review:\n",
      "\"A total waste of time\" Just throw in a few explosions, non stop fighting, exotic cars a deranged millionaire, slow motion computer generated car crashes and last but not least a Hugh Hefner like character with wall to wall hot babes, and mix in a blender and you will have this sorry excuse for a movie. I really got a laugh out of the \"Dr. Evil\" like heavily fortified compound. The plot was somewhere between preposterous and non existent. How many millionaires are willing to make a 25 million dollar bet on a car race? Answer: 4 but, didn't they become millionaires through fiscal responsibility? This was written for pubescent males, it plays like a video game. I did enjoy the Gulfstream II landing in the desert though.\n",
      "\n",
      "Reasoning:\n",
      "This review is negative because of the following reasons:\n",
      "\n",
      "* The reviewer calls the movie \"a total waste of time\" and \"a sorry excuse for a movie\", which indicates a strong negative opinion.\n",
      "* The reviewer sarcastically lists various ingredients that they think were thrown together to make the movie, implying that the film is shallow and lacks substance.\n",
      "* They criticize the plot, calling it \"preposterous\" and \"non-existent\".\n",
      "* They question the plot's plausibility, suggesting that the movie's concept is unrealistic.\n",
      "* They imply that the movie is only suitable for \"pubescent males\" and that it's more like a video game than a serious film.\n",
      "* The only positive comment, about the Gulfstream II landing, is brief and doesn't outweigh the overall negative tone of the review.\n",
      "\n",
      "\n",
      "Sentiment:\n",
      "NEGATIVE\n",
      "\n",
      "Classify if this movie review is positive or negative:\n",
      "Review:\n",
      "{review}\n",
      "\n",
      "Reasoning:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_prompt_cot = raw_prompt_cot.format(example1=positive_example[\"text\"], reasoning1=positive_reasoning, response1=\"POSITIVE\",\n",
    "                                        example2=negative_example[\"text\"], reasoning2=negative_reasoning, response2=\"NEGATIVE\")\n",
    "\n",
    "\n",
    "print(base_prompt_cot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E testamos se funciona corretamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = base_prompt_cot.format(review=train_dataset[-1][\"text\"])\n",
    "\n",
    "groq_sentiment(prompt), train_dataset[-1][\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mais uma vez utilizamos uma função para realizar a avaliação e calculamos a acurácia de validação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_cot(text, label):\n",
    "    '''\n",
    "    Evaluates the chain-of-thought response\n",
    "\n",
    "    Args:\n",
    "        text (str): review to evaluate.\n",
    "        label (int): review expected label.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the model classifies correctly.\n",
    "    '''\n",
    "    \n",
    "\n",
    "    prompt = base_prompt_cot.format(review=text)\n",
    "    result = groq_sentiment(prompt)\n",
    "\n",
    "    return result == label\n",
    "\n",
    "evaluate_cot(**train_dataset[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [13:29<00:00,  8.10s/it] \n"
     ]
    }
   ],
   "source": [
    "executor = ThreadPoolExecutor(max_workers=4) #More workers -> More RateLimit exceptions\n",
    "\n",
    "futures = []\n",
    "for data in val_dataset:\n",
    "    future = executor.submit(evaluate_cot, **data)\n",
    "    futures.append(future)\n",
    "\n",
    "correct_cot = 0\n",
    "for future in tqdm.tqdm(futures):\n",
    "    correct_cot += future.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia - Chain-of-Thought - Validação: 95.0%\n"
     ]
    }
   ],
   "source": [
    "accuracy_cot = correct_cot/len(val_dataset)\n",
    "print(f\"Acurácia - Chain-of-Thought - Validação: {accuracy_cot*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparação e Teste\n",
    "\n",
    "Comparando as quatro técnicas utilizadas, podemos observar que a técnica de zero-shot é a mais rápida, sendo importante ressaltar que este tempo depende fortemente do limite de tokens da API do Groq, sendo esta a técnica que utiliza menos tokens por classificação. Já em quesitos de acurácia, a few-shot se dá melhor, não sendo observado melhorias com o uso de técnicas mais complexas.\n",
    "\n",
    "É por importante ressaltar que a quantidade reduzida de dados de validação utilizados dificultam uma comparação válida entre as técnicas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Técnica | Acurácia de Validação | Tempo\n",
    "-|-|-\n",
    "Zero-shot|88%|7 min 30 s\n",
    "Few-shot|98%|10 min 14 s\n",
    "Prompts dinâmicos|95%|11 min 13 s\n",
    "Chain-of-Thought|95%|13 min 29 s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, calculamos a acurácia de teste com o few-shot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [09:26<00:00,  5.67s/it]\n"
     ]
    }
   ],
   "source": [
    "executor = ThreadPoolExecutor(max_workers=4) #More workers -> More RateLimit exceptions\n",
    "\n",
    "futures = []\n",
    "for data in test_dataset:\n",
    "    future = executor.submit(evaluate_few, **data)\n",
    "    futures.append(future)\n",
    "\n",
    "correct_test = 0\n",
    "for future in tqdm.tqdm(futures):\n",
    "    correct_test += future.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia - Few-shot - Teste: 92.0%\n"
     ]
    }
   ],
   "source": [
    "accuracy_test = correct_test/len(test_dataset)\n",
    "print(f\"Acurácia - Few-shot - Teste: {accuracy_test*100}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
