{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python38\\lib\\site-packages\\thinc\\compat.py:36: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  hasattr(torch, \"has_mps\")\n",
      "c:\\Python38\\lib\\site-packages\\thinc\\compat.py:37: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  and torch.has_mps  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "import os #Operações com o SO (arquivos)\n",
    "import json #Leitura/escrita de arquivos JSON\n",
    "import time #Sleep\n",
    "import threading #Multithreading\n",
    "import unicodedata #Normalização de string\n",
    "import collections #Estrutura de contador e fila\n",
    "import string #Operações com strings\n",
    "import re #Expressões regulares\n",
    "import abc #Classes abstratas\n",
    "import warnings #Lançamento de warnings\n",
    "from typing import Optional, Dict, Tuple, Any #Type hints\n",
    "\n",
    "import torch #spacy não carrega sem importar antes (??)\n",
    "import spacy #Separador em sentenças\n",
    "import tqdm #Barra de progresso\n",
    "import groq #API para o Llama 3 70B\n",
    "from pyserini.search import SimpleSearcher #Busca nos documentos\n",
    "import sentence_transformers #Rerankeamento\n",
    "import bs4 #Remoção de tags HTML\n",
    "import numpy as np #Operações com arrays\n",
    "import matplotlib.pyplot as plt #Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"data\"):\n",
    "    os.mkdir(\"data\")\n",
    "\n",
    "if not os.path.isfile(\"data\\\\context_articles.json\"):\n",
    "    !curl -LO https://iirc-dataset.s3.us-west-2.amazonaws.com/context_articles.tar.gz\n",
    "    !move context_articles.tar.gz data\n",
    "    !tar -xf data/context_articles.tar.gz\n",
    "    !move context_articles.json data\n",
    "\n",
    "if not os.path.isfile(\"data\\\\iirc_test.json\"):\n",
    "    !curl -LO https://iirc-dataset.s3.us-west-2.amazonaws.com/iirc_test.json\n",
    "    !move iirc_test.json data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"data\\\\context_articles.json\", \"r\")\n",
    "articles = json.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"data\\\\iirc_test.json\", \"r\")\n",
    "test_data = json.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_question = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': {'type': 'span',\n",
       "  'answer_spans': [{'text': 'sky and thunder god',\n",
       "    'passage': 'zeus',\n",
       "    'type': 'answer',\n",
       "    'start': 83,\n",
       "    'end': 102}]},\n",
       " 'question': 'What is Zeus know for in Greek mythology?',\n",
       " 'context': [{'text': 'he Palici the sons of Zeus',\n",
       "   'passage': 'main',\n",
       "   'indices': [684, 710]},\n",
       "  {'text': 'in Greek mythology', 'passage': 'main', 'indices': [137, 155]},\n",
       "  {'text': 'Zeus (British English , North American English ; , Zeús ) is the sky and thunder god in ancient Greek religion',\n",
       "   'passage': 'Zeus',\n",
       "   'indices': [0, 110]}],\n",
       " 'question_links': ['Greek mythology', 'Zeus']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0][\"questions\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    item = test_data[i]\n",
    "\n",
    "    main_passage_name = item[\"title\"]\n",
    "\n",
    "    #Get the questions\n",
    "    for q in item[\"questions\"]:\n",
    "        data = {}\n",
    "        \n",
    "        #Get and format the answer\n",
    "        if q[\"answer\"][\"type\"] == \"span\":\n",
    "            data[\"answer\"] = q[\"answer\"][\"answer_spans\"][0][\"text\"]\n",
    "        elif q[\"answer\"][\"type\"] == \"value\":\n",
    "            data[\"answer\"] = q[\"answer\"][\"answer_value\"]+\" \"+q[\"answer\"][\"answer_unit\"]\n",
    "        elif  q[\"answer\"][\"type\"] == \"none\":\n",
    "            continue\n",
    "        elif q[\"answer\"][\"type\"] == \"binary\":\n",
    "            data[\"answer\"] = q[\"answer\"][\"answer_value\"]\n",
    "        else:\n",
    "            raise ValueError\n",
    "        \n",
    "        data[\"question\"] = q[\"question\"]\n",
    "\n",
    "        context = \"\"\n",
    "        for context_item in q[\"context\"]:\n",
    "            passage = context_item[\"passage\"]\n",
    "            if passage == \"main\":\n",
    "                passage = main_passage_name\n",
    "\n",
    "            context += f\"{passage}: {context_item['text']}\"\n",
    "            context += \"\\n\"\n",
    "\n",
    "        data[\"context\"] = context\n",
    "\n",
    "        dataset.append(data)\n",
    "\n",
    "        if len(dataset) == n_question:\n",
    "            break\n",
    "    if len(dataset) == n_question:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test_data, articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroqInterface:\n",
    "    '''\n",
    "    Interface for using the Groq API\n",
    "\n",
    "    Implements a rate limit control for multi-threading use. \n",
    "    '''\n",
    "\n",
    "    _client :groq.Groq = None \n",
    "\n",
    "    LLAMA3_70B = \"llama3-70b-8192\"\n",
    "\n",
    "    inference_lock = threading.Lock()\n",
    "    time_waiter_lock = threading.Lock()\n",
    "    SINGLE_THREAD = True\n",
    "\n",
    "    def __init__(self, model:Optional[str]=None, api_key:Optional[str]=None, json_mode:bool=False, system_message:Optional[str]=None, n_retry:int=5):\n",
    "        '''\n",
    "        GroqInterface constructor.\n",
    "\n",
    "        Args:\n",
    "            model (str, optional): model to use. Llama3 70B is used if None. Default is None\n",
    "            api_key (str, optional): Groq API key to use, if None will check the environment 'GROQ_API_KEY' variable. Default is None.\n",
    "            json_mode (bool): if the model need to output in JSON. Default is False.\n",
    "            system_message (str): the system message to send to the model, if needed. Default is None.\n",
    "            n_retyr (int): number of times to retry if the model fails (not considering RateLimitError). Default is 5.\n",
    "        '''\n",
    "        \n",
    "        if GroqInterface._client is None:\n",
    "\n",
    "            if api_key is None:\n",
    "                api_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "\n",
    "            if api_key is None:\n",
    "                raise RuntimeError(\"API key is not in the environment variables ('GROQ_API_KEY' variable is not set).\")\n",
    "\n",
    "            GroqInterface._client = groq.Groq(api_key=api_key)\n",
    "\n",
    "        if model is None:\n",
    "            model = GroqInterface.LLAMA3_70B\n",
    "        self._model = model\n",
    "\n",
    "        self._system_message = system_message\n",
    "\n",
    "\n",
    "        if json_mode:\n",
    "            self._response_format = {\"type\": \"json_object\"}\n",
    "        else:\n",
    "            self._response_format = None\n",
    "        self._json_mode = json_mode\n",
    "\n",
    "        self._n_retry = n_retry\n",
    "\n",
    "    def __call__(self, prompt:str) -> str:\n",
    "        '''\n",
    "        Generates the model response\n",
    "\n",
    "        Args:\n",
    "            prompt (str): prompt to send to the model.\n",
    "\n",
    "        Returns:\n",
    "            str: model response. \n",
    "        '''\n",
    "        done = False\n",
    "        retry_count = 0\n",
    "        while not done:\n",
    "            try:\n",
    "                if not GroqInterface.SINGLE_THREAD:\n",
    "                    GroqInterface.inference_lock.acquire()\n",
    "                    GroqInterface.inference_lock.release()\n",
    "\n",
    "                messages = []\n",
    "                if self._system_message is not None:\n",
    "                    messages.append({\"role\":\"system\", \"content\":self._system_message})\n",
    "                \n",
    "                messages.append({\"role\":\"user\", \"content\":prompt})\n",
    "\n",
    "                chat_completion = GroqInterface._client.chat.completions.create(\n",
    "                        messages=messages,\n",
    "                        model=self._model,\n",
    "                        response_format=self._response_format\n",
    "                    )\n",
    "                \n",
    "                done = True\n",
    "            except groq.RateLimitError as exception: #Wait\n",
    "                print(\"ERROR\")\n",
    "                print(exception)\n",
    "                \n",
    "                GroqInterface.error = exception\n",
    "                if not GroqInterface.SINGLE_THREAD:\n",
    "                    if not GroqInterface.time_waiter_lock.locked():\n",
    "                        GroqInterface.time_waiter_lock.acquire()\n",
    "                        GroqInterface.inference_lock.acquire()\n",
    "                        time.sleep(2)\n",
    "                        GroqInterface.time_waiter_lock.release()\n",
    "                        GroqInterface.inference_lock.release()\n",
    "                else:\n",
    "                    time.sleep(2)\n",
    "\n",
    "            except KeyboardInterrupt as e: #Stop the code\n",
    "                raise e\n",
    "            except Exception as e: #Retry\n",
    "                retry_count += 1\n",
    "                if retry_count >= self._n_retry:\n",
    "                    raise e\n",
    "\n",
    "        return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tool(abc.ABC):\n",
    "    '''\n",
    "    Base class for creating LLM agent tools.\n",
    "    '''\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def __call__(self, query:str, context:str) -> Dict[str, str]:\n",
    "        '''\n",
    "        Execute the tool.\n",
    "\n",
    "        Args:\n",
    "            query (str): query for the tool execution.\n",
    "            context (str): agent context in the tool execution moment.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, str]: tool results.\n",
    "        '''\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionGenerator(Tool, GroqInterface):\n",
    "\n",
    "\n",
    "\n",
    "    _system_message = '''You are a question generator that outputs in JSON. \n",
    "The JSON object must use the schema: {'questions':['str', 'str', ...]}\n",
    "\n",
    "Please use a valid JSON format.'''\n",
    "\n",
    "    _base_prompt = '''Generate questions for the given answer:\n",
    "\n",
    "Answer: {answer}\n",
    "'''\n",
    "\n",
    "    def __init__(self, model: Optional[str] = None, api_key: Optional[str] = None):\n",
    "\n",
    "        super().__init__(model, api_key, True, QuestionGenerator._system_message)\n",
    "\n",
    "    def __call__(self, query:Optional[Any]=None, context:str=None) -> Dict[str, str]:\n",
    "\n",
    "        \n",
    "        prompt = QuestionGenerator._base_prompt.format(answer=query)\n",
    "\n",
    "        return json.loads(GroqInterface.__call__(self, prompt=prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_generator = QuestionGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = question_generator(dataset[0][\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'questions': ['Who is the Greek god of the sky and thunder?',\n",
       "  'Which Greek god is often depicted holding a lightning bolt?',\n",
       "  'What was the name of the Greek god of the sky and thunder in ancient Greek mythology?',\n",
       "  'Who is the Greek god often associated with the sky and thunderbolt?',\n",
       "  'What is the name of the Greek god of the sky and thunder?']}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder_model:str=\"all-MiniLM-L6-v2\"\n",
    "embedder = sentence_transformers.SentenceTransformer(embedder_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_embedding = embedder.encode(dataset[0][\"question\"], convert_to_tensor=True)\n",
    "qi_embeddings = embedder.encode(questions[\"questions\"], convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.575706958770752"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_scores = sentence_transformers.util.cos_sim(q_embedding, qi_embeddings)\n",
    "\n",
    "score = cosine_scores.sum().item()\n",
    "score /= len(qi_embeddings)\n",
    "\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
