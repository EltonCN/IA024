{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício: RAGAs\n",
    "\n",
    "Elton Cardoso do Nascimento, 233840\n",
    "\n",
    "Enunciado:\n",
    "\n",
    "> - Implementar o RAGAS com o LLaMA-3 70B para avaliar a qualidade das 50 anotações do IIRC usadas no exercício passado.\n",
    "> - O RAGAS considera context, question, answer; keys que estão disponíveis no conjunto de teste do IIRC.\n",
    "> - Opcional:\n",
    ">   - Avaliar as respostas do exercício da aula 9_10\n",
    ">   - Usar multi agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Começamos pela importação de bibliotecas que serão utilizadas neste trabalho:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os #Operações com o SO (arquivos)\n",
    "import json #Leitura/escrita de arquivos JSON\n",
    "import time #Sleep\n",
    "import threading #Multithreading\n",
    "import abc #Classes abstratas\n",
    "from typing import Optional, Dict, List, Tuple, Any #Type hints\n",
    "\n",
    "import tqdm #Barra de progresso\n",
    "import groq #API para o Llama 3 70B\n",
    "import sentence_transformers #Comparação entre sentenças\n",
    "import numpy as np #Operações com arrays\n",
    "import matplotlib.pyplot as plt #Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparação dos dados\n",
    "\n",
    "Carregamos os dados do dataset IIRC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"data\"):\n",
    "    os.mkdir(\"data\")\n",
    "\n",
    "if not os.path.isfile(\"data\\\\context_articles.json\"):\n",
    "    !curl -LO https://iirc-dataset.s3.us-west-2.amazonaws.com/context_articles.tar.gz\n",
    "    !move context_articles.tar.gz data\n",
    "    !tar -xf data/context_articles.tar.gz\n",
    "    !move context_articles.json data\n",
    "\n",
    "if not os.path.isfile(\"data\\\\iirc_test.json\"):\n",
    "    !curl -LO https://iirc-dataset.s3.us-west-2.amazonaws.com/iirc_test.json\n",
    "    !move iirc_test.json data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"data\\\\context_articles.json\", \"r\")\n",
    "articles = json.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"data\\\\iirc_test.json\", \"r\")\n",
    "test_data = json.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E extraímos as 50 primeiras questões, junto das respostas e contextos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_question = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    item = test_data[i]\n",
    "\n",
    "    main_passage_name = item[\"title\"]\n",
    "\n",
    "    #Get the questions\n",
    "    for q in item[\"questions\"]:\n",
    "        data = {}\n",
    "        \n",
    "        #Get and format the answer\n",
    "        if q[\"answer\"][\"type\"] == \"span\":\n",
    "            data[\"answer\"] = q[\"answer\"][\"answer_spans\"][0][\"text\"]\n",
    "        elif q[\"answer\"][\"type\"] == \"value\":\n",
    "            data[\"answer\"] = q[\"answer\"][\"answer_value\"]+\" \"+q[\"answer\"][\"answer_unit\"]\n",
    "        elif  q[\"answer\"][\"type\"] == \"none\":\n",
    "            continue\n",
    "        elif q[\"answer\"][\"type\"] == \"binary\":\n",
    "            data[\"answer\"] = q[\"answer\"][\"answer_value\"]\n",
    "        else:\n",
    "            raise ValueError\n",
    "        \n",
    "        data[\"question\"] = q[\"question\"]\n",
    "\n",
    "        context = \"\"\n",
    "        for context_item in q[\"context\"]:\n",
    "            passage = context_item[\"passage\"]\n",
    "            if passage == \"main\":\n",
    "                passage = main_passage_name\n",
    "\n",
    "            context += f\"{passage}: {context_item['text']}\"\n",
    "            context += \"\\n\"\n",
    "\n",
    "        data[\"context\"] = context\n",
    "        data[\"context_sentence_count\"] = str(len(q[\"context\"]))\n",
    "\n",
    "        dataset.append(data)\n",
    "\n",
    "        if len(dataset) == n_question:\n",
    "            break\n",
    "    if len(dataset) == n_question:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada entrada do dataset tem então a pergunta (\"question\"), respostas (\"answer\"), contexto para responder (\"context\") e quantidade de sentenças no contexto (\"context_sentence_count\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'sky and thunder god',\n",
       " 'question': 'What is Zeus know for in Greek mythology?',\n",
       " 'context': 'Palici: he Palici the sons of Zeus\\nPalici: in Greek mythology\\nZeus: Zeus (British English , North American English ; , Zeús ) is the sky and thunder god in ancient Greek religion\\n',\n",
       " 'context_sentence_count': '3'}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test_data, articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementação do RAGAs\n",
    "\n",
    "Para realizar a implementação do RAGAs, começamos implementando uma interface para realizar inferências com a API do Groq e criar ferramentas (estágios durante a avaliação):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroqInterface:\n",
    "    '''\n",
    "    Interface for using the Groq API\n",
    "\n",
    "    Implements a rate limit control for multi-threading use. \n",
    "    '''\n",
    "\n",
    "    _client :groq.Groq = None \n",
    "\n",
    "    LLAMA3_70B = \"llama3-70b-8192\"\n",
    "\n",
    "    inference_lock = threading.Lock()\n",
    "    time_waiter_lock = threading.Lock()\n",
    "    SINGLE_THREAD = True\n",
    "\n",
    "    def __init__(self, model:Optional[str]=None, api_key:Optional[str]=None, json_mode:bool=False, system_message:Optional[str]=None, n_retry:int=5):\n",
    "        '''\n",
    "        GroqInterface constructor.\n",
    "\n",
    "        Args:\n",
    "            model (str, optional): model to use. Llama3 70B is used if None. Default is None\n",
    "            api_key (str, optional): Groq API key to use, if None will check the environment 'GROQ_API_KEY' variable. Default is None.\n",
    "            json_mode (bool): if the model need to output in JSON. Default is False.\n",
    "            system_message (str): the system message to send to the model, if needed. Default is None.\n",
    "            n_retyr (int): number of times to retry if the model fails (not considering RateLimitError). Default is 5.\n",
    "        '''\n",
    "        \n",
    "        if GroqInterface._client is None:\n",
    "\n",
    "            if api_key is None:\n",
    "                api_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "\n",
    "            if api_key is None:\n",
    "                raise RuntimeError(\"API key is not in the environment variables ('GROQ_API_KEY' variable is not set).\")\n",
    "\n",
    "            GroqInterface._client = groq.Groq(api_key=api_key)\n",
    "\n",
    "        if model is None:\n",
    "            model = GroqInterface.LLAMA3_70B\n",
    "        self._model = model\n",
    "\n",
    "        self._system_message = system_message\n",
    "\n",
    "\n",
    "        if json_mode:\n",
    "            self._response_format = {\"type\": \"json_object\"}\n",
    "        else:\n",
    "            self._response_format = None\n",
    "        self._json_mode = json_mode\n",
    "\n",
    "        self._n_retry = n_retry\n",
    "\n",
    "    def __call__(self, prompt:str) -> str:\n",
    "        '''\n",
    "        Generates the model response\n",
    "\n",
    "        Args:\n",
    "            prompt (str): prompt to send to the model.\n",
    "\n",
    "        Returns:\n",
    "            str: model response. \n",
    "        '''\n",
    "        done = False\n",
    "        retry_count = 0\n",
    "        while not done:\n",
    "            try:\n",
    "                if not GroqInterface.SINGLE_THREAD:\n",
    "                    GroqInterface.inference_lock.acquire()\n",
    "                    GroqInterface.inference_lock.release()\n",
    "\n",
    "                messages = []\n",
    "                if self._system_message is not None:\n",
    "                    messages.append({\"role\":\"system\", \"content\":self._system_message})\n",
    "                \n",
    "                messages.append({\"role\":\"user\", \"content\":prompt})\n",
    "\n",
    "                chat_completion = GroqInterface._client.chat.completions.create(\n",
    "                        messages=messages,\n",
    "                        model=self._model,\n",
    "                        response_format=self._response_format\n",
    "                    )\n",
    "                \n",
    "                done = True\n",
    "            except groq.RateLimitError as exception: #Wait\n",
    "                print(\"ERROR\")\n",
    "                print(exception)\n",
    "                \n",
    "                GroqInterface.error = exception\n",
    "                if not GroqInterface.SINGLE_THREAD:\n",
    "                    if not GroqInterface.time_waiter_lock.locked():\n",
    "                        GroqInterface.time_waiter_lock.acquire()\n",
    "                        GroqInterface.inference_lock.acquire()\n",
    "                        time.sleep(2)\n",
    "                        GroqInterface.time_waiter_lock.release()\n",
    "                        GroqInterface.inference_lock.release()\n",
    "                else:\n",
    "                    time.sleep(2)\n",
    "\n",
    "            except KeyboardInterrupt as e: #Stop the code\n",
    "                raise e\n",
    "            except Exception as e: #Retry\n",
    "                print(\"ERROR\")\n",
    "                print(e)\n",
    "                retry_count += 1\n",
    "                if retry_count >= self._n_retry:\n",
    "                    raise e\n",
    "\n",
    "        return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tool(abc.ABC):\n",
    "    '''\n",
    "    Base class for creating LLM agent tools.\n",
    "    '''\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def __call__(self, query:Dict[str, str], context:str) -> Dict[str, str]:\n",
    "        '''\n",
    "        Execute the tool.\n",
    "\n",
    "        Args:\n",
    "            query (str): query for the tool execution.\n",
    "            context (str): agent context in the tool execution moment.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, str]: tool results.\n",
    "        '''\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Relevance\n",
    "\n",
    "A relevância da resposta é definida como o alinhamento médio entre perguntas geradas a partir da resposta em relação a pergunta original.\n",
    "\n",
    "Começamos com um gerador de questões a partir da resposta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionGenerator(Tool, GroqInterface):\n",
    "    '''\n",
    "    Generates questions from a given answer\n",
    "    '''\n",
    "\n",
    "    _system_message = '''You are a question generator that outputs in JSON. \n",
    "The JSON object must use the schema: {'questions':['str', 'str', ...]}\n",
    "\n",
    "Please use a valid JSON format.'''\n",
    "\n",
    "    _base_prompt = '''Generate questions for the given answer:\n",
    "\n",
    "Answer: {answer}\n",
    "'''\n",
    "\n",
    "    def __init__(self, model: Optional[str] = None, api_key: Optional[str] = None):\n",
    "        '''\n",
    "        QuestionGenerator constructor.\n",
    "\n",
    "        Args:\n",
    "            model (str, optional): model to use. Llama3 70B is used if None. Default is None\n",
    "            api_key (str, optional): Groq API key to use, if None will check the environment 'GROQ_API_KEY' variable. Default is None.\n",
    "        '''\n",
    "\n",
    "        super().__init__(model, api_key, True, QuestionGenerator._system_message)\n",
    "\n",
    "    def __call__(self, query:Dict[str, str], context:str=None) -> Dict[str, List[str]]:\n",
    "        '''\n",
    "        Generates questions from a given answer\n",
    "\n",
    "        Args:\n",
    "            query (Dict[str, str]): query, must contain a key \"answer\" with the answer to generate questions.\n",
    "            context (str, optional): not used. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, List[str]]: result that contains key \"questions\" with the questions.\n",
    "        '''\n",
    "        answer = query[\"answer\"]\n",
    "        \n",
    "        prompt = QuestionGenerator._base_prompt.format(answer=answer)\n",
    "\n",
    "\n",
    "        return json.loads(GroqInterface.__call__(self, prompt=prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testando o gerador, podemos observar um primeiro problema do dataset: as respostas contém poucas informações para refazer a pergunta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_generator = QuestionGenerator()\n",
    "questions_0 = question_generator(dataset[0])\n",
    "questions_1 = question_generator(dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is Zeus know for in Greek mythology?: sky and thunder god\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'questions': ['Who was Zeus in Greek mythology?',\n",
       "  'What was the title of the Norse god Odin in Viking mythology?',\n",
       "  'Who was the ruler of Mount Olympus in Greek mythology?',\n",
       "  'Who controlled the weather in Greek mythology?',\n",
       "  'What was the name of the principal god of the Norse pantheon?']}"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset[0][\"question\"]+\": \"+dataset[0][\"answer\"])\n",
    "print(\"----------\")\n",
    "questions_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How long had the First World War been over when Messe was named aide-de-camp?: 5 years\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'questions': ['What is the typical tenure of a US president?',\n",
       "  'How many years are in a standard Olympic cycle?',\n",
       "  'What is the typical age of a child when they start kindergarten?',\n",
       "  \"What is the duration of a standard bachelor's degree in the US?\",\n",
       "  'How many years are in a standard presidential term in France?']}"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset[1][\"question\"]+\": \"+dataset[1][\"answer\"])\n",
    "print(\"----------\")\n",
    "questions_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos também um comparador de questões utilizando similaridade de cosseno e testamos seu funcionamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionComparator(Tool):\n",
    "    '''\n",
    "    Comparates questions\n",
    "\n",
    "    '''\n",
    "\n",
    "    def __init__(self, embedder_model:str=\"all-MiniLM-L6-v2\") -> None:\n",
    "        '''\n",
    "        QuestionComparator constructor.\n",
    "\n",
    "        Args:\n",
    "            embedder_model (str, optional): model to generate embeddings. Defaults to \"all-MiniLM-L6-v2\".\n",
    "        '''\n",
    "        self._embedder = sentence_transformers.SentenceTransformer(embedder_model)\n",
    "\n",
    "    def __call__(self, query: Dict[str, str], context: Optional[str]=None) -> Dict[str, str]:\n",
    "        '''\n",
    "        Calculates the similarity between one question and a list of questions.\n",
    "\n",
    "        Args:\n",
    "            query (Dict[str, str]): must contain keys \"query_question\" with \n",
    "                question to compare with list of questions in \"questions\" key. \n",
    "            context (Optional[str], optional): not used. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, str]: result, have a single key \"score\" with the similarity score.\n",
    "        '''\n",
    "        query_question = query[\"query_question\"]\n",
    "        questions = query[\"questions\"]\n",
    "\n",
    "        q_embedding = self._embedder.encode(query_question, convert_to_tensor=True)\n",
    "        qi_embeddings = self._embedder.encode(questions, convert_to_tensor=True)\n",
    "            \n",
    "        cosine_scores = sentence_transformers.util.cos_sim(q_embedding, qi_embeddings)\n",
    "\n",
    "        score = cosine_scores.sum().item()\n",
    "        score /= len(qi_embeddings)\n",
    "\n",
    "        result = {\"score\":str(score)}\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': '0.5827430248260498'}"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_comparator = QuestionComparator()\n",
    "\n",
    "comparator_query = {\"query_question\":dataset[0][\"question\"], \"questions\":questions_0[\"questions\"]}\n",
    "comparator_result = question_comparator(comparator_query)\n",
    "\n",
    "comparator_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E criamos a classe principal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerRelevanceEvaluator(Tool):\n",
    "    '''\n",
    "    Evaluates the relevance of a answer.\n",
    "    '''\n",
    "    def __init__(self, question_generator:QuestionGenerator=None, question_comparator:QuestionComparator=None) -> None:\n",
    "        '''\n",
    "        AnswerRelevanceEvaluator constructor\n",
    "\n",
    "        Args:\n",
    "            question_generator (QuestionGenerator, optional): generator to use, if None creates a new with default arguments. Defaults to None.\n",
    "            question_comparator (QuestionComparator, optional): comparator to use, if None creates a new with default arguments. Defaults to None.\n",
    "        '''\n",
    "\n",
    "\n",
    "        if question_generator is None:\n",
    "            question_generator = QuestionGenerator()\n",
    "        if question_comparator is None:\n",
    "            question_comparator = QuestionComparator()\n",
    "\n",
    "        self._question_generator = question_generator\n",
    "        self._question_comparator = question_comparator\n",
    "    \n",
    "    def __call__(self, query: Dict[str, str], context: str=None) -> Dict[str, str]:\n",
    "        '''\n",
    "        Evaluates the relevance of a answer.\n",
    "\n",
    "        Args:\n",
    "            query (Dict[str, str]): must contain key \"answer\" with answer to evaluate.\n",
    "            context (str, optional): not used. Defaults to None.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: if query does not contains \"answer\" key\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, str]: result with key \"score\" with the score.\n",
    "        '''\n",
    "        \n",
    "        if \"answer\" not in query:\n",
    "            raise ValueError(\"Query must have the answer to evaluate (query not have key 'answer').\")\n",
    "\n",
    "        generator_result = self._question_generator(query)\n",
    "\n",
    "        comparator_query = {\"query_question\":query[\"question\"], \"questions\":generator_result[\"questions\"]}\n",
    "        comparator_result = self._question_comparator(comparator_query)\n",
    "\n",
    "        return comparator_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_relevance_evaluator = AnswerRelevanceEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': '0.6399625142415365'}"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_relevance_evaluator(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faithfulness\n",
    "\n",
    "Para avaliar a fidelidade, a pontuação é definida como a proporção de declarações geradas a partir da resposta que estão contidas no contexto.\n",
    "\n",
    "Começamos criando um gerador de declarações:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatementGenerator(Tool, GroqInterface):\n",
    "    '''\n",
    "    Generates statements from a given answer and question.\n",
    "    '''\n",
    "\n",
    "    _system_message = '''You are a statement generator that outputs in JSON. \n",
    "The JSON object must use the schema: {'statements':['str', 'str', ...]}\n",
    "\n",
    "Please use a valid JSON format.'''\n",
    "\n",
    "    _base_prompt = '''Given a question and answer, create one or more statements from each sentence in the given answer:\n",
    "\n",
    "Question: {question}\n",
    "Answer: {answer}\n",
    "'''\n",
    "\n",
    "    def __init__(self, model: Optional[str] = None, api_key: Optional[str] = None):\n",
    "        '''\n",
    "        StatementGenerator constructor.\n",
    "\n",
    "        Args:\n",
    "            model (str, optional): model to use. Llama3 70B is used if None. Default is None\n",
    "            api_key (str, optional): Groq API key to use, if None will check the environment 'GROQ_API_KEY' variable. Default is None.\n",
    "        '''\n",
    "\n",
    "        super().__init__(model, api_key, True, StatementGenerator._system_message)\n",
    "\n",
    "    def __call__(self, query:Dict[str, str], context:str=None) -> Dict[str, str]:\n",
    "        '''\n",
    "        Generates statements based in a given answer.\n",
    "\n",
    "        Args:\n",
    "            query (Dict[str, str]): must contains keys \"question\" with the question and \"answer\" with the answer.\n",
    "            context (str, optional): not used. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, str]: has key 'statements' with a list of generated statements.\n",
    "        '''\n",
    "        question = query[\"question\"]\n",
    "        answer = query[\"answer\"]\n",
    "        \n",
    "        prompt = StatementGenerator._base_prompt.format(question=question, answer=answer)\n",
    "\n",
    "\n",
    "        return json.loads(GroqInterface.__call__(self, prompt=prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'statements': ['Zeus is known for being the sky god in Greek mythology.',\n",
       "  'Zeus is known for being the thunder god in Greek mythology.']}"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statement_generator = StatementGenerator()\n",
    "generator_result = statement_generator(dataset[0])\n",
    "\n",
    "generator_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E definimos um verificador a partir do contexto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatementContextVerificator(Tool, GroqInterface):\n",
    "    '''\n",
    "    Verifies if a list of statements is supported by a given context.\n",
    "    '''\n",
    "\n",
    "    _system_message = '''You are a statement analist that outputs in JSON. \n",
    "The JSON object must use the schema: \n",
    "{'analysis':[{'explanation':'str', 'verdict':'bool'}, {'explanation':'str', 'verdict':'bool'}, ...  ]}.\n",
    "\n",
    "The analysis list must be of same size of the input.\n",
    "\n",
    "Please use a valid JSON format.'''\n",
    "\n",
    "    _base_prompt = '''Context: \n",
    "{context}\n",
    "\n",
    "Consider the given context and following statements, then determine whether they are supported by \n",
    "the information present in the context. Provide a brief explanation for each statement before arriving at the verdict (true/false). \n",
    "Provide a final verdict for each statement in order at the end in the given format. Do not deviate from the specified format.\n",
    "\n",
    "{statements}\n",
    "'''\n",
    "\n",
    "    def __init__(self, model: Optional[str] = None, api_key: Optional[str] = None):\n",
    "        '''\n",
    "        StatementContextVerificator constructor.\n",
    "\n",
    "        Args:\n",
    "            model (str, optional): model to use. Llama3 70B is used if None. Default is None\n",
    "            api_key (str, optional): Groq API key to use, if None will check the environment 'GROQ_API_KEY' variable. Default is None.\n",
    "        '''\n",
    "\n",
    "        super().__init__(model, api_key, True, StatementContextVerificator._system_message)\n",
    "\n",
    "    def __call__(self, query:Dict[str, str], context:str=None) -> Dict[str, str]:\n",
    "        '''\n",
    "        Verifies if a list of statements is supported by a given context.\n",
    "\n",
    "        Args:\n",
    "            query (Dict[str, str]): must contain a key \"statements\" with list of statements and a key \"rag_context\" with the context.\n",
    "            context (str, optional): main LLM context, not used. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, str]: result with key 'analysis', a list of objects with keys 'explanation' and 'verdict'.\n",
    "        '''\n",
    "\n",
    "        statements = \"\"\n",
    "        \n",
    "        for statement in query[\"statements\"]:\n",
    "            statements += \"Statement: \"+statement+\"\\n\"\n",
    "        \n",
    "        prompt = StatementContextVerificator._base_prompt.format(statements=statements, context=query[\"rag_context\"])\n",
    "\n",
    "\n",
    "        return json.loads(GroqInterface.__call__(self, prompt=prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement_context_verificator = StatementContextVerificator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\"statements\": generator_result[\"statements\"], \"rag_context\":dataset[0][\"context\"]}\n",
    "verificator_result = statement_context_verificator(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analysis': [{'explanation': 'The context states that Zeus is the sky and thunder god in ancient Greek religion, which aligns with the statement that Zeus is known as the sky god in Greek mythology.',\n",
       "   'verdict': True},\n",
       "  {'explanation': 'The context states that Zeus is the sky and thunder god in ancient Greek religion, which aligns with the statement that Zeus is known as the thunder god in Greek mythology.',\n",
       "   'verdict': True}]}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verificator_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E a classe principal que calculará a pontuação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaithfulnessEvaluator(Tool):\n",
    "    '''\n",
    "    Evaluates the faithfulness of a answer.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, statement_generator:StatementGenerator=None, statement_context_verificator:StatementContextVerificator=None) -> None:\n",
    "        '''\n",
    "        FaithfulnessEvaluator constructor.\n",
    "\n",
    "        Args:\n",
    "            statement_generator (StatementGenerator, optional): generator to use, if None creates a new with default arguments. Defaults to None.\n",
    "            statement_context_verificator (StatementContextVerificator, optional): context verificator to use, if None creates a new with default arguments. Defaults to None.\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        if statement_generator is None:\n",
    "            statement_generator = StatementGenerator()\n",
    "        if statement_context_verificator is None:\n",
    "            statement_context_verificator = StatementContextVerificator()\n",
    "\n",
    "        self._statement_generator = statement_generator\n",
    "        self._statement_context_verificator = statement_context_verificator\n",
    "\n",
    "    def __call__(self, query: Dict[str, str], context: str=None) -> Dict[str, str]:\n",
    "        '''\n",
    "        Evaluates the faithfulness of a answer.\n",
    "\n",
    "        Args:\n",
    "            query (Dict[str, str]): must contain keys \"question\", \"answer\" and \"context\".\n",
    "            context (str, optional): main agent context, not used. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, str]: result with a single \"score\" key.\n",
    "        '''\n",
    "\n",
    "        generator_result = self._statement_generator(query)\n",
    "\n",
    "        verificator_query = {\"statements\": generator_result[\"statements\"], \"rag_context\":query[\"context\"]}\n",
    "        verificator_result = self._statement_context_verificator(verificator_query)\n",
    "\n",
    "        supported = 0\n",
    "        total = len(generator_result[\"statements\"])\n",
    "\n",
    "        for analisy in verificator_result[\"analysis\"]:\n",
    "            supported += analisy[\"verdict\"]\n",
    "\n",
    "        score = supported/total\n",
    "\n",
    "        return {\"score\":str(score)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faithfulness_evaluator = FaithfulnessEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': '1.0'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faithfulness_evaluator(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Relevance\n",
    "\n",
    "Para avaliar a relevância do contexto, sentenças úteis para responder a questão são extraídas do contexto, e é avaliado a proporção de sentenças úteis extraídas em relação a quantidade total de sentenças no contexto.\n",
    "\n",
    "Primeiro definimos um extrator de sentenças. Observe que o prompt foi levemente modificado em relação ao artigo para considerar que a resposta estará em formato JSON, não necessitando retornar algo especial caso não existam sentenças úteis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceExtractor(Tool, GroqInterface):\n",
    "    '''\n",
    "    Extracts relevant sentences for a question from a given context\n",
    "    '''\n",
    "\n",
    "    _system_message = '''You are a sentence extractor that outputs in JSON. \n",
    "The JSON object must use the schema: {'sentences':['str', 'str', ...]}\n",
    "\n",
    "Please use a valid JSON format.'''\n",
    "\n",
    "    _base_prompt = '''Context: \n",
    "{context}\n",
    "\n",
    "Please extract relevant sentences from the provided context that can potentially help answer the following question. \n",
    "If no relevant sentences are found, or if you believe the question cannot be answered from the given context, return a object with an empty 'sentences' list.\n",
    "While extract ing candidate sentences you’re not allowed to make any changes to sentences from given context.\n",
    "\n",
    "Question: {question}\n",
    "'''\n",
    "\n",
    "    def __init__(self, model: Optional[str] = None, api_key: Optional[str] = None):\n",
    "        '''\n",
    "        SentenceExtractor constructor.\n",
    "\n",
    "        Args:\n",
    "            model (str, optional): model to use. Llama3 70B is used if None. Default is None\n",
    "            api_key (str, optional): Groq API key to use, if None will check the environment 'GROQ_API_KEY' variable. Default is None.\n",
    "        '''\n",
    "\n",
    "        super().__init__(model, api_key, True, SentenceExtractor._system_message)\n",
    "\n",
    "    def __call__(self, query:Dict[str, str], context:str=None) -> Dict[str, str]:\n",
    "        '''\n",
    "        Extracts relevant sentences for a question from a given context.\n",
    "\n",
    "        Args:\n",
    "            query (Dict[str, str]): must contain keys \"question\" with the question that the context answers and \"rag_context\" with the context.\n",
    "            context (str, optional): main LLM context, not used. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, str]: _description_\n",
    "        '''\n",
    "        question = query[\"question\"]\n",
    "        rag_context = query[\"rag_context\"]\n",
    "        \n",
    "        prompt = SentenceExtractor._base_prompt.format(question=question, context=rag_context)\n",
    "\n",
    "        return json.loads(GroqInterface.__call__(self, prompt=prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_extractor = SentenceExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor_query = {\"question\": dataset[0][\"question\"], \"rag_context\":dataset[0][\"context\"]}\n",
    "extractor_result = sentence_extractor(extractor_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentences': ['Zeus: Zeus (British English , North American English ; , Zeús ) is the sky and thunder god in ancient Greek religion']}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractor_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E calculamos o score de relevância:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextRelevanceEvaluator(Tool):\n",
    "    '''\n",
    "    Evaluates the relevance of a context to answer a question.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, sentence_extractor:SentenceExtractor=None) -> None:\n",
    "        '''\n",
    "        ContextRelevanceEvaluator constructor.\n",
    "\n",
    "        Args:\n",
    "            sentence_extractor (SentenceExtractor, optional): extractor to use, if None creates a new with default arguments. Defaults to None.\n",
    "        '''\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        if sentence_extractor is None:\n",
    "            sentence_extractor = SentenceExtractor()\n",
    "\n",
    "        self._sentence_extractor = sentence_extractor\n",
    "\n",
    "    def __call__(self, query: Dict[str, str], context: str=None) -> Dict[str, str]:\n",
    "        '''\n",
    "        Evaluates the relevance of a context to answer a question.\n",
    "\n",
    "        Args:\n",
    "            query (Dict[str, str]): must contain keys:\n",
    "                - \"question\" with the question.\n",
    "                - \"context\" with the context used to answer the question.\n",
    "                - \"context_sentence_count\" with the number of sentences in the context\n",
    "            context (str, optional): main LLM context, not used. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, str]: result with single key \"score\".\n",
    "        '''\n",
    "\n",
    "        extractor_query = {\"question\": query[\"question\"], \"rag_context\": query[\"context\"]}\n",
    "        extractor_result = sentence_extractor(extractor_query)\n",
    "\n",
    "        score = len(extractor_result[\"sentences\"]) / int(query[\"context_sentence_count\"])\n",
    "\n",
    "        result = {\"score\":str(score)}\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_relevance_evaluator = ContextRelevanceEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = context_relevance_evaluator(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': '0.3333333333333333'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAGAs Evaluator\n",
    "\n",
    "E, por fim, definimos um avaliador que calculará os 3 scores para um item do dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGAsEvaluator(Tool):\n",
    "    '''\n",
    "    Evaluates a RAG result using RAGAs methodology.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, faithfulness_evaluator:FaithfulnessEvaluator=None, \n",
    "                 answer_relevance_evaluator:AnswerRelevanceEvaluator=None, \n",
    "                 context_relevance_evaluator:ContextRelevanceEvaluator=None) -> None:\n",
    "        '''\n",
    "        RAGAsEvaluator constructor.\n",
    "\n",
    "        Args:\n",
    "            faithfulness_evaluator (FaithfulnessEvaluator, optional): faithfulness evaluator to use, if None creates a new with default arguments. Defaults to None.\n",
    "            answer_relevance_evaluator (AnswerRelevanceEvaluator, optional): answer relevance evaluator to use, if None creates a new with default arguments. Defaults to None.\n",
    "            context_relevance_evaluator (ContextRelevanceEvaluator, optional): context relevance evaluator to use, if None creates a new with default arguments. Defaults to None.\n",
    "        '''\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        if faithfulness_evaluator is None:\n",
    "            faithfulness_evaluator = FaithfulnessEvaluator()\n",
    "        if answer_relevance_evaluator is None:\n",
    "            answer_relevance_evaluator = AnswerRelevanceEvaluator()\n",
    "        if context_relevance_evaluator is None:\n",
    "            context_relevance_evaluator = ContextRelevanceEvaluator()\n",
    "\n",
    "        self._faithfulness_evaluator = faithfulness_evaluator\n",
    "        self._answer_relevance_evaluator = answer_relevance_evaluator\n",
    "        self._context_relevance_evaluator = context_relevance_evaluator\n",
    "    \n",
    "    def __call__(self, query: Dict[str, str], context: str=None) -> Dict[str, str]:\n",
    "        '''\n",
    "        Evaluates a RAG result using RAGAs methodology.\n",
    "\n",
    "        Args:\n",
    "            query (Dict[str, str]): must contain keys:\n",
    "                - \"question\" with the question.\n",
    "                - \"answer\" with the question answer.\n",
    "                - \"context\" with the context used to answer the question.\n",
    "                - \"context_sentence_count\" with the number of sentences in the context\n",
    "            context (str, optional): main LLM context, not used. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, str]: result with scores in keys \"faithfulness\", \"answer_relevance\" and \"context_relevance\".\n",
    "        '''\n",
    "\n",
    "        faithfulness_result = self._faithfulness_evaluator(query)\n",
    "        answer_relevance_result = self._answer_relevance_evaluator(query)\n",
    "        context_relevance_result = self._context_relevance_evaluator(query)\n",
    "\n",
    "        result = { \n",
    "            \"faithfulness\" : faithfulness_result[\"score\"],\n",
    "            \"answer_relevance\" : answer_relevance_result[\"score\"],\n",
    "            \"context_relevance\" : context_relevance_result[\"score\"],\n",
    "        }\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_evaluator = RAGAsEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_result = ragas_evaluator(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'faithfulness': '1.0',\n",
       " 'answer_relevance': '0.6503564516703287',\n",
       " 'context_relevance': '0.3333333333333333'}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ragas_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geração de avaliações\n",
    "\n",
    "Com o avaliador criado podemos calcular os scores de todas as entradas no dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 26/50 [04:14<04:55, 12.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR\n",
      "Error code: 400 - {'error': {'message': \"Failed to generate JSON. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'json_validate_failed', 'failed_generation': \"{'analysis':[{'explanation':'The statement claims that the Perthshire Regiment had been in operation for 69 years when Christopher Harison joined it as a captain in 1849. From the context, we know that the 73rd Regiment of Foot was raised in 1780, and Harison joined in 1849, which means the regiment had been operating for approximately 69 years, matching the statement.', 'verdict':True}]}\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 35/50 [06:09<03:10, 12.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR\n",
      "Error code: 400 - {'error': {'message': \"Failed to generate JSON. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'json_validate_failed', 'failed_generation': '{\"analysis\":[{\"explanation\":\"The context lists four WHL Championship losses for the Saskatoon Blades, which is the team where Ashton played junior hockey.\",\"verdict\":False}]}'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 37/50 [06:38<02:55, 13.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR\n",
      "Error code: 400 - {'error': {'message': \"Failed to generate JSON. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'json_validate_failed', 'failed_generation': '{\\n\"sentences\": [\"Ron Ashton: Ashton played junior hockey for his hometown Saskatoon Blades of the Western Canadian Hockey League (now WHL), playing his first full season in 1972-73 with future NHL and WHA players Bob Bourne, Dave Lewis and George Pesut,\", \"Bob Bourne: Robert Glen Bourne (born June 21, 1954)\", \"Dave Lewis (ice hockey): David Rodney Lewis (born July 3, 1953\", \"George Pesut: George Matthew Pesut \"(Zuti)\" (born June 17, 1953\"]\\n}'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [09:24<00:00, 11.29s/it]\n"
     ]
    }
   ],
   "source": [
    "evaluations = []\n",
    "\n",
    "file = open(\"evaluations.jsonl\", \"w\")\n",
    "\n",
    "for data in tqdm.tqdm(dataset):\n",
    "    try:\n",
    "        evaluation = ragas_evaluator(data)\n",
    "\n",
    "    except KeyboardInterrupt as e:\n",
    "        file.close()\n",
    "        raise e\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Evaluation error:\", e)\n",
    "\n",
    "        evaluation = { \n",
    "            \"faithfulness\" : \"0\",\n",
    "            \"answer_relevance\" : \"0\",\n",
    "            \"context_relevance\" : \"0\",\n",
    "        }\n",
    "\n",
    "    evaluations.append(evaluation)\n",
    "\n",
    "    file.write(json.dumps(evaluation)+\"\\n\")\n",
    "    file.flush()\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"evaluations.json\", \"w\")\n",
    "json.dump(evaluations, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise\n",
    "\n",
    "Para realizar a análise, vamos extrair os scores de cada avaliação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "faithfulness = np.empty(n_question)\n",
    "answer_relevance = np.empty(n_question)\n",
    "context_relevance = np.empty(n_question)\n",
    "\n",
    "for i in range(n_question):\n",
    "    faithfulness[i] = evaluations[i][\"faithfulness\"]\n",
    "    answer_relevance[i] = evaluations[i][\"answer_relevance\"]\n",
    "    context_relevance[i] = evaluations[i][\"context_relevance\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observando inicial os intervalos, é perceptível um problema com a pontuação de relevância do contexto: como considera a proporção entre sentenças extraídas e originalmente no contexto, seu valor pode ser superior a 1 caso sejam extraídas mais sentenças do que contadas originalmente (no caso da quebra de uma sentença, por exemplo):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfullness  Min: 0.0 | Max: 1.0\n",
      "Answer Relevance  Min: 0.026833323140939076 | Max: 0.7692429224650065\n",
      "Context Relevance  Min: 0.0 | Max: 5.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Faithfullness \", \"Min:\", faithfulness.min(), \"| Max:\", faithfulness.max())\n",
    "print(\"Answer Relevance \", \"Min:\", answer_relevance.min(), \"| Max:\", answer_relevance.max())\n",
    "print(\"Context Relevance \", \"Min:\", context_relevance.min(), \"| Max:\", context_relevance.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para aliviar o impacto deste problema nas estatísticas, normalizamos esta pontuação em [0, 1]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_relevance = np.clip(context_relevance, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos calcular então a média e desvio de cada métrica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criterion|Score\n",
      "-|-\n",
      "Faithfullness| 0.73 ± 0.43829214001622246\n",
      "Answer Relevance| 0.24423735859899814 ± 0.14732980886784547\n",
      "Context Relevance| 0.6689999999999999 ± 0.2985522474282256\n"
     ]
    }
   ],
   "source": [
    "print(\"Criterion|Score\")\n",
    "print(\"-|-\")\n",
    "print(\"Faithfullness|\", faithfulness.mean(), \"±\", faithfulness.std())\n",
    "print(\"Answer Relevance|\", answer_relevance.mean(), \"±\", answer_relevance.std())\n",
    "print(\"Context Relevance|\", context_relevance.mean(), \"±\", context_relevance.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criterion|Score\n",
    "-|-\n",
    "Faithfullness| 0.73 ± 0.43829214001622246\n",
    "Answer Relevance| 0.24423735859899814 ± 0.14732980886784547\n",
    "Context Relevance| 0.6689999999999999 ± 0.2985522474282256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É perceptível como que a pontuação de fidelidade e contexto tem pontuações muito maiores do que a da relevância da resposta. Isso possivelmente se deve ao problema já observado das respostas do dataset poderem ser genéricas e ter poucas informações da questão original. Alterar o prompt para incluir a questão possivelmente pode resolver este problema.\n",
    "\n",
    "Ao avaliar o desvio padrão, a pontuação de fidelidade é a mais instável. Observando seu histograma, podemos ver que a pontuação aparenta ter uma característica binária:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(faithfulness)\n",
    "\n",
    "plt.xlim(-0.1, 1.1)\n",
    "\n",
    "plt.title(\"Histograma de Fidelidade\")\n",
    "plt.xlabel(\"Fidelidade\")\n",
    "plt.ylabel(\"Contagem\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enquanto que a relevância da resposta se concentra em pontuações baixas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(answer_relevance)\n",
    "\n",
    "plt.xlim(-0.1, 1.1)\n",
    "\n",
    "plt.title(\"Histograma de Relevância da Resposta\")\n",
    "plt.xlabel(\"Relevância da Resposta\")\n",
    "plt.ylabel(\"Contagem\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E a relevância do contexto tem um pico na pontuação máxima, devido ao problema de intervalo já discutido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(context_relevance)\n",
    "\n",
    "plt.title(\"Histograma de Relevância do Contexto\")\n",
    "plt.xlabel(\"Relevância do Contexto\")\n",
    "plt.ylabel(\"Contagem\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observando a correlação entre as métricas, mais precisamente a raiz dela, podemos perceber que as métricas são moderadamente relacionadas, principalmente a fidelidade e relevância do contexto (possivelmente um contexto mais relevante também seja mais útil para obter a resposta, tornando-a mais fiel), o que indica uma possível dificuldade em avaliar separadamente estes três aspectos (as métricas não avaliam aspectos \"ortogonais\"), exceto entre os aspectos de relevância de resposta e contexto. Todas as correlações são positivas, indicando que aumentam junto com a melhoria do processo e resposta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "faithfulness_norm = faithfulness/faithfulness.std(ddof=1)\n",
    "answer_relevance_norm = answer_relevance/answer_relevance.std(ddof=1)\n",
    "context_relevance_norm = context_relevance/context_relevance.std(ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.15436366, 0.19841588],\n",
       "       [0.15436366, 1.        , 0.02803894],\n",
       "       [0.19841588, 0.02803894, 1.        ]])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov = np.cov([faithfulness_norm, answer_relevance_norm, context_relevance_norm])\n",
    "\n",
    "cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.3928914 , 0.44543897],\n",
       "       [0.3928914 , 1.        , 0.16744832],\n",
       "       [0.44543897, 0.16744832, 1.        ]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(cov)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
